\section{Algorithme de décision optimale}

% Ce chapitre se veut une brève introduction aux propriétés des espaces de décision obtenus
% par noyaux reproduisants. En premier lieu, une discussion sur la forme duale du problème
% linéaire ainsi que les propriétés des espaces à noyau permettront d'obtenir une meilleure
% intuition (Section \ref{k:def}). Par la suite, la Section \ref{k:alg} présentera quels
% algorithmes permettent de trouver une politique d'investissement optimale pour un
% investisseur doté d'une fonction d'utilité $u$ à partir d'un ensemble d'entraînement
% $\S_n = \{x_i,r_i\}_{i=1}^n \sim M^n$ échantilloné à partir de la loi de marché $M$. Quelques
% exemples de noyaux courants seront présentés, suivis des dérivations des deux formes
% d'optimisation.



\subsection{Formulations primale et duale}

Tel que discuté en introduction, le cas le plus simple pour un espace de décision est
celui où l'espace $\X$ des variables de marché ne subit aucune transformation. L'espace
$\Q$ correspond\footnote{En fait, pour être exact, $q : \X \to \Re$ étant une
  \textit{fonction}, il est plus exact de faire correspondre $\Q$ à l'espace \textit{dual}
  $\X^*$. Mais par le théorème de Riez, à tout vecteur $v \in \bm V$ d'un espace vectoriel,
  il existe un unique vecteur dual $v^* \in \bm V^*$ (noté $v^T$ en dimension finie). On
  peut donc se contenter de faire correspondre $\Q$ à $\X$.} alors à $\X$ et le scalaire
de décision $q(x)$ est obtenu par produit scalaire $q^Tx$. On obtient donc ici le problème
sous la forme \textit{primale}:
{\begin{equation}
  \boxed{
  \maximizeEquation[q\in\X]{n^{-1}\sumi u(r_i\,q^Tx_i) - \frac{\lambda}{2}\|q\|^2}}
\end{equation}
\vspace{-\baselineskip}\captionof*{figure}{\textit{Formulation primale I}}}

Par la théorie de l'optimisation convexe (voir \cite{boyd2004convex}) on sait qu'une
solution $\qh$ existe et qu'elle est unique. En supposant que $\X \subseteq \Re^p$, on peut alors
exprimer $\qh$ comme une combinaison linéaire de $p$ coordonnées.

Cependant, $\qh$ peut aussi être exprimé comme une combinaison linéaires de la base
$\{x_1,\ldots,x_n\}$, \ie\ il existe un vecteur $\hat\alpha \in \Re^n$ tel que
$\qh = \Xi^T\hat\alpha$, où $\Xi \in \Re^{n \times p}$ est la matrice des observations.

Il suffit en effet de remarquer que l'espace $\Q = \X$ peut être décomposé comme la somme
directe du sous-espace vectoriel $\hat\X$ engendré par $\Xi^T$ et son complément orthogonal
$\hat\X^\perp$, \ie\ $\Q = \hat\X \oplus \hat\X^\perp$. Ainsi, tout vecteur de décision
$q \in \Q$ s'exprime comme la somme de $\hat x\in\hat\X$ et
$\hat x^\perp \in \hat\X^\perp$, deux vecteurs orthogonaux (voir les appendices de
\cite{boyd2004convex} ou \cite{mohri2012foundations}). La fonction objectif $\EU_\lambda$
évaluée au point $q$ se simplifie alors ainsi:
\begin{align}
  \EU_\lambda(q) &= \EU_\lambda(\hat x + \hat x^\perp)\\
           &= n^{-1}\sumi u(r_i\,(\hat x + \hat x^\perp)^Tx_i) - \frac{\lambda}{2}\|\hat x + \hat
             x^\perp\|^2\\
           &\leq n^{-1}\sumi u(r_i\,\hat x^Tx_i) - \frac{\lambda}{2}\|\hat x\|^2,
\end{align}
puisque, par définition, $(\hat x^\perp)^Tx_i = 0$ pour toute observation $x_i$ et que d'autre
part $\|\hat x\|^2 \leq \|\hat x\|^2+\|\hat x^\perp\|^2 = \|q\|^2$. Ainsi, toute solution
$\qh$ repose bien dans l'espace colonne de $\Xi^T$.

Cette observation (qui correspond en fait au célèbre théorème de la représentation) peut
se révéler très utile car elle permet de changer le domaine d'optimisation de $\X$ à
$\Re^n$ par l'identité $\qh = \Xi^T\hat\alpha$. Autrement dit, le problème d'optimisation devient
\begin{equation}
  \maximizeEquation[\alpha \in \Re^n]{n^{-1}\sumi u(r_i\,\alpha^T\Xi x_i) - \frac{\lambda}{2}\alpha^T\Xi\Xi^T\alpha.}
\end{equation}
On peut simplifier cette expression en posant $K \coloneqq \Xi\Xi^T \in \Re^{n \times n}$:
{\begin{equation}
  \boxed{
  \maximizeEquation[\alpha \in \Re^n]{n^{-1}\sumi u(r_iK_i\alpha) - \frac{\lambda}{2}\alpha^TK\alpha,}}
\end{equation}
\vspace{-\baselineskip}\captionof*{figure}{\textit{Formulation duale I}}}
où $\sum_{j=1}^nx_j^Tx_i = K_i \in \Re^n$ représente la $i$\ieme colonne (ou rangée car
$K$ est alors symétrique) de $K$. En fait $K$ correspond à la \textit{matrice gramienne},
\ie\ la matrice des produits scalaires de toutes les observations $x_i$.

Le problème dual, lorsqu'il est exprimé avec une utilité risque neutre, admet une solution
analytique équivalente à la solution primale, mais qui donne une interprétation
supplémentaire au concept de solution optimale. En considérant
\begin{equation}
  \maximizeEquation[\alpha \in \Re^n]{n^{-1}r^TK\alpha - \frac{\lambda}{2}\alpha^TK\alpha}
\end{equation}
et en faisant l'hypothèse qu'aucune observation $x_i$ n'est de norme nulle (une telle
observation serait de toute façon inutile puisqu'elle n'induirait alors aucune décision
d'investissement, étant donné que $q^Tx_i=0$ pour toute décision $q$), $K$ est alors
définie positive et la solution $\hat\alpha = (\lambda n)^{-1}r$ est unique. Ainsi, la décision
optimale $\qh = \Xi^T\hat \alpha$ peut être conçue comme une moyenne pondérée par le rendement de
chacune des observations.


\subsection{Transformations non linéaires}


Le cas $\Q = \X$ est cependant trop pour rendre compte de certaines géométries de
problème. Il est alors naturel de définir une transformation non linéaire d'une
observation $x \in \X$ par une fonction $\phi:\X\to\phi(\X)$. Par exemple, si
$\X \in \Re$, \ie\ une seule variable de marché est considérée, alors on peut chercher une
solution polynômiale en posant une transformation du genre
\begin{equation}
  \phi:x \mapsto
  \begin{pmatrix}
    1 \\ x \\ x^2 \\ \vdots \\ x^k
  \end{pmatrix}.
\end{equation}
En notant $\inp{\cdot,\cdot}$ le produit scalaire de l'espace $\phi(\X)$, le problème consiste alors
à trouver un vecteur optimal $q \in \Q = \phi(\X)$ de façon à
{\begin{equation}
    \boxed{
      \maximizeEquation[q\in\phi(\X)]{n^{-1}\sumi u(r_i\,\inp{q,\phi(x_i)}) - \frac{\lambda}{2}\|q\|^2.}}
  \end{equation}
  \vspace{-\baselineskip}\captionof*{figure}{\textit{Formulation primale II}}}

Mais puisque le théorème de représentation s'applique encore, $\qh$ peut aussi s'exprimer
comme une combinaison linéaire $\alpha$ des observations $\{\phi(x_1) ,\ldots, \phi(x_n)\}$:
{\begin{equation}
\boxed{
  \maximizeEquation[\alpha \in \Re^n]{n^{-1}\sumi u(r_iK_i\alpha) - \frac{\lambda}{2}\alpha^TK\alpha.}}
\end{equation}
\vspace{-\baselineskip}\captionof*{figure}{\textit{Formulation duale II}}}
Cette fois par contre $K_{ij} = \inp{\phi(x_i),\phi(x_j)}$; chaque élément de $K$ représente le
produit scalaire des éléments $\phi(x_i)$.


\subsection{Fonctions de noyau}

Ainsi, pour toute transformation $\phi:\X\to\phi(\X)$, quelle que soit la dimension de l'espace
$\phi(\X)$, on peut exprimer la décision optimale à partir d'une optimisation sur $n$
dimensions. En outre, ce programme d'optimisation ne dépend plus que du produit scalaire
entre ces observations transformées. De plus, on peut dans bien des cas court-circuiter le
calcul de ce produit scalaire par une fonction \textit{noyau} $\kappa:\X\times\X \to \Re$ telle que
$\kappa(x_i,x_j) = \inp{\phi(x_i),\phi(x_j)}$.

Par exemple, dans l'exemple montré plus haut, il suffirait de poser
\begin{equation}
  \kappa(x_i,x_j) = 1 + x_ix_j + (x_ix_j)^2 + \cdots + (x_ix_j)^k.
\end{equation}
Évidemment, dans un pareil cas le gain est assez faible puisqu'on a uniquement réarrangé
l'ordre des opérations. Mais, il est alors possible de circonvenir complètement la
transformation $\phi$ et de représenter sa non linéarité qu'à partir de $\kappa$.  Par exemple, le
noyau gaussien
\begin{equation}
  \kappa(x_i,x_j) = \exp\left(-\frac{\|x_i-x_j\|^2}{2\sigma^2}\right)
\end{equation}
permet de calculer directement le produit scalaire $\inp{\phi(x_i),\phi(x_j)}$ d'observations
$\phi(x_i)$ et $\phi(x_j)$ de dimension infine.

Choisir adéquatement le noyau est alors une tâche cruciale du modèle puisque tout noyau
$\kappa$ induit une géométrie particulière du modèle; il peut alors être impossible de
déterminer une fonction de décision $q$ pourvue de bonne performance si le noyau ne
correspond pas à la géométrie de la loi de marché $M$.

Il faut par ailleurs imposer une contrainte supplémentaire à la classe des noyaux
possibles. En effet $\kappa(x_i,x_j)$ représente un produit scalaire dans $\phi(\X)$ et est donc
tenu de respecter les popriétés algébriques de celui-ci. En fait, $\kappa$ doit satisfaire
l'inégalité de Cauchy-Schwartz:
\begin{align}
  0 \leq \kappa(x_i,x_j)^2 &= \inp{\phi(x_i),\phi(x_j)}^2\\
               &\leq \|\phi(x_i)\|\|\phi(x_j)\|\\
               &= \kappa(x_i,x_i)\kappa(x_j,x_j).
\end{align}
Le noyau $\kappa$ doit donc être une forme semi-définie positive sur $\X$ (voir encore
\cite{mohri2012foundations}).


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main_kernel"
%%% End:
