\documentclass[11pt,fleqn]{article}

\newcommand{\ts}{\textsuperscript}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage[parfill]{parskip}
\usepackage{bm}

\geometry{letterpaper}

\title{COMP 652 \\ End of Term Project}
\author{Thierry \textsc{Bazier-Matte}}
\date{April 2015}

\begin{document}
\maketitle

\section{Introduction}

The efficient market hypothesis states that there's no way to beat the market. At best, a
portfolio manager can try to reduce the variance on his returns by picking a large number
of stocks, following Markowitz theory. Hence the popular decision to invest in large
index, such as the S\&P500 or the Dow Jones, which provide a long-term assurance of
growing value, without the risk associated with speculation on a single asset.

However, given the current state of technology, where, with terminals like Bloomberg's, it
is trivial to obtain a large historical dataset on a variety of features, of a financial
nature or not, we can reevaluate if it is indeed impossible to beat the market. Such a
dataset could provide precious statistical information and it could be possible to
determine in which proportion does each feature has an impact on the observed results of
the market. 

This project will therefore try to address this interesting problem. We consider the
utility of an investor, that is, how risk-seeking or risk-adverse he is, in order to build
a portfolio reflecting his needs. As will be shown later, it is indeed possible to beat
the average market, especially if the utility is strongly risk-seeking. For more
conservative utilities, the model often reverts back to a much safer portofolio. 

The features used by this model will mostly be of a technical nature, ie. pure historical
financial data. It is however trivial to add any set of features to the model.

\section{Formal Problem}

We consider a machine $m$ that given a historical dataset of features $\{x_{st}\}$ will
return a decision vector $q$ from which we can make investment decision based on new
upcoming feature vectors. 

The investment decision is linear with respect to $q$, ie. $x_{st}^Tq$ is the allocation
of the portfolio to stock $s$. Because we also consider the risk-free asset, ie. a bank
loan, then the return on this decision at time $t+1$ will be
\begin{equation*}
  p_{st} = r_{s,t+1}\,x_{st}^Tq + R_f(1-x_{st}^Tq)
\end{equation*}
where $R_f$ is, for simplification purposes, a constant daily return rate. 

The decision vector $q$ to use will be based on the utility function of an investor. We
suppose this utility function to be of a concave two-pieces linear shape, mathematically
described as 
\begin{equation*}
  U(r) = p-r_c + \min(0,(\beta-1)(p-r_c))
\end{equation*}
where $0<\beta<1$ represents the lower rate at which the utility increase when exposed to
a return higher than $r_c$ (see \figref{fig:utility}), the critical return. Typically,
$r_c$ could be set to $R_f$ to reflect the fact that we would prefer our portfolio to be
at least as profitable as the risk-free rate, but it could be also set to $r_c=0$.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{Utility}
  \caption{Utility function, with $\beta=1/2$ and $r_c=5\%$}
  \label{fig:utility}
\end{figure}

We can then define $q$ to be the optimal vector in the sense that
\begin{equation*}
  q^\star = \frac{1}{N} \max_q \sum_{s,t} U(p_{st}),
\end{equation*}
ie. $q^\star$ is the decision vector maximizing the average utility over all $M$ training
instances $x_{st}$. $M$ would therefore be the number of data points (days in this case)
times the number of features.

This is an unbounded convex optimization problem, and so we must ensure to add a
constraint on the norm of $q$ or to add a $L_2$ regularization term to the objective, so
that a finite solution exists.

\section{Implementation details}

We will now give implementation details on the method used, most notably concerning
features used and a discussion on normalization of features.

We must be careful to have a normalized dataset on which we make our optimization step,
mostly because of the regularization term, but also because the components of $q$ provide
valuable information on the most important features when making a decision. 

Given a dataset matrix $X$ where each row is a data point and each column is a feature, we
can first compute $b$ such that $b$ is the mean of all columns and then subtract each row
by $b$, so that data is centered around its mean. For a given feature, we can then scale
it by its amplitude, so that it lies in a range $[-1,1]$. The resulting dataset will then
be comprised in a $L_1$ hypercube. 

The number features used in this experiment is somewhat restrained, mostly because of
unexpected connectivity issues with a data provider. Nonetheless, dates, closing stock
prices, average volatility on 20 and 50 days, trading volume and different measures of
price average are used. Concerning the price average, we take average over three different
number of days, and these numbers are to be learned via model selection. 

Dates have however been given a special treatment. We consider independently the day of
the week (Monday, Tuesday, etc.) as well as the week number. However, instead of simply
taking a binary classifier (1 on Monday, 0 otherwise), it is supposed that an exponential
term reaching 1 on the current day, then falling back to 0 on the next day would be more
appropriate. The same was also done for the week numbers. The exponnential rate is again a
parameter than can be estimated via model selection. If the exponential rate term is big
enough, we fall back in the binary situation (see \figref{fig:weekfeature}).

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{WeekFeature}
  \caption{Impact of each week for feature indicating week \#30}
  \label{fig:weekfeature}
\end{figure}

Of course, a bias term of $1$ must also be added to the dataset matrix. 


\section{Experimental Results}





\end{document}
