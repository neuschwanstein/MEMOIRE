\documentclass[11pt,fleqn]{article}

\newcommand{\ts}{\textsuperscript}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage[parfill]{parskip}
\usepackage{bm}
\usepackage{url}

\geometry{letterpaper}

\title{COMP 652 \\ End of Term Project}
\author{Thierry \textsc{Bazier-Matte}}
\date{April 2015}

\begin{document}
\maketitle

\section{Introduction}

The efficient market hypothesis states that there's no way to beat the market in the long
run. At best, a portfolio manager can try to reduce the variance on his returns by picking
a large number of stocks, following Markowitz theory. Hence the popular decision to invest
in large index, such as the S\&P500 or the Dow Jones, which provide a long-term assurance
of growing value, without the risk associated with speculation on a single asset.

However, given the current state of technology, where it is trivial using a Bloomberg
terminal to get quickly a large historical dataset of financial features, both `technical'
(ie. mean returns, mean volatility) and `fundamental' (accounting ratios), or using data
mining scripts to get more `soft' features (news feed, Twitter activity and such) off the
web.  It is therefore worthwile to reevaluate if it is indeed impossible to beat the
market. Such a dataset could provide precious statistical information and it could be
possible to determine in which proportion does each feature has an impact on the observed
results of the market.

This project will therefore try to address this interesting problem. We consider the
utility of an investor, that is, how risk-seeking or risk-adverse he is, in order to build
a portfolio reflecting his needs. As will be shown later, it is indeed possible to beat
the average market, especially if the utility is strongly risk-seeking. For more
conservative utilities, the model often reverts back to a much safer portofolio. 

The features used by this model will mostly be of a technical nature, ie. pure historical
financial data. It is however easy to add any set of features to the model.

\section{Formal Problem}

We consider a machine $m$ that given a historical dataset of features $\{x_{st}\}$ will
return a decision vector $q$ from which we can make investment decision based on new
upcoming feature vectors. 

The investment decision is linear with respect to $q$, ie. $x_{st}^Tq$ is the allocation
of the portfolio to stock $s$. Because we also consider the risk-free asset, ie. a bank
loan, then the return on this decision at time $t+1$ will be
\begin{equation*}
  p_{st}(q) = r_{s,t+1}\,x_{st}^Tq + R_f(1-x_{st}^Tq)
\end{equation*}
where $R_f$ is, for simplification purposes, a constant daily return rate. 

The decision vector $q$ to use will be based on the utility function of an investor. We
suppose this utility function to be of a concave two-pieces linear shape, mathematically
described as 
\begin{equation*}
  U(p) = p-r_c + \min(0,(\beta-1)(p-r_c))
\end{equation*}
where $0<\beta<1$ represents the lower rate at which the utility increase when exposed to
a return higher than $r_c$ (see \figref{fig:utility}), the critical return. Typically,
$r_c$ could be set to $R_f$ to reflect the fact that we would prefer our portfolio to be
at least as profitable as the risk-free rate, but it could be also set to $r_c=0$. Both
$\beta$ and $r_c$ are parameters specified by the `end user' of the model.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{Utility}
  \caption{Utility function, with $\beta=1/2$ and $r_c=5\%$}
  \label{fig:utility}
\end{figure}

We can then define $q$ to be the optimal vector in the sense that
\begin{equation*}
  q^\star = \frac{1}{N} \max_q \sum_{s,t} U(p_{st}(q)),
\end{equation*}
ie. $q^\star$ is the decision vector maximizing the average utility over all $M$ training
instances $x_{st}$. $M$ would therefore be the number of data points (days in this case)
times the number of features.

We can also add a regularization term to the expression. Such a term becomes useful when
dealing with a large number of features, as it automatically performs feature selection:
\begin{equation*}
  q^\star = \frac{1}{N} \max_q \sum_{s,t} U(p_{st}(q)) + \lambda \|q\|_2^2
\end{equation*}

This is an unbounded convex optimization problem, but the presence of the regularization
term ensures that a finite solution exists. If $\lambda=0$, a constraint 
\begin{equation*}
  \|q\|_2^2\leq1
\end{equation*}
can be added.

\section{Implementation details}

\subsection{Features and Scaling}

We will now give implementation details on the method used, most notably concerning
features used and a discussion on normalization of features.

We must be careful to have a normalized dataset on which we make our optimization step,
mostly because of the regularization term, but also because the components of $q$ provide
valuable information on the most important features when making a decision. 

Given a dataset matrix $X$ where each row is a data point and each column is a feature, we
can first compute $b$ such that $b$ is the mean of all columns and then subtract each row
by $b$, so that data is centered around its mean. For a given feature, we can then scale
it by its amplitude, so that it lies in a range $[-1,1]$. The resulting dataset will then
be comprised in a $L_1$ hypercube. 

The number features used in this experiment is somewhat restrained, mostly because of
unexpected connectivity issues with a data provider. Nonetheless, dates, closing stock
prices, average volatility on 20 and 50 days, trading volume and different measures of
price average are used. Concerning the price average, we take average over three different
number of days, and these numbers are to be learned via model selection. 

Dates have however been given a special treatment. We consider independently the day of
the week (Monday, Tuesday, etc.) as well as the week number. However, instead of simply
taking a binary classifier (1 on Monday, 0 otherwise), it is supposed that an exponential
term reaching 1 on the current day, then falling back to 0 on the next day would be more
appropriate. The same was also done for the week numbers. The exponnential rate is again a
parameter than can be estimated via model selection. If the exponential rate term is big
enough, we fall back in the binary situation (see \figref{fig:weekfeature}).

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{WeekFeature}
  \caption{Exponential impact of each week for feature indicating week \#30 using
    exponential rate $\mu=0.1$}
  \label{fig:weekfeature}
\end{figure}

Of course, a bias term of $1$ must also be added to the dataset matrix. 


\subsection{Portfolio Construction Algorithm}

Because $x_{st}^Tq$ can also be interpreted as the `confidence' we have in stock $s$ on
date $t$, during the training period, we chose among the different available stocks which
one has the higher confidence value and decide to invest in it. Therefore, the chosen
stock from which the portfolio is made is different on each day. That said, many more
strategies could be tested out to chose the `right' portfolio on each day. This is a huge
direction for future work.


\subsection{Training, Validation and Testing Samples}

The fact that we deal with historical data is very important in the testing design of the
program. First and foremost, the decision vector $q$ must imperatively be trained on data
point prior to the test points, or obvious bias would occur. Our model also relies on a
validation (or calibration) sample set that must be temprally located between the training
and the testing set.

Another point must also be considered. It is likely than if $q$ is trained on
`well-behaved' periods of growth, it might tend to be overly optimistic in the future,
therefore it is desirable to include in the training and the calibration samples periods
with different economic situations, and so it might be hard to really evaluate how the
model performs in times of high volatility (although `mini'-crash of a few days happen
relatively often, but their effect can be hard to detect in the long run).

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{DowJonesReturns}
  \caption{Dow Jones returns 2000-2015}
  \label{fig:dowjonesreturns}
\end{figure}

As shown in \figref{fig:dowjonesreturns}, the Dow Jones has experienced two significant
periods of instability in the 21st century, first around 2000 until almost 2003 (the
so-called dot-com bubble) and the 2008-2009 crisis. Therefore, by chosing a training
sample from 2000 to 2008 and a calibration period ranging from 2009 to 2012, both sets get
to have a sample of rougher and smoother returns. Ideally this process would also be
learned, although at this moment it is quite heuristic. In any case this leaves the
2013-2015 period for testing. This is in some way unfortunate since this period has been
remarkably stable, especially on american markets.


\subsection{Calibration}

Calibration takes a big part in this model. The exponential rates $\mu$ discussed above
for each temporal features (52 week features and 5 day features) could have been chosen
independantly one of each other, but we made the choice of taking the weekly $\mu_w$ equal
to each other, likewise for the day $\mu_d$. This restriction could easily be lifted by
doing optimization on the whole set of variables, however considering that each iteration
of the optimization involve a fairly big underlying convex optimization process, it is
easier to optimize on a single variable.

The optimization on the three features of mean past returns of past $n_1$, $n_2$ and $n_3$
days, we first optimize on $n_1$ and set $n_2=n_3=n_1$. We optimize for $n_2$ with the
constraint that $n_2>n_1$. Again we set $n_3=n_2$ and we repeat. This process could of
course be extended to as many different $n$ as wanted/needed. 

Finally, the $\lambda$ regularization coefficient is optimized in a regular fashion by
increasing it until the objective starts decreasing. 

In all cases, the objective of the obtimzation parts is simply the mean utility on the
calibration set.



\subsection{Technical Notes}

Finally, a word about the software used. Most of the code is running on Mathematica v10,
but the optimization part is carried out by CVX\footnote{\url{http://cvxr.com/cvx/}} on
Matlab R2014a. Data is provided with Mathematica financial API, which itself is a proxy to
Yahoo Finance API. 


\section{Experimental Results}

The test sample used was the historical data for years running from 2000 to 2015 on each
stock currently member of the Dow Jones Industrial Average\footnote{Tickers of the Dow
  Jones members are the following: MMM, AXP, AAPL, BA, CAT, CVX, CSCO, KO, DD, XOM, GE,
  GS, HD, INTC, IBM, JNJ, JPM, MCD, MRK, MSFT, NKE, PFE, PG, TRV, UNH, UTX, VZ, V, WMT and
  DIS.  However V (Visa) was removed from the training set since it went public only in
  2008.} The learning, validation and testing sets were setup as described in the previous
section. 

Unfortunately, problems at the calibration steps were encountered, where the algorithms
used to maximize the utility on the calibration sample would not converge fast
enough. Besides, empirical observation tend to suggest that even large change in those
parameters did not seriously affect the mean utility.

That being said, interesting results were nonetheless obtained. \figref{fig:results}
present the cumulative results on the test set using different valus of $\beta$, with
$r_c=0$. It is only when $\beta=1$, ie. when the utility is risk-neutral (described by a
straight line) that we get truly interesting results, and the model even returns slightly
better results than those obtained by a simple buy-and-hold on each component of the
index. For other values of $\beta$ the result quickly fall back to the risk-free
portfolio.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{Results}
  \caption{Returns for various values of $\beta$. The blue line is for $\beta=1$ and the
    orange line corresponds to a buy-and-hold policy over the index.}
  \label{fig:results}
\end{figure}

However, as soon as $\beta$ decreases, the $q$ decision vector becomes much more
conservative, with a big portion of the portfolio allocated to the risk-free asset, even
though at each day the stock providing the highest allocation proportion is picked. 

\figref{fig:pickedstocks} shows the stocks that were picked on each day by the
model. Interestingly enough, only a handful among the 29 available were ever picked. 

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{PickedStocks}
  \caption{Picked stocks for $\beta=1$}
  \label{fig:pickedstocks}
\end{figure}


\section{Conclusion and Future work}

Obviously, if the model described in this document exhibits interesting properties, such
as beating the market in a risk-neutral environment, it nonetheless suffers from the lack
of features. Only financial features of technical nature have been considered, and adding
fundamental features would most certainly help drastically the model. 

The point of the `big-data' hypothesis is also to consider features whose impact on the
stock returns might not be obvious, but they can also be integrated to the model without
problem. Again this is could probably be a big improvement to the current state of the
model. Besides, the point of regularizing the $q$ decision vector is precisely to tell
which features are truly important to the model. 

There's also the question of the portfolio construction algorithm. This model only
considered a very simple and naive implementation, but much more sophisticated method
could certainly be implemented. 

Finally, the article [1] on which this project was based provide interesting bounds on
result that might be exploited to reduce the uncertainty of the results.

One last problem with this model is of a pratical nature. A portfolio manager switching
his assets on every day would most likely incur very high transaction costs and it would
be very important to include this reality in the constraints of the optimization, or
during the reallocation, where we could for example refrain from switching asset if the
one already held had a value $x_{st}^Tq$ high enough. At this level, much sophistication
could also be added.

However, despite all its flaws, this model still demonstrated that with few features, it
seems possible, as far as our tests goes, to derive abnormal results on a portfolio, and
in an automatic fashion, which is pretty exciting! 

\paragraph{Note}

Implementation of the project can be found online: \url{https://github.com/neuschwanstein/ProjetC}


\begin{thebibliography}{9}
  
\bibitem{Rudin2015}
  Cynthia Rudin and Gah-Yi Vahn. \emph{The Big Data Newsvendor: Pratical
    Insights from Machine Learning}, Working
  Paper. \url{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2559116}{\textsf{Link to
      online version}}.

\bibitem{Wiley2007}
 Fabozzi et al. \emph{Robust Portfolio Optimization and Management}, Wiley 2007.
\end{thebibliography}

\end{document}
