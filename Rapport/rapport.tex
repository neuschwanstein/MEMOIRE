\documentclass[11pt,fleqn]{article}

\newcommand{\ts}{\textsuperscript}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage[parfill]{parskip}
\usepackage{bm}
\usepackage{url}

\geometry{letterpaper}

\title{COMP 652 \\ End of Term Project}
\author{Thierry \textsc{Bazier-Matte}}
\date{April 2015}

\begin{document}
\maketitle

\section{Introduction}

The efficient market hypothesis states that there's no way to beat the market. At best, a
portfolio manager can try to reduce the variance on his returns by picking a large number
of stocks, following Markowitz theory. Hence the popular decision to invest in large
index, such as the S\&P500 or the Dow Jones, which provide a long-term assurance of
growing value, without the risk associated with speculation on a single asset.

However, given the current state of technology, where, with terminals like Bloomberg's, it
is trivial to obtain a large historical dataset on a variety of features, of a financial
nature or not, we can reevaluate if it is indeed impossible to beat the market. Such a
dataset could provide precious statistical information and it could be possible to
determine in which proportion does each feature has an impact on the observed results of
the market. 

This project will therefore try to address this interesting problem. We consider the
utility of an investor, that is, how risk-seeking or risk-adverse he is, in order to build
a portfolio reflecting his needs. As will be shown later, it is indeed possible to beat
the average market, especially if the utility is strongly risk-seeking. For more
conservative utilities, the model often reverts back to a much safer portofolio. 

The features used by this model will mostly be of a technical nature, ie. pure historical
financial data. It is however trivial to add any set of features to the model.

\section{Formal Problem}

We consider a machine $m$ that given a historical dataset of features $\{x_{st}\}$ will
return a decision vector $q$ from which we can make investment decision based on new
upcoming feature vectors. 

The investment decision is linear with respect to $q$, ie. $x_{st}^Tq$ is the allocation
of the portfolio to stock $s$. Because we also consider the risk-free asset, ie. a bank
loan, then the return on this decision at time $t+1$ will be
\begin{equation*}
  p_{st} = r_{s,t+1}\,x_{st}^Tq + R_f(1-x_{st}^Tq)
\end{equation*}
where $R_f$ is, for simplification purposes, a constant daily return rate. 

The decision vector $q$ to use will be based on the utility function of an investor. We
suppose this utility function to be of a concave two-pieces linear shape, mathematically
described as 
\begin{equation*}
  U(r) = p-r_c + \min(0,(\beta-1)(p-r_c))
\end{equation*}
where $0<\beta<1$ represents the lower rate at which the utility increase when exposed to
a return higher than $r_c$ (see \figref{fig:utility}), the critical return. Typically,
$r_c$ could be set to $R_f$ to reflect the fact that we would prefer our portfolio to be
at least as profitable as the risk-free rate, but it could be also set to $r_c=0$.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{Utility}
  \caption{Utility function, with $\beta=1/2$ and $r_c=5\%$}
  \label{fig:utility}
\end{figure}

We can then define $q$ to be the optimal vector in the sense that
\begin{equation*}
  q^\star = \frac{1}{N} \max_q \sum_{s,t} U(p_{st}),
\end{equation*}
ie. $q^\star$ is the decision vector maximizing the average utility over all $M$ training
instances $x_{st}$. $M$ would therefore be the number of data points (days in this case)
times the number of features.

This is an unbounded convex optimization problem, and so we must ensure to add a
constraint on the norm of $q$ or to add a $L_2$ regularization term to the objective, so
that a finite solution exists.

\section{Implementation details}

\subsection{Features and Scaling}

We will now give implementation details on the method used, most notably concerning
features used and a discussion on normalization of features.

We must be careful to have a normalized dataset on which we make our optimization step,
mostly because of the regularization term, but also because the components of $q$ provide
valuable information on the most important features when making a decision. 

Given a dataset matrix $X$ where each row is a data point and each column is a feature, we
can first compute $b$ such that $b$ is the mean of all columns and then subtract each row
by $b$, so that data is centered around its mean. For a given feature, we can then scale
it by its amplitude, so that it lies in a range $[-1,1]$. The resulting dataset will then
be comprised in a $L_1$ hypercube. 

The number features used in this experiment is somewhat restrained, mostly because of
unexpected connectivity issues with a data provider. Nonetheless, dates, closing stock
prices, average volatility on 20 and 50 days, trading volume and different measures of
price average are used. Concerning the price average, we take average over three different
number of days, and these numbers are to be learned via model selection. 

Dates have however been given a special treatment. We consider independently the day of
the week (Monday, Tuesday, etc.) as well as the week number. However, instead of simply
taking a binary classifier (1 on Monday, 0 otherwise), it is supposed that an exponential
term reaching 1 on the current day, then falling back to 0 on the next day would be more
appropriate. The same was also done for the week numbers. The exponnential rate is again a
parameter than can be estimated via model selection. If the exponential rate term is big
enough, we fall back in the binary situation (see \figref{fig:weekfeature}).

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{WeekFeature}
  \caption{Exponential impact of each week for feature indicating week \#30 using
    exponential rate $\mu=0.1$}
  \label{fig:weekfeature}
\end{figure}

Of course, a bias term of $1$ must also be added to the dataset matrix. 


\subsection{Portfolio Construction Algorithm}

Because $x_{st}^Tq$ can also be interpreted as the `confidence' we have in stock $s$ on
date $t$, during the training period, we chose among the different available stocks which
one has the higher confidence value and decide to invest in it. Therefore, the chosen
stock from which the portfolio is made is different on each day. That said, many more
strategies could be tested out to chose the `right' portfolio on each day. Again, this is
a huge direction for future work. 


\subsection{Training, Validation and Testing Samples}

The fact that we deal with historical data is very important in the testing design of the
program. First and foremost, the decision vector $q$ must imperatively be trained on data
point prior to the test points, or obvious bias would occur. Our model also relies on a
validation (or calibration) sample set that must be temprally located between the training
and the testing set.

Another point must also be considered. It is likely than if $q$ is trained on
`well-behaved' periods of growth, it might tend to be overly optimistic in the future,
therefore it is desirable to include in the training and the calibration samples periods
with different economic situations, and so it might be hard to really evaluate how the
model performs in times of high volatility (although `mini'-crash of a few days happen
relatively often, but their effect can be hard to detect in the long run).

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{DowJonesReturns}
  \caption{Dow Jones returns 2000-2015}
  \label{fig:dowjonesreturns}
\end{figure}

As shown in \figref{fig:dowjonesreturns}, the Dow Jones has experienced two significant
periods of instability in the 21st century, first around 2000 until almost 2003 (the
so-called dot-com bubble) and the 2008-2009 crisis. Therefore, by chosing a training
sample from 2000 to 2008 and a calibration period ranging from 2009 to 2012, both sets get
to have a sample of rougher and smoother returns. Ideally this process would also be
learned, although at this moment it is quite heuristic. In any case this leaves the
2013-2015 period for testing. This is in some way unfortunate since this period has been
remarkably stable, especially on american markets.


\subsection{Calibration}

Calibration takes a big part in this model. The exponential rates $\mu$ discussed above
for each temporal features (52 week features and 5 day features) could have been chosen
independantly one of each other, but we made the choice of taking the weekly $\mu_w$ equal
to each other, likewise for the day $\mu_d$. This restriction could easily be lifted by
doing optimization on the whole set of variables, however considering that each iteration
of the optimization involve a fairly big underlying convex optimization process, it is
easier to optimize on a single variable.

The optimization on the three features of mean past returns of past $n_1$, $n_2$ and $n_3$
days, we first optimize on $n_1$ and set $n_2=n_3=n_1$. We optimize for $n_2$ with the
constraint that $n_2>n_3$. Again we set $n_3=n_2$ and we repeat. This process could of
course be extended to as many different $n$ as wanted/needed. 

Finally, the $\lambda$ regularization coefficient is optimized in a regular fashion by
increasing it until the objective starts decreasing. 

In all cases, the objective of the obtimzation parts is simply the mean utility on the
calibration set.



\subsection{Technical Notes}

Finally, a word about the software used. Most of the code is running on Mathematica v10,
but the optimization part is carried out by CVX\footnote{\url{http://cvxr.com/cvx/}} on
Matlab R2014a. Data is provided with Mathematica financial API, which itself is a proxy to
Yahoo Finance API. 


\section{Experimental Results}

The test sample used was the historical data for years running from 2005 to 2011 on each
stock currently member of the Dow Jones Industrial Average\footnote{Tickers of the Dow
  Jones members are the following: MMM, AXP, AAPL, BA, CAT, CVX, CSCO, KO, DD, XOM, GE,
  GS, HD, INTC, IBM, JNJ, JPM, MCD, MRK, MSFT, NKE, PFE, PG, TRV, UNH, UTX, VZ, V, WMT and
  DIS.  However V (Visa) was removed from the training set since it went public only in
  2008.} We can split the dataset matrix in $5$ (almost) equal parts in order to
cross-validate the various parameters given above. 

The test set is constituted from the Dow Jones historical data from 2012 to Jan. 1st
2015. 

\section{Conclusion and Future work}

Future work
 - More features!
 - Portfolio construction
 - Theoretical motivations

\end{document}
