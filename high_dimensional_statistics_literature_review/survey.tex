\documentclass{article}
\include{preamble}

\title{A Short Survey of Generalization and Oracle Bounds Obtained for a Lipschitz
  Objective with Regularization}
\author{Thierry Bazier-Matte}

\begin{document}
\maketitle

\section{Introduction, Notation and Asumptions} 

This document is an attempt at regrouping under a unified notation and assumptions a
number of results from machine learning, statistical learning and high-dimensional
statistics when considering a loss function of the form $(q,z) \mapsto \ell(y\,q^Tx)$,
with $\ell$ a $\gamma$-Lipschitz loss function. This particular form of loss readlily
applies to single-asset portfolio optimization (where one maximizes $u(r\,q^Tx)$ and to
SVM objective (where the loss is given by $(1-y\,q^Tx)_{+}$). We first define a number of
solutions whose properties will be studied in the following sections:
\begin{gather}
  R(q) = \E_{X,Y}\ell(Y\,q^TX)\,;\\
  \hat R(q) = n^{-1}\sum_{i=1}^n \ell(y_i\,q^Tx_i)\,;\\
  \hat q_{2} = \argmin_q\left\{\hat R(q) + \lambda_2\|q\|_2^2\right\}\,;\\
  q_{2}^\star = \argmin_q\left\{R(q) + \lambda_2\|q\|_2^2\right\}\,;\\
  \hat q_{12} = \argmin_q\left\{\hat R(q) + \lambda_1\|q\|_1 +
    \lambda_2\|q\|_2^2\right\}\,;\\
  q_{12}^\star = \argmin_q\left\{R(q) + \lambda_1\|q\|_1 + \lambda_2\|q\|_2^2\right\}\,;
\end{gather}
In particular, we will be interested in establishing generalization bounds $g_1$ and
oracle bounds $g_2$, that is,
\[
  R(\hat q) \leq \hat R(\hat q) + g_1(n,p,\delta)
\]
and
\[
  R(\hat q) \leq R(q^\star) + g_2(n,p,\delta).
\]
In particular, we will try to make the bounds explicitly depending on the sample size $n$,
the dimensionality of the problem $p$ (that is, the cardinality of $X$) and the confidence
level $1-\delta$.

We now formalize a number of assumptions.
\begin{assumption}
  The loss function $\ell$ is $\gamma$-Lipschitz, \ie, $|\ell(z_1)-\ell(-z_2)| \leq
  \gamma|z_1-z_2|$. 
\end{assumption}
\begin{assumption}
  The random variable $Y$ rests on a bounded support, \ie, $|Y|\leq\bar y$.
\end{assumption}
\begin{assumption}
  Every feature vector is standardized, \ie, for any $j\in\{1,\dots,p\}$, $\E X_i = 0$,
  $\Var X_i = 1$. In particular, this implies that $\E X_i^2 = 1$, so that
  $\E \|X\|^2_2 = \sum_{i=1}^p \E X_i^2 = p$.
\end{assumption}
% \begin{assumption}
%   \label{ass3}
%   The euclidean norm of the feature vector $X$ is bounded, \ie, $\pp\{\|X\|_2\leq\xi\} =
%   1$. 
% \end{assumption}
% In particular, Assumption \ref{ass3} will be relaxed in the next section. 


\section{Lemmas}
In this section we prove a number of lemmas whose results will be recurrent in proving
various theorems presented in the following sections.

\begin{lemma}
  \label{lemma1}
  The following bound holds with probability $1-\delta_X$:
  \[
    \|X\|_2^2 \leq \frac{p}{\delta_X}.
  \]
  Let $\xi^2 = O(p)$ denote this bound. 
\end{lemma}
\begin{proof}
  The result simply follows from the fact that $\E\|X\|^2_2 = p$ and by applying Markov's
  inequality. 
\end{proof}

\begin{rem}
  Note that this result can be made more precise if one has a better understanding of the
  features. For example, if one considers only bounded features, or only subgaussian
  features then Hoeffding's or Bernstein's inequalities apply \todo{Is it really
    Bernstein?}, thus providing tighter guarantees. However, we want to stress that the
  $\|X\|^2_2 = O(p)$ is the important result to take away from Lemma \ref{lemma1}.
\end{rem}

\begin{lemma}
  \label{lemma2}
  Let $\hat q$ denote either $\hat q_{2}$ or $\hat q_{12}$. Then with probability
  $1-\delta_X$, the following bound holds:
  \[
    \|\hat q\|_2 \leq \frac{\gamma\bar y\xi}{2\lambda_2}.
  \]
  Let $B_{q} = O(\sqrt{p})$ denote this bound. 
\end{lemma}

\begin{coro}
  \label{coro1}
  The loss suffered from applying either $\qh_2$ or $\qh_{12}$ is bounded by
  \[
    \ell(y\,\qh^Tx) \leq \gamma\bar y B_q \xi.
  \]
  Let $B_\ell = O(p)$ denote this bound.
\end{coro}
\todo{Add proof.}


\begin{lemma}
  \label{lemma3}
  This lemma concerns specifically the $\hat q_2$ case. Let
  $R_\lambda(q) = R(q) + \lambda\|q\|_2^2$ and let $\qsl$ be the theoretical
  minimizer of $R_\lambda$. Then $R_\lambda$ is $2\lambda$-strongly convex, \ie,
  \[
    \lambda\|q - \qsl\|_2^2 \leq R_\lambda(q) - R_\lambda(\qsl)
  \]
  Furthermore, if there exists a function $g$ such that
  \[
    R_\lambda(\qh) - R_\lambda(\qsl) \leq g(n,p,\delta),
  \]
  then we have the following oracle bound on $R$:
  \[
    R(\qh) \leq R(\qs) + \lambda\|\qs\|^2_2 + 2g + 2\lambda B_{q_2}\sqrt{g/\lambda}.
  \]
\end{lemma}
\begin{proof}
  First note that from the second hypothesis and the triangle inequality we have
  \[
    R(\qh) - R(\qsl) \leq g + \lambda\left(\|\qsl\|^2_2 - \|\qh\|^2_2\right) \leq g +
    \lambda\left(2\|\qh\|_2\|\qsl-\qh\|_2 + \|\qsl-\qh\|^2_2\right).
  \]
  Next, the second hypothesis combined with the first one yields that $\|\qh - \qsl\|_2
  \leq \sqrt{g/\lambda}$. Also, Lemma \ref{lemma2} implies that $\|\qh\|_2$ and
  $\|\qsl\|_2$ are bounded by $B_{q_2}$, so therefore
  \[
    R(\qh) - R(\qsl) \leq 2g + 2\lambda B_{q_2}\sqrt{g/\lambda}.
  \]
  Next, by definition of $\qsl$, we have
  \[
    R(\qsl) + \lambda\|\qsl\|_2^2 \leq R(\qs) + \lambda\|\qs\|_2^2,
  \]
  it follows that
  \[
    R(\qsl) - R(\qs) \leq \lambda\|\qs\|_2^2 - \lambda\|\qsl\|_2^2 \leq \lambda\|\qs\|^2_2,
  \]
  which combined to the equality
  \[
    R(\qh) = R(\qs) + R(\qh) - R(\qsl) + R(\qsl) - R(\qs)
  \]
  yields the claimed result.
\end{proof}


\section{Generalization Bounds}

This section applies for any $\qh$, although because $\|\hat q_1\|$ is possibly unbounded,
the results presented are only finite when $\qh$ is either $\qh_2$ or $\qh_{12}$.


\subsection{Pseudo-Dimension Bound}

The first generalization bound relies on the concept of pseudo-dimension $\Pdim$ defined
for a family of functions. Even though we shall not define it precisely, we will make use
of some theorems. For references on the concept, please consult
\cite{mohri2012foundations,anthony2009neural,vidyasagar2013learning}.

First, some definitions are in order.

\begin{definition}
  The family of linear decisions $\mathcal{Q}$ is the following set of functions:
  \[
    \mathcal{Q} = \{q:(\X,\Y)\to\real\,|\,q(x,y) = y\,q^Tx\}.
  \]
\end{definition}

\begin{rem}
  In particular, $\qh^T\in\mathcal{Q}$. 
\end{rem}

\begin{definition}
  The familiy of losses $\mathcal{L}$ associated to $\mathcal{Q}$ is the following set of
  functions:
  \[
    \mathcal{L} = \{\ell_q:(\X,\Y)\to \real\,|\,\ell_q(x,y) = \ell(q(x,y))\}.
  \]
\end{definition}

\begin{prop}
  $\Pdim(\mathcal{Q}) = p+1$.
\end{prop}
\begin{proof}
  Theorem 10.4 from \cite{mohri2012foundations} indicates that the family of hyperplanes
  in $\real^m$, \ie,
  \[
     \mathcal{W} = \{x\mapsto w^Tx\}
   \]
   has $\Pdim(\mathcal{W}) = m+1$. But $\mathcal{Q}$ can also be considered as the family
   of hyperplanes since it only differs by a scaling factor of $y$. This yields the
   claimed result.
\end{proof}

\begin{prop}
  $\Pdim(\mathcal{L}) = p+1$.
\end{prop}
\begin{proof}
  We see that $\mathcal{L} = \ell \circ \mathcal{Q}$. But by Exercise 10.1 of
  \cite{mohri2012foundations}, since $\ell$ is monotonic, the result follows. 
\end{proof}

\begin{thm}
  \label{thm1}
  Let $\qh$ be either $\qh_2$ or $\qh_{12}$. Then the following bound holds with
  probability  $1-\delta$:
  \begin{align*}
    R(\hat q) &\leq \hat R(\hat q) + B_{q_2}\left(\sqrt{\frac{2p\log(en/p)}{n}} +
                \sqrt{\frac{\log(1/\delta)}{2n}}\right)\\
    &\leq \hat R(\qh) + O\left(\frac{p\sqrt{p\log(n/p)}}{n}\right),
  \end{align*}
  with $e$ the Euler constant. 
\end{thm}

\begin{proof}
  See Theorem 10.6 from \cite{mohri2012foundations} in conjunction with the two previous propositions. 
\end{proof}

\subsection{Rademacher Bound}

Here again we will use the concept of Rademacher complexity $\rad_n$ without defining it.

Let $\mathcal{F}_{\tilde{\mathcal{Q}}}$ be a slightly different family of functions taking
in their input only from the $\X$ space:
\[
  \mathcal{F}_{\tilde{\mathcal{Q}}} = \{x\mapsto q^Tx:q\in\tilde{\mathcal{Q}}\},
\]
and
\[
  \tilde{\mathcal{Q}} = \{q \in \real^p:\|q\|_2 \leq B_q\}.
\]

We will use results from \cite{kakade2009complexity} and \cite{bartlett2002rademacher} in
order to derive our next bound.

\begin{lemma*}
  \label{rad_lemma}
  The Rademacher complexity of $\tilde{\mathcal{Q}}$ is bounded:
  \[
    \rad_n(\mathcal{F}_{\tilde{\mathcal{Q}}}) \leq \xi B_q\sqrt{\frac{1}{n}}.
    % \frac{\gamma\bar X^2\bar r}{\lambda}\sqrt{\frac{1}{n}}.
  \]
\end{lemma*}
\begin{proof}
  This results comes from Theorem 3 in \cite{kakade2009complexity}. Here is a verbatim
  transcription of the theorem.
  \begin{thm*}
    Let $\mathcal{F}_{\mathcal{W}} = \{x\mapsto\langle w,x\rangle: w\in\mathcal{W}\}$.
    Let $S$ be a closed convex set and let $F:S\to\real$ be a $\sigma$-strongly convex
    w.r.t. $\|\cdot\|_{\star}$ s.t. $\inf_{w\in S}F(w)=0$. Further, let
    $\X = \{x:\|x\|\leq X\}$. Define $\mathcal{W} = \{w\in S: F(w) \leq
    W^2_\star\}$. Then, we have
    \[
      \rad_n(\mathcal{F}_{\mathcal{W}}) \leq XW_\star\sqrt{\frac{2}{\sigma n}}.
    \]
  \end{thm*}

  We can directly apply the above theorem to our case by taking $F = \|\cdot\|^2$ and
  noticing it is $2$-strongly convex. Therefore, we have
  $\mathcal{W} = \tilde{\mathcal{Q}}$ if we set $W_\star^2=B_q^2$, which leads to the
  claimed result.
\end{proof}

Next, we inoke generalization theorem proved by Bartlett and Mendelson in 2002
\cite{bartlett2002rademacher}.

\begin{thm}
  \label{thm2}
  With probability $1-\delta$, the following bound holds:
  \begin{align*}
    R(\qh) &\leq \hat R(\qh) + \frac{2\gamma\xi B_q}{\sqrt{n}} +
             B_\ell\sqrt{\frac{\log(1/\delta)}{2n}}\\
    &\leq \hat R(\qh) + O\left(\frac{p}{\sqrt{n}}\right).
  \end{align*}
\end{thm}
\begin{proof}
  See Theorem 1 in \cite{kakade2009complexity} in addition to Corollary \ref{coro1}.
\end{proof}


\subsection{Stability Bound}

The next bound was introduced by \cite{bousquet2002stability}.

\begin{lemma*}
  \label{beta_bound}
  The $\beta$-stability of the ERM algorithm leading to $\hat q$ is bounded:
  \[
    \beta \leq \frac{(\gamma\bar y\xi)^2}{\lambda_2 n} = O\left(\frac{p}{n}\right).
  \]
\end{lemma*}
\begin{proof}
  See Proposition 11.1 in \cite{mohri2012foundations} with $\sigma = \gamma\bar y$. 
\end{proof}

\begin{thm}
  With probability $1-\delta$, the following bound holds:
  \begin{align*}
    R(\hat q) &\leq \hat R(\hat q) + \beta + (2n\beta +
                B_\ell)\sqrt{\frac{log(1/\delta)}{2n}}\\
              &\leq \hat R(\qh) + O\left(\frac{p}{\sqrt{n}}\right).
  \end{align*}
\end{thm}
\begin{proof}
  Directly by applying Theorem 11.1 from \cite{mohri2012foundations} with the above lemma
  and Corollary \ref{coro1}.
\end{proof}


\section{Oracle Bounds}

\subsection{Fast Rate Bound}

The next bound is due to \cite{sridharan2009fast}. The ``fast'' rate means that
$\qh\to\qsl$ at a $O(1/n)$ rate due to the strong convexity of the regularizer.

\begin{thm}
  The following bound holds with probability $1-\delta$:
  \begin{align*}
    R(\qh) &\leq R(\qs) + \lambda\|\qs\|^2_2 +
    \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n}
             + 8\gamma\lambda\xi B_q \sqrt{\frac{32+\log(1/\delta)}{\lambda n}}\\
           &\leq R(\qs) + \lambda\|\qs\|^2_2 + O\left(\frac{p}{\sqrt{n}}\right).
  \end{align*}
\end{thm}

\begin{proof}
  As shown in Theorem 1 of \cite{sridharan2009fast}, the following bound holds with
  probability $1-\delta$:
  \[
    R_\lambda(\qh) \leq R_\lambda(\qsl) + \frac{4\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n}.
  \]
  Applying the result of Lemma \ref{lemma3} yields the result. 
\end{proof}

\subsection{Empirical Process Bound}

The next bound has been developed using the machinery developed for the high dimensional
statistical theory. Note that unlike the previous bounds where the $p$ dependancy was a
consequence of $\xi = O(\sqrt{p})$, the dependancy on $p$ relies on the so-called
contraction inequality. Another particularity of the bound is its looseness: whereas
previous bounds holded with exponential confidence \todo{Rephrase.}, this bound is
actually concerns the expectation of the suboptimality, and Markov's inequality only
yields a weak bound.

\begin{thm}
  The following bound holds with probability $1-\delta$:
  \begin{align*}
    R(\qh) &\leq R(\qs) + \lambda\|\qs\|_2^2 +
             \frac{512\gamma^2}{\lambda\delta^2}\frac{p}{n} + 32\lambda
             B_q\frac{\gamma}{\delta\sqrt{\lambda}}\sqrt{\frac{p}{n}}\\
    &\leq R(\qs) + \lambda\|\qs\|_2^2 + \O{\frac{p}{\sqrt{n}}}.
  \end{align*}
\end{thm}

\begin{proof}
  Follows from the quadratic margin of $\exc_2$ (with constant $\lambda_2$) around
  $q^\star_2$, Lemma 6.6 and Lemma 14.19 in \cite{buhlmann2011statistics} in conjunction
  with Markov's inequality, followed by Lemma \ref{lemma3}. \todo{More details?}
\end{proof}

% Note that the $O(1/n)$ rate toward in the suboptimality is consistent with Shai Shalev
% \todo{Ref needed.} \Thierry{Consistent also with nips draft establishing $p/\sqrt{n}$
%   toward the ``true'' $q^\star$}. However, the $p$ value in the numerator makes explicit
% the dependency on the dimensionality of the problem.

\subsection{Elastic Net Penalization}

Although high dimensional statistics was developed with a ``fixed number of \textit{real}
features, high number of unimportant features'' philosophy (especially useful in certain
fields like biology), we can use its results on the solution $\qh_{12}$, but we have to
suppose that all considered features are useful. This leads to a worsening of $\O{\log p}$
of previous bounds. This is because, by considering $\qsl$ as the objective, we know that
none of its components is zero, almost surely. Although we could also consider a single
linear penalty (thus leading to a lasso generalized linear model), our lack of knowledge
of curvature at $q^\star$ would yield a bound with unknown constant. 

\begin{thm}
  Let
  \[
    \lambda_1 = 16\gamma\left(4\sqrt{\frac{2\log2p}{n}} +
      \sqrt{\frac{2\log(1/\delta)}{n}}\right).
  \]
  Then the following bound holds with probability $1-\delta$:
  \begin{align*}
    R(\qh) &\leq R(\qsl) + \lambda_2\|\qs\|_2^2 + \frac{32\lambda_1^2p}{\lambda_2} +
             4\lambda_2 B_q\frac{\lambda_1}{\sqrt{\lambda_2}}\sqrt{p}\\
   &\leq R(\qsl) + \lambda_2\|\qs\|^2_2 + \O{\frac{p\log p}{\sqrt{n}}}.
  \end{align*}
\end{thm}
\begin{proof}
  The proof first follows from
  \[
    R(\qh) - R(\qsl) \leq \frac{16\lambda_1 p}{\lambda_2}
  \]
  (Corrolary 6.3 in \cite{buhlmann2011statistics} using quadratic margin with constant
  $c=\lambda_2$) in conjunction to Example 14.2 also in \cite{buhlmann2011statistics}.
\end{proof}

\bibliographystyle{abbrv}
\bibliography{bibliography2}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
