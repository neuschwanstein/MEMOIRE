\section{Model and Main Results}

\subsection{Assumptions and definitions}


\comment{Nécesaire?} In the following, $\bm A$ (capital boldface) are assumed to represent
a real subset of any dimension, $A$ (capital case) represents random variables (or
distributions) and $a$ (lower case) represents deterministic variables or
realizations. $\real$ represents the real set, $\subsetsim$ the support of a random
variable, and $\|\cdot\|$ is the euclidean norm.

Our model considers the market $M$ as being a $p+1$-variate random distribution, with on
its first margin a random (finite) return
$R\subsetsim\bm R=[r_{\min},r_{\max}]\subset\real$ \comment{Il peut être plus simple
  d'avoir $|R|\leq\bar r$, notamment dans l'expression de $\Omega$} and on the other
margin a random vector of features $(X_1,\dots,X_p)$, which would typically represent
financial or economic news, etc. We will assume that all features are pairwise
independant.

We also suppose that the investor is endowed with a monotonically increasing concave
utility function $\bar u:\bm R\to\bm U$, such that $\bar u$ can be rescaled to $u$ with
$\bar u(r) = ku(r) + l$, with the additionnal requirements that $u(0)=0$,
$\lim_{r\to0^+}\grad u(r) = 1$ and that $u$ is $\gamma$-Lipschitz, ie. such that for any
$r_1,r_2\in\bm R$, $|u(r_1) - u(r_2)| \leq \gamma|r_1-r_2|$. For example, any piece-wise
linear utility would fit the Lipschitz requirements.

Our method studies optimal linear investment decisions $q\in\bm Q\subseteq\real^p$ over
the random features so as to maximize the certainty equivalent $\CE(q)$ of the portfolio,
where:
\[
  \CE(q) = u^{-1}(\Psi(q)),
\]
with
\[
  \Psi(q) = \E_M[u(R\,q^TX)]
\]
the out-sample utility of $q$. We would typically add a riskless return rate to the
equation, however we set it to 0 for the sake of simplicity. Additionnally, given a sample
$\{(x_i,r_i)\}_{i=1}^n$ drawn from $M^n$, we will also study the sample certainty
equivalent $\hat\CE$:
\[
  \hat\CE(q) = u^{-1}(\hat\Psi(q)),
\]
with
\[
  \hat\Psi(q) = n^{-1}\sum_{i=1}^n u(r_i\,q^Tx_i)
\]
the in-sample utility of $q$. 

\subsection{Out-sample complexity}

Supposing we have a size $n$ sample drawn \textit{i.i.d.} from the market, then a natural
choice for $q$ would be the optimal solution of the regularized in-sample utility:
\[
  \hat q = \argmax_q\{\hat\Psi(q)-\lambda\|q\|^2\}  = \argmax_q \left\{\frac{1}{n}\sum_{i=1}^n u(r_i\,q^Tx_i) - \lambda\|q\|^2\right\},
\]
where $\lambda\|q\|^2$ is here to avoid overfitting on the training sample. We will
sometimes refer to $\hat q$ as the algorithm mapping from a market sample to the decision
vector, rather than the decision vector itself.

Before going on, we will assume that the random features vector is bounded,
ie. $\|X\|\leq\xi$, although we will relax this hypothesis in the next subsection.

We are now in a position to present a bound on the out-sample error:
\begin{thm}
  With probability $1-\delta$, the error between the in- and out-sample certainty
  equivalent is bounded by the following relation:
  \[
    \CE(\hat q) \geq \hat\CE(\hat q) - \Omega\cdot\nabla u^{-1}(\hat\CE(\hat q)),
  \]
  where
  \[
    \Omega = \frac{(\bar r\xi)^2}{2\lambda} \left(\frac{\gamma^2}{n} + \frac{\gamma(1+3\gamma)}{\sqrt{2n}}\sqrt{\log(1/\delta)}\right).
  \]
In particular, this implies that the error bound shrinks at a $O(1/\sqrt{n})$ rate. 
\end{thm}

Our proof of Theorem 1 proceeds as follow. First, borrowing from the terminology
introduced by \cite{bousquet2002stability}, we show that the algorithm leading to $\hat q$
is $\beta$-stable. We then show that for any $\hat q$ generated from a sample of $M$, the
utility derived from applying this decision will be absolutely bounded, regardless of the
outcome from $M$. These last two conditions can therefore lead to a direct application of
Bousquet-Ellisseef out-sample error bound theorem on the $\bm U$ space. We finally show how
this result can be inverted back to the $\bm R$ space.

\begin{lemma}
  \label{sigma-adm}
  Let $q_1,q_2\in\bm Q$, $x\sim X$ and $r\sim R$. The algorithm generating $\hat q$ has
  $\sigma$-admissiblity of $\gamma\bar r$, ie.
  \[
    |u(r\,q_1^T x) - u(r\,q_2^T x)| \leq \gamma\bar r|q_1^Tx - q_2^Tx|.
  \]
\end{lemma}
\begin{proof}
  This lemma follows trivially from the Lipschitz property of $u$. See Definition 19 from
  \cite{bousquet2002stability} for more details.
\end{proof}

\begin{lemma}
  \label{beta-bound}
  The algorithm generating $\hat q$ has $\beta$-stability, ie. with $s_n \sim M^n$ and
  $s_n'$ differing from $s_n$ by a single resampling from $M$, then, 
  \[
    |u(R\,\hat q(s_n)^T X) - u(R\,\hat q(s_n')^T X)| \leq \beta,
  \]
  where
  \[
    \beta \leq \frac{(\gamma\bar r\xMax)^2}{2\lambda n}.
  \]
\end{lemma}
\begin{proof}
  Using Lemma \ref{sigma-adm}, this follows directly from Theorem 22 in
  \cite{bousquet2002stability}.
\end{proof}

\begin{lemma}
  \label{q-bound}
  The norm of the decision $\hat q$ is bounded:
  \[
    \|\hat q\| \leq \frac{\gamma\bar r\xMax}{2\lambda}.
  \]
\end{lemma}
\begin{proof}
  Let $\{(x,r)\}_n \sim M^n$ be an \textit{i.i.d.} sample of the market. The empirical decision
  algorithm is equivalent to
  \begin{align*}
    \maximizeEquationSt{n^{-1}\sum_{i=1}^n u(r_i\,q^Tx_i) - \lambda s^2}[s\geq0\\,\|q\|=1],
  \end{align*}
  where the optimization variables are now the direction $q$ and the scale $s$. Therefore,
  for any direction $q$, we can define a concave function $g(s)$ which becomes the
  objective:
  \begin{align*}
    \maximizeEquationSt{g(s) = n^{-1}\sum_{i=1}^n u(r_i\,sq^Tx_i) - \lambda s^2}[s\geq 0].
  \end{align*}
  
  Because $g:\real_{+}\to\real$ is concave, we can consider two cases: either the maximum
  is realized at the boundary, ie. $s^\star=0$, or there exists an optimal value
  $s^\star > 0$ such that $\nabla g(s^\star)=0$. To derive a bound on $s^\star$, we can seek a
  value $\bar s$ such that for any $q$, $\nabla{}g(\bar s)<0$ and therefore $s^\star < \bar s$.

  To do so, we first note that 
  \[
    \grad g(s) = n^{-1}\sum_{i=1}^nr_i\,q^Tx_i\,u'(r_i\,sq^Tx_i) - 2\lambda s,
  \]
  bu because $\|q\|=1$, we have $q^Tx_i\leq\|x_i\|\leq\xMax$. We also have
  $r_i\leq \bar r$ and $u'\leq\gamma$, so that
  \[
    \grad g(s) \leq \gamma\bar r\xi - 2\lambda s.
  \]
  Therefore, with
  \[
    \bar s = \frac{\gamma\bar r\xMax}{2\lambda},
  \]
  we have $\grad g(\bar s)\leq 0$. 
\end{proof}

\begin{lemma}
  \label{u-bound}
  For any $(x,r)\sim M$ and any $\hat q$,
  \[
    -\frac{(\gamma\xi \bar r)^2}{2\lambda} \leq u(r\,\hat q^Tx) \leq \frac{\gamma(\xi\bar r)^2}{2\lambda}.
  \]
\end{lemma}

\begin{proof}
  The maximum utility will be realized when $r = \bar r$, so that
  \[
    u(r\,\hat q^T x) \leq r {\hat q}^T x \leq \frac{\gamma(\xi\bar r)^2}{2\lambda},
  \]
  since the identity function bounds $u$ above. Likewise for negative returns, although
  this time $\gamma$ applies. 
\end{proof}

The following theorem was first proven in \cite{bousquet2002stability}, although its
statement is adapted from \cite{mohri2012foundations} and is presented in accordance to
our particular setting.
\begin{thm*}[Bousquet-Ellisseef Outsample Error Theorem]
  Let $s_n=\{(x_i,r_i)\}_{i=1}^n$ by a size $n$ sample drawn \textit{i.i.d.} from $M$. If
  $\hat q$ has $\beta$-stability and
  $\hat u_{\min}\leq u(R\,\hat q^TX) \leq \hat u_{\max}$, then, with probability
  $1-\delta$, 
  \[
    \Psi(\hat q) \geq \hat\Psi(\hat q) - \Omega_u,
  \]
  where
  \[
    \Omega_u = \beta + (2n\beta + (\hat u_{\max}-\hat u_{\min}))\sqrt{\frac{\log(1/\delta)}{2n}}.
  \]
\end{thm*}

Using directly Lemma \ref{beta-bound} and \ref{u-bound}, we therefore find the following outsample
error bound on the utility
\[
  \Omega_u = \frac{(\bar r\xi)^2}{2\lambda} \left(\frac{\gamma^2}{n} + \frac{\gamma(1+3\gamma)}{\sqrt{2n}}\sqrt{\log(1/\delta)}\right).
\]

We now show how to transform this last result on a bound on the CE of the decision. Note
that for any convex function $f$, $f(a+b) \geq f(a) + b\cdot\grad f(a)$. Therefore, from
the out-sample error bounding theorem, we have
\[
  u^{-1}(\Psi(\hat q)) \geq u^{-1}(\hat\Psi(\hat q) - \Omega_u) \geq u^{-1}(\hat\Psi(\hat
  q)) - \Omega_u\cdot\grad(u^{-1})(\hat\Psi(\hat q)),
\]
since $u^{-1}$ is also a monotonic function. This proves Theorem 1. 


\subsection{Big Data Phenomenon}
We now take a closer look on the effect the dimension of the feature space can have on the
bound $\Omega$ stated in Theorem 1, and in particular on the bound $\xi^2$. If we let
$Z^2 = \sum_{i=1}^nX_i^2$ be the random squared norm of $X$, we can show that $Z^2$ is of
the order $O(p)$ with high probabiltiy . This implies that the algorithm $\hat q$ has in
fact a sample complexity $O(p/\sqrt{n})$.

We present three cases, each with additional generalization properties. In what follows,
we will assume with no loss of generality (because it is an affine transformation) that
$\E X_i=0$ and $\Var X_i = 1$, which already implies that $\E X_i^2=1$, and therefore
$\E Z^2=p$.

\comment{Ajouter de l'intuition pour le lecteur.}

Let us first consider the specific case where $X\sim\normal(0,I)$, ie. $X$ is a
$p$-mutlinormal random vector. It then follows that $Z^2\sim\chi^2(p)$. But we know from
\cite{laurent2000adaptive} that a chi-square distribution has the following property for
all $t$:
\[
\pp\{Z^2-p \geq 2\sqrt{pt} + 2t\} \leq e^{-t},
\] 
which is equivalent, with probability $1-\delta$ to:
\[
  Z^2 < p + 2\sqrt{p\log(1/\delta)} + 2\log(1/\delta).
\]

\comment{Tail bound on squared $t$-Student distribution being a $F$-distribution?}

As a somewhat more natural example, without making any asumption on the distribution of
the features, we can consider the case where each of them is bounded, either by truncation
in the pre-processing step or because their support is known to be finite. If
$X_i^2 \leq \nu_i$, and we let $\nu^2_0 = \sum_{i=1}^p \nu_i^2$, then, by Hoeffding's
theorem,
\[
  \pp\{Z^2 - p\geq t\} \leq \exp\left(-\frac{t^2}{\nu_0^2}\right),
\]
which, again, can be reexpressed as the following inequality with probability $1-\delta$:
\[
  Z^2 < p + \nu_0\sqrt{\log(1/\delta)}.
\]
Finally, Markov's inequality provides the most general theorem for the situation, since it
simply states that with probability $1-\delta$, 
\[
  Z^2 < \delta^{-1}p.
\]
\begin{thm}
  With probability $1-(\delta_1+\delta_2)$, the error between the in- and out-sample certainty
  equivalent is bounded by the following relation:
  \[
    \CE(\hat q) \geq \hat\CE(\hat q) - \Omega\cdot\nabla u^{-1}(\hat\CE(\hat q)),
  \]
  where
  \[
    \Omega = \frac{\bar r^2p}{2\lambda\delta_1} \left(\frac{\gamma^2}{n} + \frac{\gamma(1+3\gamma)}{\sqrt{2n}}\sqrt{\log(1/\delta_2)}\right).
  \]
  If the features are bounded, then 
  \[
    \Omega = (p+\nu_0\sqrt{\log(1/\delta_1)})\frac{\bar r^2}{2\lambda} \left(\frac{\gamma^2}{n} + \frac{\gamma(1+3\gamma)}{\sqrt{2n}}\sqrt{\log(1/\delta_2)}\right).
  \]
In particular, this implies that the error bound shrinks at a $O(p/\sqrt{n})$ rate. 
\end{thm}

\subsection{Market efficiency and true optimal}

The last theoretical topic we want to discuss is the behaviour of
$q^\star = \argmax_q\Psi(q)$ the optimal investment decision of $M$. Specifically, we want
to know under which circumstances $\|q^\star\|$ is bounded. This notion relates to the
efficiency of the market. To understand why, consider a risk neutral utility $u(r) =
r$. Then,
\begin{align*}
  \E[u(R\,q^TX)] &= q_1\E[RX_1] + \cdots + q_p\E[RX_p].\\
                 &= q_1\Cov(R,X_1) + \cdots + q_p\Cov(R,X_p),
\end{align*}
since $\E X_i=0$ for all $i$. Therefore, if $\Cov(R,X_i)>0$ set $q_i=\infty$ and if
$\Cov(R,X_i)<0$, set $q_i=-\infty$. Such a decision $q$ will therefore yield an infinite
expected utility. This is clearly unacceptable, and therefore we want to understand the
conditions on $u$ and $M$ that make $q^\star$ bounded.

\begin{lemma}
  Let $u:\real\to\real$ be a monotonically increasing Lipschitz function such that
  $u(0) = 0$ and that there exists $r_M$ such that $\grad u(r) = 0$ for any $r>r_M$. Then,
  for any real random variable $Z$ such that $\pp\{Z<0\}>0$,
  \[
    \argmax_{k>0} \E[u(kZ)].
  \]
\end{lemma}

\begin{proof}
  We first note that $\lim_{k\to\infty}\E[u(kZ)]=-\infty$ is a sufficient condition. Now,
  let $\bar Z$ be such that its distribution function $F_{\bar Z}(r)$ is defined as
  follows:
  \[
    F_{\bar Z}(r) =
    \begin{cases}
      F_Z(r)&r \leq r_M\\
      % \pp\{Z>r_M\}&r = r_M\\
      1 & r>r_M.
    \end{cases}
  \]
  Then, 
  \[
    \E[u(kZ)] = \pp\{kZ \leq r_M\}\E[u(kZ)|kZ\leq r_M] + \pp\{kZ > r_M\}u(r_M) = \E[u(k\bar Z)].
  \]
  Now, by hypothesis, there exists $\delta<0$ such that
  $\pp\{Z<\delta\}=\pp\{\bar Z<\delta\} = p > 0$. Let $B$ be a discrete random variable
  with two states such that $\pp\{B=\delta\} = 1-\pp\{B=r_M\} = p$. Then, by construction
  $\pp\{B\geq r\} \geq \pp\{\bar Z\geq r\}$, so that $\E B \geq \E\bar Z$. This in turn
  implies \comment{reference needed!} that $\E[u(kB)] \geq \E[u(k\bar Z)]$. But,
  \[
    \lim_{k\to\infty}\E[u(kB)] = \lim_{k\to\infty}\big(u(k\delta) + (1-p)u(kr_M)\big) = \infty,
  \]
  since $u_{+}(x) = o(u_{-}(x))$. \comment{Prove the other way?? Also, can it be proven
    only with $r = o(u_{+}(r))$, ie. without asuming the $r_M$?}
\end{proof}

\begin{thm}
  If, for all $1\leq i\leq p$, $\pp\{RX_i<0\}>0$ and $\pp\{RX_i>0\}>0$ and if, in addition
  to previous assumptions on $u$, we suppose that $u$ is bounded in such a way that there
  exists $r_M$ such that $\grad u(r) = 0$ for all $r>r_M$, ie. additionnal returns past
  $r_M$ provide no additional utility, then $\|q^\star\|$ is bounded.
\end{thm}
\begin{proof}
  Let $q\in\real^p$ such that $\|q\|=1$ and $Z=R\,q^TX\subsetsim\real$. By asumption, $XR$
  can take negative values on all its axes and therefore so can the values of $Z$. The
  results of Lemma 5 apply to $Z$, ie. $\argmax_{k>0} \E[u(kZ)]$ is bounded. Because we
  considered an arbitrary $q$, the result of Theorem 3 follows.
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "big_data_portfolio_optimization"
%%% End:
