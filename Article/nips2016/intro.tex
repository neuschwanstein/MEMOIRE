\section{Introduction}
\label{sec:intro}

There is no doubt that modern portfolio management theory has been dramatically affected
by two important historical events. First, Markowitz in 1952 highlighted in his seminal
paper \cite{markowitz1952portfolio} how investment decisions needed to inherently
trade-off between risk (typically measured using variance) and returns (in the form of
expected returns). This was later reinterpreted as a special case of characterizing risk
aversion using expected utility theory \cite{neumann44:tgeb}.  The flexibility of such a
theory has since then been demonstrated in many occasions regarding the wide diversity of
investors' risk aversion that it can represent (see \cite{ingersoll87} and reference
therein for an overview of the type of attitudes that can be modeled).

The second turning point of this theory can be considered to have occurred with the
financial crisis of 2008 which provided strong evidence that the use of statistics such as
variance and value-at-risk, and of distribution models that are calibrated using historical data could
provide a false sense of security \cite{Salmon09}.  In an attempt to address some of these
new challenges, researchers have proposed using more robust statistical estimators
\cite{madan1998variance,goldfarb03,olivares2015robust} while others encouraged the use of
robust portfolio management models that are designed to produce out-of-samples guarantees
by exploiting the use of a confidence region for the distribution of future returns
\cite{delageYe10,huangzhu10,Esfahani15}.
%Delage and Ye, 

In this work, we draw on statistical learning theory to establish what are the
out-of-sample guarantees that can be obtained when using regularization in an expected
utility model that allows to exploit side information about the financial markets (see
\cite{brandt2009parametric} where non-regularized version was introduced).  This side
information could consist of fundamental analysis (as was famously done in
\cite{fama1993common}), but also of technical analysis, of financial news, etc. Overall,
we consider our contribution to be three-fold.
\begin{enumerate}
\item We derive a lower bound on the out-of-sample performance of the investment strategy
  returned by this regularized model.  In this respect, our results differ from the usual
  statistical learning and stability theory results in the sense that our guarantees will
  not be in terms of quality of fit of a model (\eg, expected squared loss, hinge loss,
  etc.), but rather in terms of the actual performance perceived by the investor (through
  the notion of a certainty equivalent).
\item We derive an upper bound on the suboptimality of the investment strategy when
  compared to the optimal strategy that would be derived using the full knowledge of the
  sample distribution. Note that such guarantees have not been established for data-driven
  or distributionally robust optimization.
\item Considering that nowadays a growing amount of side-information can be exploited by
  individuals to make their investments, we establish precisely how these bounds are
  affected at a high-dimensional (or ``big data'') regime.
\end{enumerate}
It is worth mentioning that the above contributions have a similar flavour as those of
\cite{rudin2015big} who applied stability theory to provide generalization bounds for a
newsvendor problem yet we believe that our bounds are more precisely defined and actually
motivate us to correct a misleading statement made by these authors regarding how
regularization can immunize the out-of-sample performance in high-dimensional regimes.

The rest of the paper is divided as follows. First, we formally introduce our model and
assumptions in Section \ref{sec:model}. Section \ref{sec:oos} then presents what kind of
out-of-sample guarantees can be provided on the certainty equivalent (CE) of the investor
using a sample of market returns and side information when assuming a stationary market
distribution. We then proceed in Section \ref{sec:sub} to show that the same kind of
guarantees can also be derived for the CE suboptimality, before showing in Section
\ref{sec:bigdata} what kind of behaviour can be expected in ``big-data'' situation. We
then conclude in Section \ref{sec:conc}. All proofs have been pushed to the appendix
section.

% \Erick{Complete this paragraph. Consider replacing section 2 with four section.}


%Much of the theory supporting our framework comes from statistical learning and stability
%theory, and in particular from \cite{bousquet2002stability} who showed that explicit
%performances guaranteed on out-of-sample could be obtained using a convex loss and a
%regularization penalty using algorthimic analysis. We also use some results from
%\cite{sridharan2009fast} to obtain the same kind of results on the suboptimality of the
%problem. In fact, a key insight of our work is perhaps that this kind of stability
%analysis can be directly done on utility functions, as opposed to more classical loss
%functions, \eg, squared loss, hinge loss, etc. 


% This work is an attempt at bridging these two concepts. Using a size $n$ sample of the
% (unknown) market distribution consisting of market features and market returns, a
% regularization parameter $\lambda$ and by specifying an arbitrary concave utility
% function, we can derive an in-sample optimal linear investment policy by optimizing the
% certainty equivalent on the sample. We first show that the out-sample performance of the
% policy is bounded by a $O(1/\sqrt{n})$ error term. Second, We also investigate how this
% this method scales when the number of market features $p$ is of the order of $n$, ie. in a
% \textit{big-data} regime, and show that the performance scales linearly in the number $p$
% of available features. As far as we are aware, this situation has not been studied by the
% learning theory, and consequently we hope to enrich the field. \comment{Remanier.}
% Finally, we determine the conditions under which the true optimal solution in regard to
% the market distribution can be attained. \comment{We conclude by presenting numerical results from
% different degenerated distributions.}


% The \textit{market} considered by this document could  be any  asset
% traded on the market.\comment{Incorporer quelque part.}

% At a higher level, this document should be mostly understood as providing guidance to
% portfolio managers who would wish to incorporate general statistical and machine learning
% strategies in order to uncover market returns indicators. In fact, as more and more
% features are poured into a model (for example by considering polynomial kernels
% \comment{reference needed}), there is real possibility that the out-sample performance
% becomes degraded, and we wish to show how it can be prevented. 



% Most of this work derives from statistical learning theory, and in particular from
% stability theory, as exposed by Bousquet and Elisseef in their seminal paper
% \cite{bousquet2002stability}.  The author showed, using powerful concentration
% inequalities, how the empirical risk minimization of a Lipschitz loss function with
% additional convexity driven by a $\ell_2$ regularization on the decision would converge in
% the size of the sample toward the out-sample performance. In particular, their results
% were a departure from classical learning theory as the tools they were using stems
% strictly from algorithmic and convexity analysis. 

% We also improve on results from \cite{rudin2015big} who study the application of learning
% theory to a feature based newsvendor problem. However, while they explicitly consider the
% big-data regime, we believe our model is more general in the sense that we directly show
% the effects of $p$ on the performance of the algorithm.

%\comment{Padder davantage, plus de details sur 1. theorie moderne de portefeuille,
%  2. portefeuille universel, 3. Theorie de la stabilité, 4. Donner plus de références.}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "big_data_portfolio_optimization"
%%% End:
