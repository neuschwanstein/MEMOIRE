\section*{Appendix}

\comment{Move this elsewhere and put in context.}
\begin{claim}
  \label{stability}
  The uniform stability $\beta$ of our algorithm is given by:
  \[
    |\ell(\hat q_n,m) - \ell(\hat q_{n-1},m)| \leq \frac{(\gamma\bar r\xMax)^2}{2\lambda n}
  \]
\end{claim}

\begin{claim}
  \label{sigma_admissibility}
  The following inequality holds:
  \[
    |c(p_1,r) - c(p_2,r)| \leq \gamma\,\bar r\,|p_1-p_2|.
  \]
\end{claim}
\begin{proof}
  Using the Lipschitz property of $u$, the claim follows trivially.
\end{proof}


\begin{claim}
  \label{q_bound}
  The following inequality holds:
  \[
    \|\hat q\| \leq \frac{\gamma\xMax r_{\max}}{2\lambda} = O\left(\frac{\xMax}{\lambda}\right).
  \]
\end{claim}
\begin{proof}
  Let $\mu_n$ be a sample of the market. The empirical decision algorithm 
  \[
    \minimizeEquation{n^{-1}\sum_{i=1}^n\ell(m_i,q) + \lambda\|q\|^2}
  \]
  is equivalent to 
  \begin{align*}
    \minimizeEquationSt{n^{-1}\sum_{i=1}^n \ell(m_i,sq) + \lambda s^2}[s\geq0\\,\|q\|_2=1],
  \end{align*}
  where the optimization variables are now on the direction ($q$) and the scale
  ($s$). Therefore, for any direction $q$, we can define a convex function $g(s)$ which
  becomes the objective:
  \begin{align*}
    \minimizeEquationSt{g(s)}[s\geq 0],
  \end{align*}
  where
  \[
    g(s) = n^{-1}\sum_{i=1}^n \ell(m_i, sq) + \lambda s^2.
  \]
  
  Because $g:(0,+\infty)\to\real$ is convex, we can consider two cases: either the minimum
  is realized at the boundary, ie. $s^\star=0$, or there exists an optimal value
  $s^\star > 0$ such that $g'(s^\star)=0$. To derive a bound on $s^\star$, we can seek a
  value $\bar s$ such that for any $q$, $g'(\bar s)>0$ and therefore $\bar s > s^\star$.

  To do so, we first note that 
  \begin{align*}
    g'(s) &= \grad_s \left[n^{-1}\sum_{i=1}^n\ell(m_i,sq) + \lambda s^2\right]\\
          &= 2\lambda s - n^{-1}\sum_{i=1}^n \grad_s u(r_i\,sq^Tx_i)\\
          &= 2\lambda s - n^{-1}\sum_{i=1}^n r_i\,q^Tx_i \, u'(r_i\,sq^Tx_i).
  \end{align*}
  Now, because $\|q\|=1$, we have $q^Tx_i\leq\|x_i\|\leq\xMax$. We also have
  $r_i\leq r_{\max}$ and $u'\leq\gamma$, so that
  \[
    n^{-1}\sum_{i=1}^n r_i\,q^Tx_i \, u'(r_i\,sq^Tx_i) \leq \gamma\xMax r_{\max}.
  \]
  Therefore, with
  \[
    \bar s := \frac{\gamma\xMax r_{\max}}{2\lambda},
  \]
  for any $s>\bar s$,
  \[
    g'(s) \geq 0.\qedhere
  \]
\end{proof}

\begin{claim}
  \label{p_bound}
  The following inequality holds:
  \[
    |\hat p| = |\hat q^Tx| \leq \bar p =\frac{\gamma\xMax^2 r_{\max}}{2\lambda} =
    O\left(\frac{\xMax^2}{\lambda}\right).
  \]
\end{claim}
\begin{proof}
  The claim follows directly from HÃ¶lder's inequality and Claim \ref{q_bound}.
\end{proof}

\begin{claim}
  \label{ell_bound}
  The following inequalities hold:
  \[
    -\frac{\gamma\xMax^2r_{\max}\bar r}{2\lambda} \leq \ell(M,\hat q) \leq \frac{\gamma^2
      \xMax^2 r_{\max} \bar r}{2\lambda}.
  \]
\end{claim}
First, the maximum loss (ie. worst realized utility) is reached when $\hat p=\bar p$ as
expressed by Claim \ref{p_bound}, and $r=-\bar r$, ie.
$c(\bar p,-\bar r) \leq \gamma^2\xMax^2 r_{\max}\bar r/2\lambda$. Likewise, the minimum
loss (best utility) occurs with $\hat p  = \bar p$ and $r = \bar r$, so that $c(\bar p,
\bar r) \geq -\gamma\xMax^2r_{\max}\bar r/2\lambda$. 

\begin{claim}
  \label{out_of_sample_claim}
  The out of sample average returns are at least $u^{-1}(-\hat R(\qHat_n) - \Omega_n)$.
\end{claim}
\begin{proof}
  Using the convexity of $u^{-1}$ and Jensen's inequality, we first have
  \begin{align*}
    \E[u^{-1}(-\ell(M,\qHat_n))] &\geq u^{-1}(\E[-\ell(M,\qHat_n)])\\
                                &=u^{-1}(-R(\qHat_n)).
  \end{align*}
  From Theorem \ref{thm2}, we also have 
  \[
    -R(\qHat_n) \geq -\Omega_n - \hat R(\qHat_n).
  \]
  Since $u^{-1}$ is monotonically increasing, we finally obtain
  \[
    u^{-1}(-R(\qHat_n)) \geq u^{-1}(-\Omega_n - \hat R(\qHat_n)).\qedhere
  \]
\end{proof}

\begin{claim}
  The following inequality holds:
  \[
    |R(q_1) - R(q_2)| \leq \gamma\bar r\xMax\|q_1-q_2\|.
  \]
\end{claim}

\begin{proof}
  We have the following chain of inequality:
  \begin{align*}
    |R(q_1) - R(q_2)| &= |\E[\ell(q_1,M)] - \E[\ell(q_2,M)]|\\
                      &= |\E[\ell(q_1,M) - \ell(q_2,M)]|\\
                      &\leq \E[|\ell(q_1,M) - \ell(q_2,M)|]\\
                      &= \E[|u(Rq_1^TX) - u(Rq_2^TX)|]\\
                      &\leq \gamma \E[|R(q_1-q_2)^TX|]\\
                      &\leq \gamma\bar r\xMax\|q_1-q_2\|.\qedhere
  \end{align*}
\end{proof}

\begin{claim}
  \label{delage_bound_claim}
  The following inequality holds:
  \[
    |R(q^\star) - R_\lambda(q^\star_\lambda)| \leq \lambda\|q^\star\|^2_2.
  \]
\end{claim}
\begin{proof}
  We first have that
  \[
    R(q^\star) = \min_q R(q) \leq \min_q R(q) + \lambda\|q\|_2^2 = R_\lambda(q^\star_\lambda).
  \]
  We also have that
  \[
    R_\lambda(q^\star_\lambda) = \min_q R(q) + \lambda\|q\|^2_2 \leq R(q^\star) + \lambda\|q^\star\|^2_2,
  \]
  and therefore
  \[
    R_\lambda(q^\star_\lambda) - \lambda\|q^\star\|^2_2 \leq R(q^\star) \leq R_\lambda(q^\star_\lambda),
  \]
  leading to
  \[
    0 \leq R_\lambda(q^\star_\lambda) - R(q^\star) \leq \lambda\|q^\star\|^2_2.\qedhere
  \]
\end{proof}

\begin{claim}
  \label{claim_finite}
  If $u:\real\to\real$ monotonically increasing is such that $u(0) = 0$,
  $u(x) = u_-(x)\bm1_{\{x<0\}}+u_+(x)\bm 1_{\{x\geq 0\}}$ and $u_+(x) = o(u_-(x))$, then,
  for any real random variable $Z$ with $|Z|\leq M$ and $\lim_{x\to0^-}F_Z(x)>0$,
  \[
    \argmax_{k>0} \E[u(kZ)]
  \]
  is finite.
\end{claim}
\begin{proof}
  We first note that $\lim_{k\to\infty} \E[u(kZ)]=-\infty$ is a sufficient condition. Next,
  by hypothesis, there exists a $\delta<0$ such that $\pp\{Z<\delta\}=p>0$. Let $B$ a
  discrete random variable such that $\pp\{B=\delta\}=1-\pp\{B=M\}=p$. Then $\pp\{B\geq
  z\}\geq\pp\{Z\geq z\}$ for any $z$. This in turn implies that
  $\E[u(kB)]\geq\E[u(kZ)]$ \comment{Too fast?}. But 
  \[
    \lim_{k\to\infty}\E[u(kB)] = \lim_{k\to\infty}p\,u(k\delta)+(1-p)u(kM) = -\infty.\qedhere
  \]
\end{proof}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "master"
%%% End:
