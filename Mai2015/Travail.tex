\documentclass[11pt]{article}

\newcommand{\ts}{\textsuperscript}
\newcommand{\figref}[1]{Fig.~\ref{#1}}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{hyperref}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage{mathtools}
\usepackage{color}
\usepackage{marginnote}
\usepackage[utf8]{inputenc}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\st}{s.t.}
\DeclareMathOperator{\epi}{epi}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\newcommand{\iso}{\simeq}
\newcommand{\dd}{\partial}
\newcommand{\real}{\bm R}


\newcommand{\hilight}[1]{\colorbox{yellow}{#1}}
\let\oldmarginnote\marginnote
\renewcommand{\marginnote}[1]{\oldmarginnote{\footnotesize\emph{#1}}[0cm]}

\theoremstyle{plain}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}

\theoremstyle{definition}
\newtheorem*{deff}{Definition}
\newtheorem*{rem}{Remark}


\geometry{letterpaper}
\IEEEeqnarraydefcolsep{0}{\leftmargini}

\title{The Big Data Newsvendor Problem in a Portfolio Optimization Context}
\author{Thierry \textsc{Bazier-Matte}}
\date{Summer 2015}

\begin{document}
\maketitle

\begin{abstract}
  Following \cite{rudin2015}, we provide a portfolio optimization method based on machine
  learning methods.
\end{abstract}


\section{Introduction}
\label{sec:intro}

\marginnote{Maybe considerations about the length of the period should be added? For
  example, it's not specified what's the period length of $R_f$.}  This document considers
a two-asset portfolio, of which one is the risk-free asset, yielding a constant return
rate $R_f$, and the other being a risky asset $s$, typically a stock, yielding a random
return rate $r_{st}$ for each period $t$. We suppose that each risky asset $s$ can be
decribed daily by an \emph{information vector} $x_{st}$ containing potentially useful
information, such as technical, fundamental or news-related information. Furthermore, we
assume that the allocation of each asset of the portfolio $p_{st}$ can be fully determined
using a \emph{decision vector} $q$. The allocation rule is the folowing: $q^Tx_{st}$ is
allocated to the risky asset and $1-q^Tx_{st}$ is allocated to the risk-free asset. Over
the period $t$, the portfolio $p_{st}$ consisting of asset $s$ will therefore yield a
return rate of:
\begin{equation}
  p_{st}(q) = r_{st}q^Tx_{st} + (1-q^Tx_{st})R_f.
\end{equation}

The question we now wish to ask is how the decision vector $q$ should be chosen. We assume
we have access to a training dataset $S_n$, comprising of $s$ different assets over
$t$\marginnote{What's the difference between having $n$ points with having $s\times t$
  points? For example, what if $s\gg t$ or the reverse?} periods, such that
$n = s\times t$. 



\section{Definitions and Bounds}

\subsection{Definitions Notation}

Most of the following notation and defintions follow directly from \cite{bousquet2002}.

Let $S_n$ be a set of $n$ vectors of $\real^p\times\real$ of the form:
\begin{equation}
  S_n = \{(x_1,r_1),\ldots,(x_n,r_n)\}.
\end{equation}
Each component of $S_n$ is a tuple $(x,r)$, where $x$ is the information vector and $r$ is
the observed return rate.

Using $S_n$, we wish to create a decision vector $q_{S_n}\in\real^p$ from which we can
make an investment decision when confronted with a random draw $d=(x,r)$.

\paragraph{Loss and Cost.}
We introduce the loss $\ell$ and the cost $c$ of using $q$ with a random draw $d=(x,r)$:
\begin{equation}
\ell(q,d) = c(q(x),r) = c(q^Tx,r)
\end{equation}

Supposing an utility $U$, there are two different cost functions we can use. The first
one, and perhaps the most obvious one is defined by
\begin{equation}
  c(p,r) = -U(pr + (1-p)R_f).
\end{equation}
If the utility is piecewise linear, then the cost is not bounded, and so an infinite
return would yield a negatively infinite cost. This means that risk-taking will be
encouraged since risky position, ie. $|p|>1$ can in fact yield a negative cost. On the
other hand, with an unbounded utility, the algorithm minimizing the cost over the sample
must have $\lambda > 0$, otherwise the problem might be unconstrained. 

We can also define a new cost function that is always non-negative:
\begin{equation}
  c(p,r) =
  \begin{cases}
    \lfloor U(r) - U(pr + (1-p)R_f)\rfloor & \text{if } r>R_f\\
    \lfloor U(R_f) - U(pr + (1-p)R_f)\rfloor &\text{if } r\leq R_f,
  \end{cases}
\end{equation}
where by $\lfloor \cdot \rfloor$ we mean $\max(.,0)$. In the case of exponential
utilty, this cost function is actually equivalent to the one previously defined, but is
quite different in the case of an unbounded utility function, for example the piece-wise
linear one. Such a cost does not encourage risky position, since the cost is at least
0. This means that if $r>0$, then there's no point having $p>1$ instead of $p=1$ since
both will yield a zero cost position. 

\paragraph{Utility.}
There are two ways we can model our utility, and both are concave shaped, to represent a
risk-averse approach. The first utility is the linear utility of the form
\begin{equation}
  U(r) = r + \min(0, \beta r),
\end{equation}
with $0<\beta<1$. The other utility is exponential:
\begin{equation}
  U(r) = -\exp(-\mu r),
\end{equation}
with $\mu > 0$.

\begin{figure}
  \centering
  \begin{subfigure}{.4\textwidth}
  \includegraphics[width=1.1\textwidth]{ExpULossAboveZero.pdf}
\end{subfigure}%
\begin{minipage}{.4\textwidth}
  \includegraphics[width=1.1\textwidth]{ExpULossBelowZero.pdf}
\end{minipage}
\end{figure}

\paragraph{Algorithm.}
We will be concerned with probabilistic confidence bounds on results produced using the
following algorithm, using dataset $S_n$.
\begin{equation}
  q^\star = \argmin_{q\in\real^p}\frac{1}{n} \sum_{i=1}^{n} c(q^Tx_i,r_i) + \lambda\|q\|^2_2.
\end{equation}

\paragraph{Assumptions.}
We will assume that information vectors have been pre-processed and lie in a $X^2_{\max}$
radius ball. We also assume that the return rates observed are comprised within $[-\bar r,
\bar r]$. This last assumption will be relaxed. 

\marginnote{Include reference for definitions and theorems}

\begin{deff}
  An algorithm $A$ has uniform stability $\beta$ with respect to the loss function $\ell$
  if, for all $S_n\in\bm D^n$ and $i\in\{1,\ldots,n\}$, the following holds:
  \begin{equation}
    \|\ell(A_{S_n},.) - \ell(A_{S^{\backslash i}_n},.)||_{\infty} \leq \beta_n,
  \end{equation}
  or, equivalently,
  \begin{equation}
    \sup_{d\in\bm D}|\ell(A_{S_n},d) - \ell(A_{S^{\backslash i}_n},d)| \leq \beta_n.
  \end{equation}
  Here, $S^{\backslash i}$ means the set $S$ with the $i$th data point removed.

  Furthermore, $A$ is stable when $\beta_n = O(1/n)$.
\end{deff}

\begin{deff}
  A loss function $\ell$ is $\sigma$-admissible if the associated cost function $c$ is
  convex with respect to its first argument and the following condition holds for any
  $p_1,p_2$ and $r$:
  \begin{equation}
    |c(p_1,r) - c(p_2,r)| \leq \sigma |p_1 - p_2|
  \end{equation}
\end{deff}

\begin{rem}
  Our loss function $\ell$ is $\sigma$-admissible with $\sigma=\bar r+R_f$ in the linear
  case and $\sigma=(\bar r+R_f)\exp(\mu\bar r)$ in the exponential case.
\end{rem}

\begin{proof}
  First, we remark that both forms of $U$ yield a convex function of $p$ with $r$ fixed. 

  Now we'll suppose that $c(p_1,r), c(p_2,r) > 0$. Then the expression
  $|c(p_1,r)-c(p_2,r)|$ reduces to
  \begin{equation}
    \label{eq:above1}
    |U(p_1r + (1-p_1)R_f) - U(p_2r + (1-p_2)R_f|.
  \end{equation}
  Now because $r\in[-\bar r,\bar r]$, $U$ is Lipschitz continuous on its domain, and so
  \eqref{eq:above1} is bounded by
  \begin{equation}
    \label{eq:above2}
    \alpha |p_1r + (1-p_1)R_f - (p_2r + (1-p_2)R_f)| = \alpha|p_1-p_2||r-R_f|
  \end{equation}
  where
  \begin{equation}
    \alpha = \sup_{r\in[-\bar r,\bar r]} |U'(r)|.
  \end{equation}

  In the linear case, the derivative is piecewise constant, and is set to 1 on for returns
  below $r_c$, so that $\alpha=1$. In the exponential case, $U'(r) = \exp\mu r$, and
  $\alpha = \exp \mu \bar r$.

  The bound \eqref{eq:above2} must hold for any $r$. The expression $|r-R_f|$ will reach
  its largest value at $r=-\bar r$, since $R_f$ is assumed to be non-negative.

  Finally we consider the case where, without loss of generality, $c(p_2,r)=0$. Then, if
  $c$ had not been defined using $\lfloor\cdot\rfloor$, then we would have
  \begin{align}
    |\floor*{c(p_1,r)} - \floor*{c(p_2,r)}| &\leq |c(p_1,r) - c(p_2,r)|\\
    &\leq \sigma|p_1-p_2|.\qedhere
  \end{align}
\end{proof}

\begin{thm}
  Let $F$ be a reproducing kernel Hilbert space with kernel $\kappa$ that
  $\forall x\in X$, $\kappa(x,x) \leq \kappa^2 <\infty$. If $\ell$ is $\sigma$-adimissible
  with respect to $F$, then the learning algorithm defined by
  \begin{equation}
    \label{eq:above3}
    A_S = \argmin_{g\in F}\frac{1}{n}\sum_{i=1}^n \ell(g,d_i) + \lambda\|g\|^2_k
  \end{equation}
  has uniform stability $\alpha_n$ with respect to $\ell$ with
  \begin{equation}
    \alpha_n \leq \frac{\sigma^2 \kappa^2}{2\lambda n}.
  \end{equation}
\end{thm}

\begin{rem}
  Our proposed algorithm has the form \eqref{eq:above3}, and so has algorithmic stability
  bounded by
  \begin{equation}
    \alpha_n \leq \frac{(\bar r+R_f)^2X^2_{\max}}{2\lambda n}
  \end{equation}
  with linear utility and
  \begin{equation}
    \alpha_n \leq \frac{\exp(2\mu\bar r)X^2_{\max}}{2\lambda n}
  \end{equation}
  in the case of exponential utility.
\end{rem}

\begin{deff}
  The \emph{true risk} with respect to algorithm $A$ and set $S_n$ is defined as
  \begin{equation}
    R_{\text{true}}(A,S_n) = E_d[\ell(A_{S_n},d)],
  \end{equation}
  which is, in plain words, the expected loss incured when applying the algorithm created
  from training set $S_n$ in the wild, ie. out of sample.
\end{deff}

\begin{deff}
  The \emph{empirical risk} with respect to algorithm $A$ and set $S_n$ is defined as
  \begin{equation}
    \hat R(A,S_n) = \frac{1}{n} \sum_{i=1}^n \ell(A_{S_n},d_i),
  \end{equation}
  which is, in plain words, the average cost incured by our model over all the training
  set.
\end{deff}

\begin{rem}
  The maximum loss we can suffer over a single data point happens when $r_i=-\bar r$ and
  $p=1$, ie.
  \begin{equation}
    c(1,-\bar r) = U(R_f) - U(\bar r).
  \end{equation}
  We shall call this quantity $\gamma$.
\end{rem}

\begin{thm}
  Let $A$ be an algorithm with uniform stability $\alpha_n$ with respect to a loss
  function $\ell$ such that $0\leq\ell(A_{S_n},d)\leq M$ for all $d=(x,r)\sim D$ and all sets
  $S_n$ of size $n$. Then for any $n\geq1$ and any $\delta\in(0,1)$, the following bound
  holds with probability at least $1-\delta$ over the random draw of the sample $S_n$:
  \begin{equation}
    |R_{\text{true}}(A,S_n) - \hat R(A,S_n)| \leq 2\alpha_n + (4n\alpha_n + M) \sqrt{\frac{\log(2/\delta)}{2n}}.
  \end{equation}
\end{thm}

\begin{rem}
  Our algorithm has a generalization bound of
  \begin{equation}
    |R_{\text{true}}(A,S_n) - \hat R(A,S_n)| \leq 2\alpha_n + (4n\alpha_n + \gamma) \sqrt{\frac{\log(2/\delta)}{2n}}.
  \end{equation}
\end{rem}


\section{Multi-asset Portfolio}



\begin{thebibliography}{99}

\bibitem{rudin2015}
  Cynthia Rudin and Gah-Yi Vahn. \textit{The Big Data Newsvendor: Pratical Insights from
    Machine Learning}, Operations Research, 2015.

\bibitem{bousquet2002}
  Olivier Bousquet and André Elisseeff. \textit{Stability and Generalization}, Journal of
  Machine Learning Research, 2002.

\bibitem{lipschitz}
  ``Si la valeur absolue de la dérivée est majorée par $k$, $f$ est $k$-lipschitzienne''.
  \href{https://fr.wikipedia.org/wiki/Application_lipschitzienne}{Application
    lipschitzienne}.

\bibitem{rockafellar}
  Rockafellar, R.~T. \emph{Convex Analysis}, Princeton University Press, 1970.

\bibitem{supergradients}
  \href{http://people.hss.caltech.edu/~kcb/Notes/Supergrad.pdf}{Supergradients}.

\bibitem{refneeded} Reference needed!

\end{thebibliography}
\end{document}
