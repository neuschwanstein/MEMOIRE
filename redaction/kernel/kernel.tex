\section{Introduction aux fonctions de décisions non linéaires}

\subsection{Introduction}

Soit $\kappa: \X \times \X \to \Re$ un noyau semi-défini positif, $\H$ l'espace de Hilbert à noyau
reproduisant induit par $\kappa$ et $K \in \Re^{n \times n}$ la matrice associée à l'ensemble
d'échantillonage $S_n \sim M^n$. Le problème d'optimisation de portefeuille régularisé
s'exprime alors par
\begin{equation}
  \label{k:basic}
  \maximizeEquation[q \in \H]{n^{-1}\sumi u(r_i\,q(x_i)) - \lambda\|q\|^2_{\H}.}
\end{equation}

Tel que mentionné, la dimension de $\H$ est possiblement infinie, ce qui rend
numériquement impossible la recherche d'une solution $q^\star$. Toutefois, le théorème de la
représentation permet de rendre le problème résoluble. 

\begin{thm*}
  Toute solution $q^\star$ de \eqref{k:basic} repose dans le sous-espace vectoriel engendré
  par l'ensemble des $n$ fonctions $\{\phi_i\}$, où $\phi_i = \kappa(x_i,\cdot)$. Numériquement, il
  existe un vecteur $\alpha^\star \in \Re^n$ tel que,
  \begin{equation}
    q^\star = \sumi \alpha^\star_i \phi_i = (\alpha^\star)^T\phi.
  \end{equation}
\end{thm*}
\begin{proof}
  Voir \cite{mohri2012foundations}, Théorème 5.4 pour une démonstration tenant compte d'un
  objectif régularisé général. La démonstration est dûe à \cite{kimeldorf1971some}.
\end{proof}

Le théorème de la représentation permet donc de chercher une solution dans un espace à $n$
dimensions, plutôt que la dimension possiblement infinie de $\H$. En effet, puisque
\begin{equation}
  q^\star = \sumi \alpha_i \phi_i,
\end{equation}
où $\alpha \in \Re^n$ \todo{Espace cotangant???}, on peut donc restreindre le domaine
d'optimisation à $\Re^n$. L'objectif de \eqref{k:basic} devient alors
\begin{equation}
  n^{-1}\sumi u(r_i \textstyle \sumj \alpha_j \phi_j(x_i)) - \lambda \langle q,q \rangle_{\H}. 
\end{equation}
Le premier terme se réexprime comme
\begin{equation}
  n^{-1} \sumi u(r_i\,\alpha^T\phi(x_i)),
\end{equation}
alors qu'en employant les propriétés de linéarité du produit intérieur, on transforme le
second terme par
\begin{align}
  \langle q,q \rangle^2_{\H} &= \sumi \sumj \alpha_i \alpha_j \langle \phi_i,\phi_j \rangle_{\H}\\
                 &= \sumi \sumj \alpha_i \alpha_j \kappa(x_i,x_j)\\
                 &= \alpha^T K \alpha.
\end{align}
De sorte que le problème général \eqref{k:basic} peut se reformuler par
\begin{equation}
  \boxed{\maximizeEquation[\alpha \in \Re^n]{n^{-1}\sumi u(r_i \alpha^T \phi(x_i)) - \lambda\,\alpha^TK\alpha}}\quad.
\end{equation}


\subsection{Dualité}


Let us consider the following problem, optimized over $q \in \Re^p$:
\begin{equation}
  \label{primal}
  \minimizeEquation{\sumi \ell(r_i\,q^Tx_i) + n\lambda\|q\|^2,}
\end{equation}
where $\ell=-u$. Alternatively, this problem can be respecified using slack vector
$\xi \in \Re^n$ as
\begin{equation}
  \minimizeEquationSt{\label{primal2}\sumi \ell(\xi_i) + n\lambda\|q\|^2}[\xi_i = r_i\,q^Tx_i.] 
\end{equation}
Let $\alpha\in\real^n$. The Lagrangian of \eqref{primal2} can be written as
\begin{equation}
  \lag(q,\xi,\alpha) = \sumi \ell(\xi_i) + n\lambda\|q\|^2 + \sumi \alpha_i(r_iq^Tx_i - \xi_i).
\end{equation}
Because the objective \eqref{primal2} is convex and its constraints are affine in $q$ and
$\xi$, Slater's theorem states that the duality gap of the problem is zero. In other words,
solving \eqref{primal} is equivalent to maximizing the Lagrange dual function $g$ over
$\alpha$:
\begin{equation}
  \maximizeEquation{g(\alpha) = \inf_{q,\xi} \lag(q,\xi,\alpha).}
\end{equation}
Now, note that
\begin{align}
  g(\alpha)
  &= \inf_{q,\xi} \left\{\sumi \ell(\xi_i) + n\lambda\|q\|^2 + \sumi \alpha_i(r_iq^Tx_i -
    \xi_i)\right\}\\
  &= \inf_\xi\left\{\sumi \ell(\xi_i) - \alpha^T\xi\right\} + \inf_q\left\{\sumi \alpha_ir_iq^Tx_i +
    n\lambda\|q\|^2\right\}\\
  & = -\sup_\xi\left\{a^T\xi - \sumi \ell(\xi_i)\right\} + \inf_q\left\{\sumi \alpha_ir_iq^Tx_i +
    n\lambda\|q\|^2\right\}\\
  & = -\sumi \ell^*(\alpha_i) + \inf_q\left\{\sumi \alpha_ir_iq^Tx_i +n\lambda\|q\|^2\right\}.\label{deriv1} 
\end{align}
Where $\ell^*$ is the convex conjugate of the loss function and is defined by
\begin{equation}
  \ell(\alpha_i) = \sup_{\xi_i}\left\{\alpha_i\xi_i - \ell(\xi_i)\right\}.
\end{equation}
Note that the identity
\begin{equation}
  \label{lemma1}
  f(\xi_1,\dots,\xi_n) = \sumi \ell(\xi_i) \Longrightarrow f^*(\xi_1,\dots,\xi_n) = \sumi \ell^*(\xi_i)
\end{equation}
was used. Consider now the second part of \eqref{deriv1}. Since
the expression is differentiable, we can analytically solve for $q$:
\begin{equation}
  \nabla_q \left\{\sumi \alpha_ir_iq^Tx_i + n\lambda\|q\|^2\right\} = 0
\end{equation}
implies that
\begin{equation}
 q = -\frac{1}{2n\lambda}\sumi \alpha_ir_ix_i \label{qdef}
\end{equation}
at the infimum.

Using \eqref{qdef}, we can eliminate $q$ from \eqref{deriv1}, so that
\begin{align}
  g(\alpha) &= -\sumi \ell^*(\alpha_i) - \frac{1}{2n\lambda}\sumij \alpha_i\alpha_jr_ir_jx_i^Tx_j + \frac{1}{4n\lambda}
         \sumij \alpha_i\alpha_jr_ir_jx_i^Tx_j\\
       &= -\sumi \ell^*(\alpha_i) - \frac{1}{4n\lambda}(\alpha \circ r)^TK(\alpha \circ r). 
\end{align}
Therefore, in its dual form, the problem \eqref{primal} is equivalent to solving
\begin{equation}
  \label{dual}
  \minimizeEquation{\sumi \ell^*(\alpha_i) + \frac{1}{4n\lambda}(\alpha \circ r)^TK(\alpha \circ r).}
\end{equation}

\paragraph{Prescribed investment}

In its original form, given a feature vector $\tilde x$, the algorithm \eqref{primal}
suggests an investment size of $p_0=q^T\tilde x$, where $q$ is the trained value obtained by
optimizing \eqref{primal}. In the dual formulation \eqref{dual}, with optimal value
$\alpha$, we have from \eqref{qdef}:
\begin{align}
  p_0 &= q^Tx_0\\
      &= - \frac{1}{2n\lambda}\sumi \alpha_ir_ix_i^Tx_0.
\end{align}
\todo{Insert kernel formulation with vector $\phi$.}


\subsection{Alternate problem}

We now consider a new problem, slightly different from \eqref{primal} where a
regularization based on the sum of the square of the investment sizes $q^Tx_i$ is applied:
\begin{equation}
  \label{primal2000}
  \minimizeEquation{\sumi \ell(r_i\,q^Tx_i) + \gamma\sumi(q^Tx_i)^2 + n\lambda\|q\|^2.}
\end{equation}
Again, this problem can be respecified using slack vector $\xi \in \Re^n$ as
\begin{align}
  \minimizeEquationSt{\label{primal2001}\sumi \ell(\xi_i) + \gamma\sumi(\xi_i/r_i)^2 + n\lambda\|q\|^2}[\xi_i = r_i\,q^Tx_i.]
\end{align}
The constraints in \eqref{primal2001} are again affine, so that Slater's theorem apply.

The lagrangian of \eqref{primal2001} is
\begin{equation}
  \lag(q,\xi,\alpha) = \sumi \ell(\xi_i) + \gamma\sumi(\xi_i/r_i)^2 + n\lambda\|q\|^2 + \sumi \alpha_i(r_iq^Tx_i - \xi_i),
\end{equation}
and we seek its infimum over $(q,\xi)$.
\begin{align}
  &\inf_{q,\xi} \left\{\sumi \ell(\xi_i) + \gamma\sumi (\xi_i/r_i)^2 + n\lambda\|q\|^2 + \sumi \alpha_i(r_iq^Tx_i -
    \xi_i)\right\}\\
  &\quad =\inf_\xi\left\{\sumi \ell(\xi_i) + \gamma\sumi(\xi_i/r_i)^2 - \alpha^T\xi\right\} + \inf_q\left\{\sumi
    \alpha_ir_iq^Tx_i - n\lambda\|q\|^2\right\}\\
  &\quad = - \sup_\xi\left\{a^T\xi - \left(\sumi \ell(\xi_i) + \gamma\sumi(\xi_i/r_i)^2\right)\right\}
    - \frac{1}{4n\lambda} (\alpha \circ r)^TK(\alpha \circ r)\label{this}.
\end{align}

Let $f_i(\xi_i) := h_1(\xi_i)+ h_2(\xi_i) = \ell(\xi_i) + \gamma(\xi_i/r_i)^2$. Then, using \eqref{lemma1},
the first expression of \eqref{this} can be restated as
\begin{equation}
  \label{firstconj}
  -\sup_\xi\left\{\alpha^T \xi - \sumi f_i(\xi_i)\right\} = -\sumi f_i^*(\alpha_i).
\end{equation}
Let us introduce another identity:
\begin{equation}
  \label{lemma2}
  (h_1+h_2)^*(\alpha_i) = \inf_{\alpha_i'+\alpha_i''=\alpha_i} \{h_1^*(\alpha_i') + h_2^*(\alpha_i'')\}.
\end{equation}
Using \eqref{lemma2}, \eqref{firstconj} can be written as
\begin{align}
  -\sumi f_i^*(\alpha_i) &= -\sumi (h_1+h_2)^*(\xi_i)\\
                    &= -\sumi \inf_{\alpha_i'+\alpha_i''=\alpha_i} \{h_1^*(\alpha_i') + h_2^*(\alpha_i'')\}.
\end{align}
The first conjugate function $h_1^*$ is simply $\ell^*$. The second conjugate function can be
derived analytically:
\begin{align}
  h_2^*(\alpha_i'') &= \sup_{\xi_i}\,\{\alpha_i''\xi_i - h_2(\xi_i)\}\\
               &= \sup_{\xi_i}\,\{\alpha_i''\xi_i - \gamma(\xi_i/r_i)^2\}\label{lastsup}.
\end{align}
The supremum occurs when
\begin{equation}
  \label{supconj}
  \xi_i = \frac{r_i^2}{2\gamma}\alpha_i''.
\end{equation}
Therefore, \eqref{lastsup} simplifies to
\begin{equation}
  h_2^*(\alpha_i'') = \frac{r_i^2}{4\gamma}(\alpha_i'')^2.
\end{equation}

Putting it all back together, the dual of \eqref{primal2000} is
\begin{equation}
  -\sumi \inf_{\alpha_i'+\alpha_i''=\alpha_i}\,\left\{\ell^*(\alpha_i') + \frac{r_i^2}{4\gamma}(\alpha_i'')^2\right\}  - \frac{1}{4n\lambda} (\alpha \circ r)^TK(\alpha \circ r),
\end{equation}
which is equivalent to
\begin{equation}
  -\sumi \ell^*(\alpha_i) - \frac{1}{4\gamma}\sumi (r_i\,\beta_i)^2 - \frac{1}{4n\lambda}(r \circ (\alpha+\beta))^TK(r \circ (\alpha+\beta)),
\end{equation}
with new optimization variables $\alpha=\alpha',\beta=\alpha'' \in \Re^n$. The dual optimization problem is
therefore
\begin{equation}
  \minimizeEquation{\sumi \ell^*(\alpha_i) + \frac{1}{4\gamma}\|r\circ\beta\|^2 + \frac{1}{4n\lambda}(r \circ (\alpha+\beta))^TK(r \circ (\alpha+\beta)).}
\end{equation}


\paragraph{Prescribed investment}
\todo{}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main_kernel"
%%% End:
