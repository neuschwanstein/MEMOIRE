\section{Garanties statistiques}
\label{sec:bound}

\epigraph{Et nous avons aussi des Maisons consacrées aux Erreurs des Sens [...] Ce sont là, ô
  mon fils, les richesses de la Maison de Salomon.}{Francis Bacon\\\textsc{New Atlantis}}



La section précédente était dédiée à l'approche algorithmique du problème: comment, donnés
un ensemble d'entraînement et un espace de décision $\Q$ induit par un noyau $\kappa$, une
fonction de décision $\hat q:\X \to \Re$ permettant de prescrire un investissment pouvait être
déterminée. Cette section sera consacrée aux garanties statistiques de cette
décision $\qh$. Dans un premier temps, une étude de la stabilité de l'algorithme d'optimisation
permettra de dériver une borne de généralisation sur la performance hors-échantillon
(Section \ref{b:gen}). Par la suite, le problème sera approché d'un point probabiliste (en
terme de variables aléatoires) afin de comparer les performances de la décision optimale
d'investissement sur $M$ par rapport à la décision empirique (Section
\ref{b:sopt}). Enfin, la Section \ref{b:dim} portera sur l'influence de la dimensionalité
de l'espace $\Q$ sur la qualité des bornes alors obtenues. 

Dans un premier temps, ces bornes seront données en terme d'utilité espérée
$\nEU$. Cependant, comme discuté en introduction, l'utilité étant adimensionelle, un
théorème sera énoncé afin de pouvoir exprimer les garanties de performance en terme de
rendement ou \textit{d'équivalent certain} $\nCE = u^{-1}(\nEU)$. 


\subsection{Hypothèses et discussion}

Certaines bornes devront d'abord être posées afin d'être en mesure d'obtenir des résultats
finis: ce sera en fait le prix à payer pour l'absence de contraintes sur la forme de la
loi de marché $M$, notamment sur l'amplitude de ses moments. 

\begin{assumption}
  La norme d'une observation est bornée: pour tout $x \in \X$, $\kappa(x,x) \leq \xi^2$.
\end{assumption}
\begin{assumption}
  Le rendement aléatoire est borné: $|R| \leq \rmax$.
\end{assumption}
\begin{assumption}
  \label{hyp:lip}
  Un investisseur est doté d'une fonction d'utilité $u$ concave, monotone et standardisée,
  c'est-à-dire que $u(0) = 0$ et $1 \in \partial u(0)$\footnote{Ici, $\partial u(r)$ signifie l'ensemble
    des sur-gradients de $u$. Dans le cas dérivable, cela revient à la notion de
    dérivée. Dans le cas simplement continu, $\partial u(r)$ est l'ensemble des fonctions affines
    ``touchant'' à $u(r)$ et supérieures à $u(r)$ pour tout $r$ du domaine).}. De plus,
  $u$ est défini sur l'ensemble de $\Re$. Enfin, $u$ est $\gamma$-Lipschitz, c'est-à-dire que
  pour tout $r_1,r_2 \in \Re$, $|u(r_1) - u(r_2)| \leq \gamma|r_1-r_2|$.
\end{assumption}

Avant d'aller plus loin, il convient de discuter de la plausiblité de ces
contraintes. Cependant, compte tenu de l'aspect central de la première hypothèse, une
discussion approfondie ne sera abordée qu'à la section \ref{b:dim}.

Pour ce qui est de la seconde hypothèse, si on définit les rendements selon
l'interprétation usuelle d'un changement de prix $p$, \ie, $r = \Delta p/p$, on constatera que
$r$ est nécessairement borné par 0. De plus, selon la période de temps pendant laquelle
$\Delta p$ est mesuré, il y a forcément moyen de limiter l'accroissement dans le prix, pour
autant que $\Delta t$ soit suffisament court.

La troisième hypothèse est davantage contraignante. Elle exclut d'emblée plusieurs
fonctions d'utilité courantes; par exemple l'utilité logarithmique et racine carrée
puisqu'elles ne sont définies que pour $\Re_{+}$. Une utilité quadratique, comme celle de
Markowitz est également inadmissible puisqu'elle est non-monotone. Les utilités de forme
exponentielle inverse $u(r) = \mu(-\exp(-r/\mu)+1)$ de paramètre $\mu > 0$ violent quant à elles
la condition Lipschitz. On peut cependant définir une utilité exponentielle \textit{à
  pente contrôlée}, c'est à dire dont la pente devient constante lorsque $r \leq r_0$. La
Section \ref{sec:emp} emploiera une telle fonction d'utilité pour illustrer numériquement
ces résultats. En outre, une utilité qui serait définie par morceaux linéaires serait
parfaitement acceptable. 


\subsection{Garantie de généralisation}
\label{b:gen}


Soit $\Q$ un espace de décision induit par un noyau $\kappa : \X \times \X \to \Re$ et soit un ensemble
d'entraînement $\S_n = \{(x_i,r_i)\}_{i=1}^n \sim M^n$ échantilloné à partir de la
distribution de marché. Alors on peut définir \textit{l'algorithme de décision}
$\alg:\M^n \to \Q$ par
\begin{equation}
  \label{b:basic}
  \alg(\S_n) = \argmax_{q \in \Q} \hEU_\lambda(\S_n,q)
\end{equation}
où
\begin{equation}
  \hEU_\lambda(\S_n,q) \coloneq \hEU(\S_n,q) - \frac{\lambda}{2}\|q\|^2
\end{equation}
et
\begin{equation}
  \hEU(\S_n,q) \coloneq n^{-1}\sumi u(r_i\,q(x_i)).
\end{equation}

L'erreur de généralisation $\hat\zeta:\M^n \to \Re$ de cet ensemble d'entraînement est alors définie
par
\begin{equation}
  \hat\zeta(\S_n) \coloneq \hEU(\S_n,\alg(\S_n)) - \EU(\alg(\S_n)).
\end{equation}
Ainsi, plus l'erreur de généralisation $\hat\zeta$ est élevée, plus l'investisseur sera déçu
par l'utilité espérée de la fonction de décision $\qh$ par rapport à ce qu'il aura observé
en échantillon. Le théorème suivant établit de façon probabiliste la différence maximale
de ces deux mesures de performance:

\begin{thm}[Borne sur l'erreur de généralisation (util)]
  \label{thm1}
  Avec probabilité d'au moins $1-\delta$, 
  \begin{equation}
    \hat\zeta(\S_n) \leq \frac{2\gamma^2\rmax^2\xi^2}{\lambda n} + \frac{(4\gamma^2 + \gamma +
      1)\rmax^2\xi^2}{\lambda}\sqrt{\frac{\log(1/\delta)}{2n}}.
  \end{equation}
\end{thm}

Tel que discuté, un investisseur sera avant tout concerné par l'erreur de généralisation
hors échantillon exprimée en \textit{équivalent certain:}
\begin{equation}
  \hat\zeta_e(\S_n) \coloneq \hCE(\S_n,\alg(\S_n)) - \CE(\alg(\S_n))
\end{equation}
où $\hCE = u^{-1} \circ \hEU$ et $\CE = u^{-1} \circ \EU$ dénotent l'équivalent certain en et hors
échantillon. Le Théorème 2 fournit alors à l'investisseur la déviation maximale que peut
subir l'équivalent certain hors échantillon par rapport au rendement équivalent en
échantillon.

\begin{thm}[Borne sur l'erreur de généralisation (rendement)]
  \label{thm2}
  Avec probabilité d'au moins $1-\delta$,
  \begin{equation}
    \hat\zeta_e(\S_n) \leq \frac{1}{\partial u(\nhCE)}\left( \frac{2\gamma^2\rmax^2\xi^2}{\lambda n} + \frac{(4\gamma^2 + \gamma +
        1)\rmax^2\xi^2}{\lambda}\sqrt{\frac{\log(1/\delta)}{2n}}\right) 
  \end{equation}
  où $\nhCE = \hCE(\S_n,\alg(\S_n))$. 
\end{thm}

Cette borne permet ainsi d'appréhender dans quelle mesure un large échantillonage est
nécessaire pour obtenir un degré de confiance suffisament élevé. On notera l'influence de
plusieurs facteurs sur la qualité de la borne (la discussion sur l'influence du terme
$\xi^2$ est repoussé à la Section \ref{b:dim}).

Ainsi, la constante $\gamma$ et le terme du sur-gradient inverse $\partial u^{-1}(\nhCE)$ sont tous
deux susceptibles de dégrader considérablement la borne, particulièrement lorsque
l'investisseur est doté d'une utilité très averse au risque; dans des cas extrêmes, par
exemple une utilité exponentielle inverse, ces deux valeurs divergeront très
rapidement. Il convient cependant de prendre note que la constante Lipschitz est
globalement plus importante puisqu'on considère son carré. Il devient alors essentiel de
contrôler l'agressivité de l'algorithme en choisissant des valeurs élevées pour la
régularisation $\lambda$ de manière à chercher une utilité espérée relativement proche de
$u(0)$.

On constate par ailleurs le rôle de premier plan que joue le terme de régularisation. Avec
une régularisation élevée, on obtiendra sans surprise une borne très serrée, mais aux
dépens de la politique d'investissement qui varie selon $\O(1/\lambda)$. Il est donc primordial
de faire une validation croisée sur $\lambda$ pour déterminer le meilleur compromis entre la
variance des résultats et l'objectif à atteindre. La constante de confiance $\delta$ est quant
à elle très performante; une confiance de \num{99.9}\% n'accroît la borne que par un
facteur de \num{2.63}. Enfin, compte tenu du théorème limite centrale, l'ordre de
convergence de $\O(1/\sqrt{n})$ n'a finalement rien de surprenant.




\subsection{Bornes de sous optimalité}
\label{b:sopt}

\subsubsection{Hypothèses supplémentaires}



Jusqu'ici, les efforts théoriques ont été déployés pour déterminer comment se comportait
la fonction de décision $\hat q = \alg(\S_n)$ dans un univers probabiliste par rapport à
l'univers statistique dans lequel elle avait été construite. Notre attention va maintenant
se tourner vers la performance de $\hat q$ dans l'univers probabiliste par rapport à la
meilleure décision disponible, c'est à dire la solution $q^\star$ de 
\begin{equation}
  \maximizeEquation[q \in \Q]{\EU(q)}
\end{equation}
où
\begin{equation}
  \EU(q) \coloneq \E u(R\,q(X)).
\end{equation}
Il convient cependant de réaliser que l'existence d'une décision optimale $q^\star$ finie
n'est pas assurée. En effet, supposons d'une part que l'on dispose d'une utilité neutre au
risque $\rn$, telle que $\rn(r) = r$, et d'autre part que $\E R=0$. Soit $\alpha>0$. On
pourrait alors définir la fonction suivante:
\begin{equation}
  q = \alpha\E(R|X\rangle)
\end{equation}
On aurait alors, du fait de la linéarité du produit scalaire,
\begin{align}
  \EN(q) & = \E(R\,q(X))\\
         & = \E\langle q|(R|X)\rangle\\
         & = \langle q|\E(R|X\rangle)\rangle\\
         & = \alpha\|q\|^2 \geq 0.
\end{align}
On peut alors obtenir une utilité espérée non bornée à mesure que $\alpha\to\infty$. Par ailleurs,
ainsi défini, $q$ représente effectivement la covariance entre $R$ et la projection de $X$
dans l'espace dual de $\Q$; par exemple dans le cas d'un noyau linéaire on aurait
$q = \E(RX^T) = \Cov(R,X)$. On sait qu'en espérance l'application de $q$ à $X$ variera de
la même façon que celle de $R$ et donc qu'on aura une utilité infinie, puisque l'utilité
est neutre.

Pour empêcher une telle situation d'exister on introduit l'hypothèse suivante. Elle exclut
toute forme d'utilité à pente constante pour $r \geq r_0$, notamment l'utilité risque neutre.
\begin{assumption}
  L'utilité croît sous-linéairement, ie. $u(r) = o(r)$.\footnote{Mathématiquement, on exige
    donc que $u(r)/r \to 0$.} 
\end{assumption}

Une autre hypothèse est maintenant nécessaire pour s'assurer que $\qs$ soit borné:
l'absence d'arbitrage. D'un point de vue strictement financier, cela fait certainement du
sens en vertu de l'efficience des marchés, version semi-forte (voir
\cite{fama1970efficient}). Cette hypothèse précise en effet qu'un investisseur doté de
toute l'information publique disponible au sujet d'un titre risqué devrait pas pouvoir
être en mesure de ``battre'' le marché sans prendre de risque. D'un point de vue
théorique, ceci exige en fait qu'il n'y ait pas de région dans $\X$ telle que tous les
rendements s'y produisant soient nécessairement positifs ou négatifs.\todo{Inserer
  image}. Ainsi, même en ayant une conaissance parfaite du monde, il subsistera toujours
un terme de bruit rendant incertains la réalisation des rendements.

\begin{assumption}
  \label{hyp:arb}
  Pour toute région $\mathcal{X}\subseteq\X$,
  \begin{equation}
    \pp\{R \geq 0 \mid X \in \mathcal{X}\} < 1,
  \end{equation}
  et de la même façon avec l'évènement $\pp\{R \leq 0 \mid X \in \mathcal{X}\}$. 
\end{assumption}


\paragraph{Décision optimale finie}

On veut montrer que $\nq{q^\star}$ est borné. Pour ce faire, on va tout d'abord décomposer
$q = s\theta$, où on pose $\nq{\theta}=1$ et $s>0$; ainsi on peut poser notre problème
d'optimisation comme la recherche d'une `direction' $\theta$ et d'une magnitude $s$ dans
$\Q$. De plus, puisque $\nq{q} = s$, il suffit de montrer que $s^\star$ est borné.

Notons d'abord que l'hypothèse \ref{hyp:arb} entraîne en particulier qu'il existe
$\delta > 0$ et $\varrho \geq 0$ tels que
\begin{equation}
  \pp\{R\cdot \theta(X) \leq -\delta\} > \varrho
\end{equation}
pour tout $\theta \in \Q$ tel que $\nq{\theta} = 1$. Définissons maintenant une variable aléatoire à
deux états: $B = -\delta$ avec probabilité $\varrho$ et $B = \rmax\xi$ avec probabilité
$1-\varrho$. Puisque $R\cdot \theta(X) \leq \rmax\xi$, on a alors que, pour tout $r \in \R$,
\begin{equation}
  \pp\{B\geq r\} \geq \pp\{R\cdot \theta(X)\geq r\}
\end{equation}
\todo{voir figure a produire.}

Puisque par hypothèse $u$ est concave et puisque que $B$ domine stochastiquement
$R\cdot \theta(X)$, on a nécessairement que $\E u(sB) \geq \E u(R\cdot s\theta(X))$, pour tout
$s > 0$. Or, par hypothèse de sous-linéarité on obtient que
\begin{align}
  \lim_{s\to\infty}\E u(R\cdot s\theta(X)) &\leq \lim_{s\to\infty}u(sB)\\
                           & = \lim_{s\to\infty}(\varrho u(-s\delta) + (1-\varrho)u(s\rmax\xi))\\
                           & \leq\lim_{s\to\infty}-\varrho s \delta + (1-\varrho)o(s) = -\infty,
\end{align}
ce qui démontre bien que $s$ est borné.

\subsubsection{Garanties de sous optimalité}

Soit \textit{l'erreur de sous optimalité en util} $\zeta(\S_n)$ définie par
\begin{equation}
  \zeta(\S_n) \coloneq \EU(\alg(\S_n)) - \EU(\qs)
\end{equation}
et \textit{l'erreur de sous optimalité en rendement} $\zeta_e(\S_n)$ définie par
\begin{equation}
  \zeta_e(\S_n) = \CE(\alg(\S_n)) - \CE(\qs).
\end{equation}
Ces deux notions d'erreur indiquent alors, en terme d'util ou de rendement, à quel point
la performance hors échantillon est éloignée de la performance suivant la politique
optimale $\qs$. Les deux théorèmes suivant précisent comment se comportent ces deux
erreurs.
\begin{thm}[Erreur de sous optimalité (util)]
  \label{thm3}
  Avec probabilité d'au moins $1-\delta$, l'erreur de sous optimalité en util est bornée par
  \begin{equation}
    \zeta(\S_n) \leq \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n} +
    \frac{4\gamma\rmax\xi^2}{\lambda}\sqrt\frac{32+\log(1/\delta)}{n} + \frac{\lambda}{2}\|\qs\|^2 
  \end{equation}
\end{thm}

\begin{thm}[Erreur de sous optimalité (équivalent certain)]
  \label{thm4}
  Avec probabilité d'au moins $1-\delta$, l'erreur de sous optimalité exprimée en équivalent
  certain est bornée par
  \begin{equation}
    \zeta_e(\S_n) \leq \frac{1}{\partial u(\nCE)}\left( \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n} +
      \frac{4\gamma\rmax\xi^2}{\lambda}\sqrt\frac{32+\log(1/\delta)}{n} + \frac{\lambda}{2}\|\qs\|^2 \right),
  \end{equation}
où $\nCE = \CE(\alg(\S_n))$ représente l'équivalent certain de la politique $\qh =
\alg(\S_n)$ hors échantillon. À défaut de la connaître, on lui substituera son
approximiation $\nhCE(\S_n,\alg(\S_n))$. 
\end{thm}


Les bornes de sous-optimalité convergent ainsi environ à la même vitesse que celle de
généralisation, c'est-à-dire dans un régime de $\bigO(1/\sqrt{n})$. Bien sûr, une
différence majeure est la présence de $\nq{\qs}$ qui est a priori impossible à déterminer,
dans la mesure où aucune hypothèse n'est faite sur la distribution de $M$. On constate
d'ailleurs sans surprise qu'une faible valeur de régularisation permet au résultat
algorithmique de se raprocher du résultat optimal, bien que les autres termes de la borne
aient un effet inverse. Par ailleurs, le sur-gradient inverse de $u$ à $\nCE$ ne peut lui
non plus être déterminé précisément, aussi pour estimer la borne on lui substituera
$\partial u^{-1}(\nhCE)$.

En fait, ces bornes de sous optimalité établissent rigoureusement de quelle façon la
régularisation devrait être décrue afin de s'assurer de converger vers la solution
optimale tout en contrôlant l'erreur de généralisation. Par exemple, en choisissant un
facteur de régularisation $\lambda = \bigO(n^{-1/4})$, toute chose étant égale par ailleurs, on
s'assure une convergence vers une sous optimalité nulle à un rythme
$\bigO(n^{-1/4})$. Cependant, alors que l'erreur de généralisation est normalement de
$\bigO(n^{-1/2})$ lorsque $\lambda$ est constant, l'erreur de généralisation ne décroit alors
plus qu'à un rythme $\bigO(n^{-1/4})$. La Section \ref{sec:emp} sera l'occasion de mesurer
en pratique la validité de ces ordres de convergence. 




\subsection{Garanties et dimensionalité du problème}
\label{b:dim}

Toutes les bornes considérées jusqu'à présent ont été dérivées sans faire apparaître
explicitement la relation qui les lient avec avec la dimension $p$ de l'espace $\Q$;
autrement dit, on a implicitement considéré que cette dimension était
constante. Cependant, à mesure qu'on ajoute de nouvelles variables de marché, on s'expose
en fait à un risque de généralisation et de sous optimalité croissant puisque $\Q$ dispose
alors de degrés de liberté supplémentaires.

La première chose à prendre en considération est l'effet du noyau $\kappa$ ou de la projection
$\phi:\X\to\phi(\X)$ sur la dimension de $\Q$ lorsqu'on ajoute à $\X$ de nouvelles variables de
marché. Dans le cas le plus simple où $\Q = \X$, alors nécessairement la dimension des
deux espaces concordent:
\begin{equation}
  \dim\Q = \dim\X = p.
\end{equation}
Par contre, si on considère par exemple un noyau polynômiale de degré 2 tel que
$\kappa(x_i,x_j) = (x_i^Tx_i)^2$, alors $\dim\Q = (p+1)(p+2)/2$. En fait, (voir
\cite{mohri2012foundations} ou \cite{bishop2006pattern}), si un noyau polynômial de degré
$k$ est utilisé, alors
\begin{equation}
  \dim\Q = \binom{p+k}{k} = \frac{1}{k!}\prod_{j=1}^k(p+j) = \bigO(n^k).
\end{equation}

Néanmoins, on a vu à la Section \ref{sec:kernel} que la solution $\qh$ d'un problème donné
pouvait être obtenue indépendamment de la dimension de $\Q$. Toutefois, cette dimension
jouera un rôle dans la borne $\kappa(x,x) \leq \xi^2$ qu'on retrouve dans les bornes de
généralisation et de sous optimalité des Théorèmes \ref{thm1}, \ref{thm2}, \ref{thm3} et
\ref{thm4}.

Si on prend le cas simple d'un noyau linéaire, cette variable correspond alors simplement
à la norme maximale du vecteur d'information: $\xi^2 = \max\|X\|^2$.  Par contre, si on
avait plutôt employé un noyau polynômial de degré $k$, en supposant que $\|X\|^2 \leq \nu^2$, la
borne à employer serait donnée par
\begin{equation}
\kappa(X,X) = (X^TX + 1)^k \leq (\xi^2 + 1)^k.
\end{equation}
Par contre, les noyaux affines de forme $\kappa(x_i,x_j) = \kappa(\|x_i - x_j\|)$ (comme par exemple
le noyau gaussien) auront nécessairement une borne $\xi^2$ constante puisque
$\kappa(x,x) = \kappa(0)$ peu importe la dimension initiale de $\X$.

On comprend cependant de cette discussion que dans le cas où $\kappa$ n'impose aucune borne
naturelle au produit scalaire des transformations $\phi$, il est nécessaire de prendre les
mesures nécessaires pour faire respecter la contrainte $\kappa(x,x)\leq\xi^2$. Mais il y a en fait
plusieurs façons d'y parvenir. Pour ce faire, nous allons considérer à
des fins de simplification uniquement le cas linéaire où $\kappa(x_i,x_j) = x_i^Tx_j$.

D'abord, on peut décider d'imposer une borne rigide à chaque variable de marché: $X_j^2 \leq
\nu_j^2$. On alors 
\begin{equation}
  \kappa(X,X) = \sum_{j=1}^p X_j^2 \leq \|\nu\|^2.
\end{equation}
Pour ce faire, on peut supposer que les variables sont déjà bornées naturellement : en
utilisant notre connaissance de du domaine, on peut juger que chacune de ces variables ne
peut qu'avoir des réalisations dans un intervalle fini. Mais si on peut également
\textit{saturer} les variables de marché par une certaine borne $\nu_j$. Par exemple, une
variable de marché $X_j$ dont l'amplitude est incertaine pourrait être remplacé par la
variable $\tilde X_j$ par 
\begin{equation}
  \tilde X_j = \begin{cases}
    X_j & |X_j| \leq \nu_j\\
     \operatorname{signe}(X_j)\nu_j & |X_j| > \nu_j,
  \end{cases}
\end{equation}
quitte à introduire une nouvelle variable de marché indiquant si $\tilde X_j$ est saturé
ou non.

Il y a également une autre façon d'obtenir une borne sur $\|X\|^2$ sans limiter
individuellement le support de chaque variable de marché. Plusieurs théorèmes, en
utilisant des hypothèses plus ou moins fortes, permettent en effet d'affirmer qu'un
phénomène de concentration de la norme aléatoire $\|X\|^2$ autour de $\E\|X\|^2$ aura lieu
à mesure que $p$ croît.

Par exemple, si les variables de marché $X_j$ sont indépendantes l'une à l'autre et que
$X_j^2$ a une distribution sous exponentielle\footnote{Tel est le cas par exemple de $Z^2$
  lorsque $Z \sim \mathscr{N}(0,1)$. Précisément, une variable aléatoire $Z$ est sous
  exponentielle s'il existe deux paramètres $\nu$ et $\beta$ non négatifs tels que
  $\E e^{\lambda(Z-\E Z)} \leq e^{\nu^2\lambda^2/2}$ pour tout
  $|\lambda| \leq \beta^{-1}$. Voir \cite{boucheron2013concentration}.}, l'inégalité de Bernstein
implique qu'avec probabilité $1-\delta$,
\begin{equation}
  |\|X\|^2 - \E\|X\|^2| \leq \sqrt\frac{2\omega\log(1/\delta)}{p}
\end{equation}
où $\omega$ caractérise $\|X\|^2$.  Autrement dit, à mesure que $p$ croît, la norme
$\|X\|^2$ sera fortement concentrée autour de son espérance. En supposant que chaque
variable est standardisée, alors $\E\|X\|^2 = \sum_{j=1}^pX_j^2 = p$, puisque les composantes
$X_j$ sont indépendantes. Ainsi, $\xi^2 \approx p$ avec haute probabilité.

En fait, on peut parvenir au même type de constat en n'utilisant que l'inégalité de
Markov. Ainsi, quelle que soit la loi de $M$, on aura
\begin{equation}
  \pp\{\|X\|^2 \ge t\E\|X\|^2\} \leq \frac{1}{t}
\end{equation}
et on retrouve donc une fois de plus, dans des conditions beaucoup plus générales, une
concentration autour de $\E\|X\|^2$. 

En fait, le point à retenir est que $\xi^2 = \bigO(p)$ dans un contexte d'hypothèses assez
faibles sur la loi de marché. C'est à dire que les bornes dérivées aux Théorèmes
\ref{thm1}, \ref{thm2}, \ref{thm3} et \ref{thm4}, qu'on croyait
$\bigO(n^{-1/2})$ sont en fait $\bigO(pn^{-1/2})$. Ces bornes exposent donc un
danger potentiel à un investisseur. Par exemple, dans un régime qu'on pourrait qualifier
de \textit{big data} où $p=\bigO(n)$, les bornes sur l'erreur de généralisation et de sous
optimalité seraient en fait \textit{divergentes!} à un rythme $\bigO(n^{1/2})$. Il faut en
fait imposer un rythme $p = \omega(n^{1/2})$ pour s'assurer d'une convergence vers une erreur
nulle.

Nous verrons cependant à la Section \ref{sec:emp} le comportement de l'erreur empirique
par rapport à ces ordres de grandeur asymptotiques. En particulier, il faut bien
comprendre que ces garanties forment une \textit{borne supérieure} impliquant \textit{qu'au plus}
l'erreur croît à un rythme $\bigO(p)$. 
Cependant, cette discussion n'est valide que dans le cas particulier des noyaux
linéaires. Les noyaux gaussiens conservent quant à eux une indépendance par rapport à la
dimensionalité, alors que les noyaux polynomiaux l'exacerbent; pour un noyau de degré $k$
il devient plus juste d'indiquer
\begin{equation}
  \hat\zeta \leq \bigO(p^k/\lambda\sqrt{n}).
\end{equation}

\subsection{Conclusion}

Cette section a permis d'obtenir des garanties probabilistes sur les erreurs de
généralisation et de sous optimalité de l'équivalent certain. Dans les deux cas, une
décroissance de $\bigO(\xi^2/n^{-1/2})$ a lieu. On a également indiqué de quelle façon
devrait être réduit le facteur de régularisation $\lambda$ pour conserver ces garanties tout en
convergeant vers une erreur de sous optimalité nulle.

La Section suivante illustrera empiriquement le comportement de ces garanties. En
employant un environnement contrôlé, il sera possible d'établir si ces bornes sont serrées
ou non, et si elles permettent bien de déduire l'ordre de décroissance de l'erreur. 






%%% Local Variables:
%%% mode: latex
%%% TeX-master: "memoire"
%%% End:
