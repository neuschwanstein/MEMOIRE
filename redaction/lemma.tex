\section{Démonstrations}

\subsection{Lemmes}

\begin{lemme}[Décision neutre au risque]
  \label{lem:ndef}
  La solution au problème
  \begin{equation}
    \maximizeEquation[q \in \Q]{\hEN(q) - \frac{\lambda}{2}\|q\|^2}
  \end{equation}
  est donnée par
  \begin{equation}
    \qh_1 = \frac{1}{n\lambda}\sumi r_i|x_i\rangle.
  \end{equation}
  On en déduit par ailleurs que
  \begin{equation}
    \hEN = \lambda\langle\qh_1|
  \end{equation}
  et donc que
  \begin{equation}
    \hEN(\qh_1) = \lambda\|\qh_1\|^2
  \end{equation}
  et
  \begin{equation}
    \hEN_\lambda(\qh_1) = \frac{\lambda}{2}\|\qh_1\|^2.
  \end{equation}
\end{lemme}

\begin{proof}
  Si on considère un déplacement de décision $\qh_1 + \Delta q$, alors par linéarité le premier
  terme de l'objectif devient $\hEN(\qh_1+\Delta q) = \hEN(\qh_1) + \hEN(\Delta q)$ et le terme de
  régularisation devient
  \begin{equation}
    -\lambda/2\|\qh_1 + \Delta q\|^2 = -\lambda/2\|\qh_1\|^2 - \lambda\braket{\qh_1|\Delta q} - \lambda/2\|\Delta q\|^2.
  \end{equation}
  On a donc
  \begin{align}
    \hEN_\lambda(\qh_1)  - \hEN_\lambda(\qh_1+\Delta q) &= -\hEN(\Delta q) + \lambda\braket{\qh_1|\Delta q} + \lambda/2\|\Delta q\|^2\\
                                       &= -\lambda\braket{\qh_1|\Delta q} + \lambda\braket{\qh_1|\Delta q} +
                                         \lambda/2\|\Delta q\|^2\\
                                       &= \lambda/2\|\Delta q\|^2 \geq 0,
  \end{align}
  Ce qui entraîne $\hEN_\lambda(\qh_1) \geq \hEN_\lambda(\qh_1 + \Delta q)$ pour tout déplacement $\Delta q\in\Q$. 
\end{proof}


\begin{lemme}[Forte concavité de l'objectif]
  \label{lem:conv}
  Soit $\S_n$ un ensemble d'entraînement et $\qh = \argmax\EU_\lambda(q)$ la décision
  régularisée optimale. Alors pour toute décision $q \in \Q$,
  \begin{equation}
    \frac{\lambda}{2}\|\qh - q\|^2 \leq \hEU_\lambda(\qh) - \hEU_\lambda(q).
  \end{equation}
\end{lemme}

\begin{rem}
  Naturellement, ce résultat s'applique aussi à l'opérateur d'utilité espérée hors
  échantillon hors échantillon $\EU_\lambda$ autour de $\qsl$, où
  $\qsl = \argmax_q\EU_\lambda(q)$. La démonstration demeure en effet identique dans les deux
  cas.
\end{rem}



\begin{lemme}[Forte concavité II]
  L'objectif est fortement concave, que ce soit sous sa version statistique $\hEU_\lambda$ ou
  probabiliste $\EU_\lambda$. Autrement dit, pour tout $\alpha \in [0,1]$, on a
  \begin{equation}
    \EU_\lambda(tq_1 + (1-\alpha )q_2) \geq \alpha \EU_\lambda(q_1) + (1-\alpha)\EU_\lambda(q_2) + \lambda \alpha(1-\alpha)\nq{q_1-q_2}^2,
  \end{equation}
  et de même pour $\hEU_\lambda$. Effectivement, puisque $u$ est concave et $\nq{\cdot}^2$ est
  convexe, on a successivement:
  \begin{align}
    & \EU_\lambda(\alpha q_1 + (1-\alpha )q_2)\\
    &\qquad= \E u(R\cdot (\alpha q_1+(1-\alpha )q_2)(X)) - \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
    &\qquad= \E u(\alpha (R\cdot q_1(X)) + (1-\alpha )(R\cdot q_2(X)))- \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
    &\qquad\geq \E(\alpha  u(R\cdot q_1(X)) + (1-\alpha )u(R\cdot q_2(X))) - \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
    &\qquad= \alpha\EU(q_1) + (1-\alpha)\EU(q_2) - \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
    &\qquad= \alpha\EU_\lambda(q_1) + (1-\alpha)\EU_\lambda(q_2) - \lambda(\nq{\alpha q_1+(1-\alpha)q_2}^2 - \alpha\nq{q_1}^2 -
      (1-\alpha)\nq{q_2}^2).
  \end{align}
  Mais d'autre part,
  \begin{align}
    &-\lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2 + \lambda\alpha\nq{q_1}^2+\lambda(1-\alpha)\nq{q}^2\\
    &\qquad = \lambda\alpha(1-\alpha)(\nq{q_1}^2 + \nq{q_2}^2 - 2\langle q_1,q_2\rangle)\\
    &\qquad = \lambda\alpha(1-\alpha)\nq{q_1-q_2}^2,
  \end{align}
  Ce qui complète la démonstration. La dérivation demeure exactement la même lorsqu'on
  considère $\hEU_\lambda$.
\end{lemme}

\begin{lemme}[Sigma admissibilité]
  \label{lem:sig}
  Soit $q,q' \in \Q$ deux vecteurs de décision et $(x,r) \in \M$ deux points quelconques du
  support de la loi de marché. Alors
  \begin{equation}
    |u(r\,q(x)) - u(r\,q'(x))| \leq \gamma\rmax\xi\|q-q'\|.
  \end{equation}
\end{lemme}

\begin{proof}
  D'abord avec la propriété Lipschitz de $u$ puis par l'hypothèse $|r|\leq\rmax$, on obtient
  \begin{align}
    |u(r\,q(x)) - u(r\,q'(x))| &\leq \gamma\rmax|(q(x) - q'(x))|.\\
    \intertext{Puis en notation vectorielle on obtient}
                               &=\gamma\rmax|\braket{q|x} - \braket{q'|x}|\\
                               &=\gamma\rmax|\langle q-q'|x \rangle|\\
                               &\leq\gamma\rmax\xi\|q-q'\|
  \end{align}
  successivement par Cauchy Schwartz et par l'hypothèse $\kappa(x,x)\leq\xi$, ce qui complète la
  démonstration.
\end{proof}

\begin{lemme}[Stabilité]
  \label{lem:stab}
  Soit $\S_n$ et $S_n'$ deux ensembles d'entraînement ne différant qu'à leur $j$\ieme
  point:
  \begin{align}
    \S_n &= \{(x_1,r_1),\ldots,(x_j,r_j),\ldots,(x_n,r_n)\}\\
    \S_n' &= \{(x_1,r_1),\ldots,(x'_j,r'_j),\ldots,(x_n,r_n)\},
  \end{align}
  et soit $\qh = \alg(\S_n)$ et $\qh' = \alg(\S_n')$ les deux décisions optimales
  correspondantes. Alors
  \begin{equation}
    \|\qh - \qh'\| \leq \frac{2\gamma\xi\rmax}{\lambda n}.
  \end{equation}
\end{lemme}

\begin{rem}
  Cette propriété a été démontrée par \cite{bousquet2002stability}. Nous en donnons ici
  une version simplifiée et adaptée à la situation. Voir aussi \cite{mohri2012foundations}
  pour une démonstration dans un contexte général.
\end{rem}

\begin{proof}
  Posons $\hEU = \hEU(\S_n,\cdot)$ et $\hEU' = \hEU(\S_n',\cdot)$. Du Lemme \ref{lem:conv}, on
  obtient
  \begin{equation}
    \lambda\|\qh - \qh'\|^2 \leq \hEU_\lambda(\qh) - \hEU_\lambda(\qh') + \hEU'_\lambda(\qh') - \hEU'_\lambda(\qh).
  \end{equation}
  Les termes de régularisation s'annulent et on obtient donc:
  \begin{equation}
    \lambda\|\qh - \qh'\|^2 \leq \hEU(\qh) - \hEU(\qh') + \hEU'(\qh') - \hEU'(\qh)
  \end{equation}
  Ces deux différences font disparaître tous les termes, excepté le
  $j$\ieme:
  \begin{align}
    \lambda\|\qh - \qh'\|^2 \leq &\,n^{-1}(u(r_j\qh(x_j)) - u(r_j\qh'(x_j)))\, + \\
                        & \qquad n^{-1}(u(r_j'\qh'(x_j')) - u(r_j'\qh'(x_j'))).
  \end{align}
  D'autre part, cette somme est positive par le terme de gauche, on peut donc
  successivement appliquer l'opérateur valeur absolue, l'inégalité du triangle et
  le résultat du Lemme \ref{lem:sig} pour obtenir
  \begin{equation}
    \lambda\|\qh - \qh'\|^2 \leq \frac{2}{n}\gamma\rmax\xi\|\qh - \qh'\|,
  \end{equation}
  d'où on tire le résultant annoncé.
\end{proof}

\begin{lemme}[Décision limite]
  \label{lem:dl}
  Soit $\S_n$ un ensemble d'entraînement, $\qh_u = \argmax_q\hEU_\lambda(q)$ la solution au
  problème pour une utilité $u$ quelconque (respectant les hypothèses) et
  $\qh_1 = \argmax_q\hEN_\lambda(q)$ la solution au problème risque neutre. Alors
  \begin{equation}
    \|\qh_u\| \leq \|\qh_1\| \leq \frac{\rmax\xi}{\lambda}.
  \end{equation}
\end{lemme}

\begin{proof}
  D'abord, par la propriété de forte concavité de $\hEU_\lambda(\qh_u)$ (Lemme \ref{lem:conv}),
  en posant $q=0$, on obtient, $\frac{\lambda}{2}\|\qh_u\|^2 \leq \hEU_\lambda(\qh_u)$, ou encore
  $\lambda\|\qh_u\|^2 \leq \hEU(\qh_u)$.

  On a par ailleurs,
  \begin{equation}
    \hEU(\qh_u) \leq u\big(\hEN(\qh_u)\big) \leq \hEN(\qh_u)
  \end{equation}
  en appliquant successivement l'inégalité de Jensen et l'inégalité $u(x) \leq x$. Ainsi, en
  appliquant Cauchy Schwartz et le résultat du Lemme \ref{lem:ndef},
  \begin{equation}
    \lambda\|\qh_u\|^2 \leq \hEN(\qh_u) =  \lambda\langle\qh_1|\qh_u\rangle \leq \lambda\|\qh_1\|\|\qh_u\|
  \end{equation}
  pour obtenir $\|\qh_u\| \leq \|\qh_1\|$. On obtient la deuxième inégalité simplement avec
  la définition de $\qh_1 = \lambda^{-1}\sumi r_i|x_i\rangle$ et en appliquant les hypothèses
  $r_i\leq\rmax$ et $|x_i\rangle \leq \xi$.
\end{proof}

\begin{lemme}[Domaine d'utilité limite]
  \label{lem:domu}
  Soit $\qh = \alg(\S_n)$ et $\qh = \alg(\S_n')$ deux décision obtenues à partir de deux
  ensembles d'entraînement $\S_n,S_n' \sim M^n$, et soit $(x,r),(x',r') \in \M$ deux points du
  domaine de marché. Alors
  \begin{equation}
    |u(r\,q(x)) - u(r'\,q'(x'))| \leq \lambda^{-1}(\gamma+1)\rmax^2\xi^2.
  \end{equation}
\end{lemme}

\begin{proof}
  Du Lemme \ref{lem:dl} et par hypothèse, on sait que pour tout $(x,r) \in \M$,
  $|r\,\qh(x)| \leq \lambda^{-1}\rmax^2\xi^2$. De plus, $u(x)\leq x$ sur tout le domaine de
  $u$ et $\gamma x \leq u(x)$ si $x<0$ par hypothèse Lipschitz. On en déduit donc que
  \begin{equation}
    \lambda^{-1}\gamma\rmax^2\xi^2 \leq u(r\,\qh(x)) \leq \lambda^{-1}\rmax^2\xi^2.
  \end{equation}
  Donc au plus, deux valeurs d'utilité ne peuvent différer que de $\lambda^{-1}(\gamma+1)\rmax^2\xi^2$.
\end{proof}

\begin{lemme}[Théorème de McDiarmid]
  \label{lem:mcdiarmid}
  Soit $\S_n$ et $\S_n'$ deux ensembles d'entraînement échantillonés à partir d'une
  quelconque variable aléatoire réelle $D$ supportée par $\bm D$ et ne
  différant que d'un seul point, et soit $g:{\bm D}^n\to\Re$ telle que
  \begin{equation}
    |g(\S_n) - g(\S_n')| \leq c.
  \end{equation}
  Alors pour tout $\epsilon>0$ et pour tout échantillon aléatoire $\S_n \sim D^n$,
  \begin{equation}
    \pp\{g(\S_n) - \E g(\S_n) \geq \epsilon\} \leq \exp\left(-\frac{2\epsilon^2}{nc^2}\right).
  \end{equation}
\end{lemme}

\begin{rem}
  De façon équivalente, en posant
  \begin{equation}
    \delta = \exp\left(-\frac{2\epsilon^2}{nc^2}\right),
  \end{equation}
  alors on aura, avec probabilité au moins $1-\delta$, $g(\S_n) < \epsilon + \E g(\S_n)$. Autrement
  dit, avec probabilité au moins $1-\delta$, l'évènement suivant aura lieu:
  \begin{equation}
    g(\S_n) \leq \E g(\S_n) + \sqrt{\frac{nc^2\log(1/\delta)}{2}}.
  \end{equation}
\end{rem}

\begin{proof}
  Consulter \cite{mohri2012foundations} ou \cite{boucheron2013concentration}.
\end{proof}

\begin{lemme}[Inégalité de Shalev-Schwartz]
  \label{lem:ss}
  La forte concavité de $\EU_\lambda$ (voir Lemme \ref{lem:conv}) fait en sorte que pour toute
  solution $\qh = \alg(\S_n)$, la sous optimalité dans l'objectif sera bornée avec
  probabilité d'au moins $1-\delta$ par
  \begin{equation}
    \EU_\lambda(\qh^\star_\lambda) - \EU_\lambda(\qh) \leq \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n}
  \end{equation}
  où $\qh^\star_\lambda$ est la solution de
  \begin{equation}
    \maximizeEquation[q \in \Q]{\EU_\lambda(q).}
  \end{equation}
\end{lemme}

\begin{proof}
  Voir le résultat principal de \cite{sridharan2009fast}.
\end{proof}


\begin{lemme}[Borne sur l'équivalent certain]
  \label{lem:ce}
  Soient $\nCE_1 = u^{-1}(\nEU_1)$ et $\nCE_2 = u^{-1}(\nEU_2)$ et soit une borne $\Omega_u$ telle
  que
  \begin{equation}
    \nEU_1 \geq \nEU_2 - \Omega_u.
  \end{equation}
  Par définition du sur-gradient, pour tout $r \in \Re$,
  $u(r+\Delta) \leq u(r) + \Delta\cdot\partial u(r)$. Donc en posant
  $\Delta = \nCE_1 - \nCE_2$ et $r=\nCE_2$, on obtient ces deux inégalités:
  \begin{equation}
    -\Omega_u \leq \nEU_1 - \nEU_2 = u(\nCE_1) - u(\nCE_2) \leq \partial u(\nCE_2)(\nCE_1 - \nCE_2).
  \end{equation}
  On trouve ainsi:
  \begin{equation}
    \nCE_1 \geq \nCE_2 - \Omega_u\cdot \partial u^{-1}(\nCE_2).
  \end{equation}
  Typiquement, $\nCE_1$ et $\nEU_1$ seront des quantités inobservables, alors que $\nCE_2$
  et $\nEU_2$ seront des quantités calculables. De plus, si $\partial u^{-1}(\nCE_2)$ comporte
  plusieurs éléments (\eg\ si la dérivée de $u$ est discontinue à $\nCE_2$), on choisira
  l'élément le plus favorable; la plupart du temps ce sera équivalent à
  $\lim_{r\to\nCE_2^{-}}1/u'(r)$ dans la région où $1/u'(r)$ est défini. Enfin, on note que
  cette limite existe puisque $u$ est strictement monotone, et donc sa pente ne s'annule
  nulle part.
\end{lemme}


\subsection{Erreur de généralisation}

\paragraph{Théorème 1.} On rappelle qu'on veut démontrer que pour tout ensemble
d'entraînement, avec probabilité $1-\delta$, 
\begin{equation}
  \zeta(\S_n) = \EU(\alg(\S_n)) - \hEU(\S_n,\alg(\S_n)) \leq \Omega_u.
\end{equation}

\begin{proof}
  L'idée est en fait d'appliquer le théorème de McDiarmid (énoncé au Lemme
  \ref{lem:mcdiarmid}) à l'erreur de généralisation $\hat\zeta(\S_n)$. Pour ce faire, on va
  donc d'abord chercher à borner la différence d'erreur entraînée par deux fonctions de
  décision $\qh = \alg(\S_n)$ et $\qh' = \alg(\S_n')$, où $\S_n$ et $\S_n$ ne diffèrent
  que d'un seul point qu'on supposera être le $j$\ieme.

  Formellement, si on pose
  \begin{align}
    \S_n &= \{(x_1,r_1),\ldots,(x_j,r_j),\ldots,(x_n,r_n)\}\\
    \S_n' &= \{(x_1,r_1),\ldots,(x'_j,r'_j),\ldots,(x_n,r_n)\}.
  \end{align}
  Alors
  \begin{align}
    |\hat\zeta(\qh) - \hat\zeta(\qh')| &= |\EU(\qh) - \hEU(\qh) -\EU(\qh') + \hEU'(\qh')|\\
                               &\leq |\EU(\qh) - \EU(\qh')| + |\hEU(\qh) - \hEU'(\qh')|\label{eq:wtv1}
  \end{align}
  Par le théorème de Jensen appliqué à la fonction valeur absolue, on obtient du premier
  terme que
  \begin{align}
    |\EU(\qh) - \EU(\qh')| &= |\E(u(R\cdot\qh(X)) - u(R\cdot\qh'(X)))|\\
                           &\leq \E|u(R\cdot\qh(X)) - u(R\cdot\qh'(X))|\\
                           &\leq \gamma\rmax\xi\|\qh-\qh'\|\\
                           &\leq \frac{2\gamma^2\rmax^2\xi^2}{\lambda n},
  \end{align}
  en appliquant successivement les Lemmes \ref{lem:sig} et \ref{lem:stab}. Quant au
  deuxième terme de \eqref{eq:wtv1} on peut le borner de la façon suivante:
  \begin{align}
    &|\hEU(\qh) - \hEU'(\qh')|\nonumber\\
    &\qquad = \frac{1}{n}\bigg|u(r_j\qh(x_j)) - u(r_j'\qh'(x_j')) + \sum_{\substack{i=1\\i\neq
    j}}^n\big(u(r_i\qh(x_i)) - u(r_i\qh'(x_j))\big)\bigg|,
  \end{align}
  qu'on peut décomposer en deux termes en appliquant l'inégalité du triangle. Le premier
  terme:
  \begin{equation}
    n^{-1}|u(r_j\qh(x_j)) - u(r_j'\qh'(x_j'))| \leq \frac{(\gamma+1)\rmax^2\xi^2}{\lambda n}
  \end{equation}
  en appliquant le résultat du Lemme \ref{lem:domu}. En appliquant une deuxième fois les
  Lemmes \ref{lem:sig} et \ref{lem:stab}, le deuxième terme est borné par
  \begin{equation}
    \frac{1}{n}\bigg|\sum_{\substack{i=1\\i \neq j}}^n\big(u(r_i\qh(x_i)) -
    u(r_i\qh'(x_j))\big)\bigg| \leq \frac{n-1}{n}\frac{2\gamma^2\rmax^2\xi^2}{\lambda n} \leq \frac{2\gamma^2\rmax^2\xi^2}{\lambda n}.
  \end{equation}

  Une fois toutes réunies, ces inégalités donnent donc
  \begin{align}
    |\hat\zeta(\qh) - \hat\zeta(\qh')| &\leq \frac{2\gamma^2\rmax^2\xi^2}{\lambda n} + \frac{(\gamma+1)\rmax^2\xi^2}{\lambda n}
                                 + \frac{2\gamma^2\rmax^2\xi^2}{\lambda n}\\
                               & = \frac{(4\gamma^2 + \gamma + 1)\rmax^2\xi^2}{\lambda n}.
  \end{align}

  On peut alors directement appliquer le corrolaire du Théorème de McDiarmid (Lemme
  \ref{lem:mcdiarmid}). On trouve donc qu'avec probabilité au moins $1-\delta$, on aura
  \begin{equation}
    \hat\zeta(\S_n) \leq \E\hat\zeta(\S_n) + \frac{(4\gamma^2 + \gamma +
      1)\rmax^2\xi^2}{\lambda}\sqrt{\frac{\log(1/\delta)}{2n}} 
  \end{equation}

  \todo{Mais},
  \begin{equation}
    \E\hat\zeta(\S_n) \leq \frac{2\gamma\rmax^2\xi^2}{\lambda n}
  \end{equation}
  et donc,
  \begin{equation}
    \hat\zeta(\S_n) \leq \frac{2\gamma\rmax^2\xi^2}{\lambda n} + \frac{(4\gamma^2 + \gamma +
        1)\rmax^2\xi^2}{\lambda}\sqrt{\frac{\log(1/\delta)}{2n}}
  \end{equation}
\end{proof}
ce qui correspond au résultat annoncé.

\paragraph{Théorème 2.}

Le Lemme \ref{lem:ce} permet alors directement d'inverser le domaine des résultats afin de
l'exprimer en rendement et non pas en unités \textit{d'util}.


\subsection{Théorèmes de sous optimalité}

De l'inégalité de Shalev-Schwartz (Lemme \ref{lem:ss}), on sait qu'avec une probabilité
d'au moins $1-\delta$, avec $0<\delta<1$, l'évènement suivant aura lieu:
\begin{equation}
  \EU_\lambda(\qsl) - \EU_\lambda(\qh) \leq \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n}.
\end{equation}
Donc de façon équivalente, on aura également
\begin{align}
  \EU(\qsl) - \EU(\qh) &\leq \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n} + \frac{\lambda}{2}\|\qsl\|^2 -
                         \frac{\lambda}{2}\|\qh\|^2\\
                       &= \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n} + \frac{\lambda}{2}\langle\qsl - \qh |
                         \qsl + \qh\rangle\\
                       &\leq \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n} +
                         \frac{\lambda}{2}\|\qsl - \qh\|\|\qsl + \qh\|\\
                       &\leq \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n} + \frac{\lambda}{2}\|\qsl -
                         \qh\|\big(\|\qsl\| + \|\qh\|\big).
\end{align}
Cette expression peut être bornée par des constantes si on considère d'abord le Lemme
\ref{lem:dl} duquel on sait que $\|\qh\|$ et $\|\qsl\|$ sont bornés par
$\rmax\xi/\lambda$. D'autre part, en combinant la propriété de forte convexité de
$\EU_\lambda$ (Lemme \ref{lem:conv}) et l'inégalité de Shalev Schartz, on aura, toujours avec le
même $\delta$,
\begin{equation}
  \frac{\lambda}{2}\|\qh - \qsl\|^2 \leq \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n},
\end{equation}
ou encore
\begin{equation}
  \|\qh - \qsl\| \leq \frac{4\gamma\xi}{\lambda}\sqrt\frac{32+\log(1/\delta)}{n}.
\end{equation}

Ainsi, en simplifiant, 
\begin{equation}
  \label{arg1}
  \EU(\qsl) - \EU(\qh) \leq \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n} +
  \frac{4\gamma\rmax\xi^2}{\lambda}\sqrt\frac{32+\log(1/\delta)}{n}.
\end{equation}
Mais d'autre part, par définition de $\qsl$, $\EU_\lambda(\qs) \leq \EU_\lambda(\qsl)$, \ie,
\begin{align}
  \label{arg2}
  \EU(\qs) - \EU(\qsl) &\leq \frac{\lambda}{2}\|\qs\|^2 - \frac{\lambda}{2}\|\qsl\|^2 \leq \frac{\lambda}{2}\|\qs\|^2
\end{align}
Et donc en combinant les inégalités \eqref{arg1} et \eqref{arg2}:
\begin{equation}
  \EU(\qs) - \EU(\qh) \leq \frac{8\gamma^2\xi^2(32+\log(1/\delta))}{\lambda n} +
  \frac{4\gamma\rmax\xi^2}{\lambda}\sqrt\frac{32+\log(1/\delta)}{n} + \frac{\lambda}{2}\|\qs\|^2,
\end{equation}
ce qui correspond bien au résultat annoncé. De plus, le Lemme \ref{lem:ce} permet une fois
de plus d'exprimer ce résultat en terme d'équivalent certain.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "memoire"
%%% End:
