\section{Garanties statistiques}





\subsection{Bornes de généralisation}




\paragraph{Exposition du problème}

Soit $\Q$ un espace de Hilbert à noyau reproduisant induit par $\kappa$ et soit un ensemble
d'entraînement $\S_n = \{(x_i,y_i)\}_{i=1}^n \sim M^n$ échantilloné à partir de la
distribution de marché. Alors on peut définir \textit{l'algorithme de décision}
$\alg:\M^n \to \Q$ par
\begin{equation}
  \label{b:basic}
  \alg(\S_n) = \argmax_{q \in \Q} \left\{\widehat\EU(\S_n,q) - \lambda\nq{q}^2\right\}.
\end{equation}

Comme on l'a vu, résoudre \eqref{b:basic} est aussi équivalent à
\begin{equation}
  \maximizeEquation[\alpha \in \Re^n]{n^{-1} \sumi u(r_i\,\alpha^T\phi(x_i)) - \lambda \|\alpha\|^2_K,}
\end{equation}
où $\phi:\Re^p \to \Re^n$ le vecteur d'application induit par la matrice d'information $\Xi$. La
relation $q = \alpha^T\phi$ permet de passer d'une représentation à l'autre.

La question qui se pose naturellement est de savoir dans quelle mesure une fonction de
décision $\hat q = \alg(\S_n)$ est capable d'offrir à un investisseur une utilité espérée
comparable à celle qu'il aurait observée au sein de l'ensemble d'entraînement. Il serait
aussi souhaitable qu'une telle garantie soit indépendante de l'ensemble d'entraînement
$\S_n$. Autrement dit, on cherche à déterminer une borne probabiliste $\Omega$ sur l'erreur de
généralisation de $\hat q = \alg(\S_n)$ valide pour tout $\S_n \sim M^n$:
\begin{equation}
 \hat\zeta(\S_n) \leq \Omega(n,\ldots),
\end{equation}
où
\begin{equation}
  \label{b:zeta}
  % \hat\zeta(\hat q) = n^{-1}\sumi u(r_i\,\hat q(x_i)) - \E u(R \cdot \hat q(X))
  \hat\zeta(\S_n) =  \hEU(\S_n,\alg(\S_n)) - \EU(\alg(\S_n))
\end{equation}
représente l'erreur de généralisation. On peut en fait montrer que $\Omega \to 0$ à mesure que
$n \to \infty$. En fait, on peut démontrer que $\Omega = O(n^{-1/2})$, ce qui permet de quantifier la
``vitesse'' à laquelle la convergence à lieu.


\paragraph{Démonstration}

Considérons deux ensembles d'entraînement: $\S_n \sim M^n$ et $\S'_n$, où $\S'_n$ ne diffère
de $\S_n$ que par un seul point (par exemple le $j$-ème point serait rééchantilloné de la
distribution de marché $M$). De l'algorithme $\alg$ on dérivera alors deux décisions:
$\hat q$ et $\hat q'$. Pour $n$ suffisament grand, on peut alors s'attendre à ce que
l'utilité dérivée de ces deux décisions soit relativement proche, et ce, pour toute
observation. On aurait alors une borne $\beta(n)$ telle que pour tout $(x,r) \sim M$,
\begin{equation}
  |u(r\,\hat q(x)) - u(r\,\hat q'(x))| \leq \beta.
\end{equation}
C'est ce qu'on appelle dans la littérature la \textit{stabilité algorithmique}. La plupart
des algorithmes régularisés classiques disposent par ailleurs d'une telle stabilité. En
particulier, le terme de régularisation $\lambda \nq{q}^2$, combiné à la continuité
Lipschitz de $u$ font en sorte que $\beta = O(n^{-1})$. 

Doté de cette stabilité de $\alg$, on peut alors borner la différence dans l'erreur de
généralisation de $\S_n$ et $\S_n'$:
\begin{align}
  |\hat\zeta(\S_n) - \hat\zeta(\S_n')| &= |\EU(\hat q) - \EU(\hat q') + \hEU(\S_n,\hat q) - \hEU(\S_n',\hat q')|\\
                       &\leq |\EU(\hat q) - \EU(\hat q')| + |\hEU(\S_n,\hat q) -
                         \hEU(\S_n',\hat q')|. \label{b:p1}                         
\end{align}
Or, par le théorème de Jensen appliqué à la fonction valeur absolue, on obtient du premier
terme que
\begin{align}
  |\EU(\hat q) - \EU(\hat q')| &= |\E(u(R \cdot \hat q(X)) - u(R \cdot \hat q'(X)))|\\
                                       & \leq \E(|u(R\cdot \hat q(X)) - u(R \cdot \hat q'(X)|)\\
                                       & \leq \beta,
\end{align}
par définiton de la stabilité. Quant au deuxième terme de \eqref{b:p1} on peut le borner
de la même façon:
\begin{align}
  &|\hEU(\S_n,\hat q) - \hEU(\S_n',\hat q')|\\
  &\qquad = n^{-1}\left|\sumi \mathbb{I}_{i\neq j}u(r_i\,\hat q(x_i)) + u(r_j\,\hat q(x_j)) - \sumi
    \mathbb{I}_{i\neq j}u(r_i\,\hat q'(x_i)) - u(r_j'\,\hat q'(x_j'))\right|\\
  &\qquad \leq n^{-1}\left(| u(r_j\,\hat q(x_j) - u(r_j'\,\hat q'(x_j'))| +\sumi \mathbb{I}_{i\neq
    j}\big|u(r_i\,\hat q(x_i)) - u(r_i\,\hat q'(x_i))\big|\right)\\
  &\qquad \leq n^{-1}\left(| u(r_j\,\hat q(x_j) - u(r_j'\,\hat q'(x_j'))| + (n-1)\beta\right).
\end{align}
Considérons le premier terme. Par le lemme \todo{}, On sait que
$\hat q(x) \leq (2\lambda)^{-1}\rmax\xi^2$ et que $|R|\leq\rmax$. On peut donc borner cette différence
par la différence dans l'utilité dérivée par la meilleure décision d'investissement sur le
meilleur rendement et sur le pire rendement. Par hypothèse Lipschitz et de sous-gradient
de 1 à $r=0$, on sait que pour $r>0, u(r)<r$ et que pour $r<0$, $\gamma r \leq u(r)$. On peut donc
conclure que 
\begin{align}
  |u(r_j\,\hat q(x_j) - u(r_j'\,\hat q'(x_j')| &\leq u((2\lambda)^{-1}\rmax^2\xi^2) -
                                                 u(-(2\lambda)^{-1}\rmax^2\xi^2)\\
  & \leq (2\lambda)^{-1}(\gamma+1)\rmax^2\xi^2.
\end{align}
Ce qui entraîne donc que
\begin{align}
  |\hEU(\S_n,\hat q) - \hEU(\S_n',\hat q')| &\leq \frac{\gamma+1}{2\lambda n}\rmax^2\xi^2 +
                                              \frac{n-1}{n}\beta\\
                                            &\leq \beta + \frac{\gamma+1}{2\lambda n}\rmax^2\xi^2.
\end{align}
Next McDiarmid et autre. Easy stuff.

\paragraph{Équivalent certain}

Puis inverser pour obtenir l'équivalent certain.

\paragraph{Note bibliographique}

La théorie de la stabilité algorithmique remonte en fait aux années 70 avec les travaux de
Luc Devroye appliqués à l'algorithme des $k$ plus proches voisins\cit. Jusqu'alors, les
bornes de généralisation étaient présentées pour toute décision $q \in \Q$ (ie
Vapnik). Bousquet\cit a été le premier a présenter des résultats dans des espaces de
Hilbert à noyau reproduisant. La démonstration est fortement inspirée de l'excellente
référence Mohri\cit. La démonstration de la borne de la décision bornée est un résultat
inédit, dûe à Delage dans le cas linéaire.


\subsection{Bornes de sous optimalité}


\paragraph{Exposition du problème}

Jusqu'ici, les efforts théoriques ont été déployés pour déterminer comment se comportait
la fonction de décision $\hat q = \alg(\S_n)$ dans un univers probabiliste par rapport à
l'univers statistique dans lequel elle avait été construite. Notre attention va maintenant
se tourner vers la performance de $\hat q$ dans l'univers probabiliste par rapport à la
meilleure décision disponible, c'est à dire la solution $q^\star$ de 
\begin{equation}
  \maximizeEquation[q \in \Q]{\EU(R \cdot q(X)).}
\end{equation}

Il convient cependant de réaliser que l'existence d'une borne sur $q^\star$ n'est pas
assurée. En effet, supposons d'une part que l'on dispose d'une utilité neutre au risque,
telle que $u(r) = r$, et d'autre part que $\E R=0$. Soit $\alpha>0$. On pourrait alors définir
la fonction suivante:
\begin{equation}
  q = \alpha\E(R\,\kappa(X,\cdot))
\end{equation}
On aurait alors
\begin{align}
  \EU(q) &= \E(R q(X)) = \E(R\E(R\,\kappa(X,X)))\\
         &= \E(R^2\,\kappa(X,X)) \geq 0,
\end{align}
On peut alors obtenir une utilité espérée non bornée à mesure que $\alpha\to\infty$. Par ailleurs,
ainsi défini, $q$ représente effectivement la covariance entre $R$ et la projection de $X$
dans l'espace dual de $\Q$. Puisque l'utilité est neutre, on sait qu'en espérance
l'application de $q$ à $X$ variera de la même façon que celle de $R$ et donc qu'on aura
une utilité infinie. On verra plus loin au cours d'une démonstration la motivation
derrière cette hypothèse supplémentaire:
\begin{assumption}
  L'utilité croît sous-linéairement, ie. $u(r) = o(r)$. 
\end{assumption}

Une autre hypothèse est maintenant nécessaire pour s'assurer que $q^\star$ soit borné:
l'efficience des marchés. Dans notre cadre théorique, ceci se traduit par l'absence de
l'existence d'une fonction $q \in \Q$ telle que
\begin{equation}
  \pp\{R\cdot q(X) > 0\} = 1.
\end{equation}
D'un point de vue strictement financier, cela fait certainement du sens en vertu de
l'efficience des marchés, version semi-forte\cit. D'un point de vue théorique, ceci exige
en fait qu'il n'y ait pas de région dans $\X$ telle que tous les rendements s'y produisant
soient nécessairement positifs ou négatifs.\todo{Inserer image}.

\begin{assumption}
  \label{hyp:arb}
  Pour toute région $\mathcal{R}\subseteq\X$,
  \begin{equation}
    \pp\{R \geq 0 \mid X \in \mathcal{R}\} < 1,
  \end{equation}
  et de la même façon avec l'évènement $\pp\{R \leq 0\}$. 
\end{assumption}

\paragraph{Borne}

On cherchera donc à établir une borne sur \textit{l'erreur de sous-optimalité} de
$\hat q \sim \alg(M^n)$.




\subsection{Lemmes}

\paragraph{Stabilité}

On montre ici que
\begin{equation}
  \beta \leq \frac{(\gamma\rmax\xi)^2}{2\lambda n}.
\end{equation}

\paragraph{Borne sur la décision algorithmique}

On va ici démontrer que la décision $\hat q(x)$ est bornée, et ce, pour tout $x \in \X$ et
pour toute solution $\hat q$ de
\begin{equation}
  \label{b:l1}
  \maximizeEquation[q \in \Q]{n^{-1}\sumi u(r_i\,q(x_i)) - \lambda\nq{q}^2.}
\end{equation}
Pour ce faire, on va mettre à profit la propriété reproductive de $\Q$ induite par $\kappa$. En
effet, celle-ci stipule que
\begin{equation}
  q(x) = \langle q, \kappa(x,\cdot)\rangle_{\Q} \leq\nq{q}\,\sqrt{\kappa(x,x)},
\end{equation}
où l'inégalité découle de l'inégalité Cauchy-Schwartz appliquée au produit interne de
$\Q$.  On rappelle que, par hypothèse, $\forall x\in\X, \kappa(x,x) \leq \xi^2$; il suffit donc de borner
$\|q\|_{\Q}$. Or, puisque $u(r) \leq r$, on remarque que
\begin{align}
  n^{-1}\sumi u(r_i\,q(x_i)) &\leq n^{-1}\sumi r_i\,q(x_i)\\
                             & \leq n^{-1}\sumi r_i\sqrt{\kappa(x_i,x_i)}\nq{q}\\
                             & \leq \rmax \xi \nq{q}.
\end{align}
Puisque l'expression $\rmax \xi \nq{q} - \lambda\nq{q}^2$ est quadratique et atteint son maximum
lorsque
\begin{equation}
  \nq{q} = \frac{\rmax \xi}{2\lambda},
\end{equation}
on en conclut que $\nq{\hat q} \leq (2\lambda)^{-1}\rmax\xi$ et donc que
\begin{equation}
  \hat q(x) \leq \frac{\rmax\xi^2}{2\lambda}.
\end{equation}
\todo{Montrer qu'on peut effectivement montrer que la borne de l'expression est plus
  grande que celle du problème initial... Pour ce faire, utiliser a) le sous gradient de
  $u$ et b) la dominance de $q'$ sur $q$.}


\paragraph{Forte concavité}

L'objectif est fortement concave, que ce soit sous sa version statistique $\hEU_\lambda$ ou
probabiliste $\EU_\lambda$. Autrement dit, pour tout $\alpha \in [0,1]$, on a
\begin{equation}
  \EU_\lambda(tq_1 + (1-\alpha )q_2) \geq \alpha \EU_\lambda(q_1) + (1-\alpha)\EU_\lambda(q_2) + \lambda \alpha(1-\alpha)\nq{q_1-q_2}^2,
\end{equation}
et de même pour $\hEU_\lambda$. Effectivement, puisque $u$ est concave et $\nq{\cdot}^2$ est
convexe, on a successivement:
\begin{align}
  & \EU_\lambda(\alpha q_1 + (1-\alpha )q_2)\\
  &\qquad= \E u(R\cdot (\alpha q_1+(1-\alpha )q_2)(X)) - \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
  &\qquad= \E u(\alpha (R\cdot q_1(X)) + (1-\alpha )(R\cdot q_2(X)))- \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
  &\qquad\geq \E(\alpha  u(R\cdot q_1(X)) + (1-\alpha )u(R\cdot q_2(X))) - \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
  &\qquad= \alpha\EU(q_1) + (1-\alpha)\EU(q_2) - \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
  &\qquad= \alpha\EU_\lambda(q_1) + (1-\alpha)\EU_\lambda(q_2) - \lambda(\nq{\alpha q_1+(1-\alpha)q_2}^2 - \alpha\nq{q_1}^2 -
    (1-\alpha)\nq{q_2}^2)\\
  &\qquad \geq \alpha\EU_\lambda(q_1) + (1-\alpha)\EU_\lambda(q_2) -\lambda(\alpha\nq{q_1}^2 + (1-\alpha)\nq{q_2}^2 - \alpha\nq{q_1}^2 -
    (1-\alpha)\nq{q_2}^2)\\
  &\qquad = \alpha\EU_\lambda(q_1) + (1-\alpha)\EU_\lambda(q_2).
\end{align}
La preuve est la même lorsqu'on considère $\hEU_\lambda$.


\paragraph{Borne sur la décision optimale}

On veut montrer que $\nq{q^\star}$ est borné. Pour ce faire, on va tout d'abord décomposer
$q = s\theta$, où on pose $\nq{\theta}=1$ et $s>0$; ainsi on peut poser notre problème
d'optimisation comme la recherche d'une `direction' $\theta$ et d'une magnitude $s$ dans
$\Q$. De plus, puisque $\nq{q} = s$, il suffit de montrer que $s^\star$ est borné.

Notons d'abord que l'hypothèse \ref{hyp:arb} entraîne en particulier qu'il existe $\delta > 0$
tel que
\begin{equation}
  \pp\{R\cdot \theta(X) \leq -\delta\} > \varrho \geq 0
\end{equation}
pour tout $\theta \in \Q$ tel que $\nq{\theta} = 1$. Définissons maintenant une variable aléatoire à
deux états: $B = -\delta$ avec probabilité $\varrho$ et $B = \rmax\xi$ avec probabilité $1-\varrho$. Puisque
$R\cdot \theta(X) \leq \rmax\xi$, on a alors que, pour tout $r \in \R$,
\begin{equation}
  \pp\{B\geq r\} \geq \pp\{R\cdot \theta(X)\geq r\}
\end{equation}
\todo{voir figure a produire.}

Puisque $u$ est concave\cit et que $B$ domine stochastiquement $R\cdot \theta(X)$, on a
nécessairement que $\E u(sB) \geq \E u(R\cdot s\theta(X)$, pour tout $s > 0$. Or, par hypothèse de
sous-linéarité on obtient que
\begin{align}
  \lim_{s\to\infty}\E u(R\cdot s\theta(X)) &\leq \lim_{s\to\infty}u(sB)\\
                           & = \lim_{s\to\infty}(\varrho u(-s\delta) + (1-\varrho)u(s\rmax\xi))\\
                           & \leq\lim_{s\to\infty}-\varrho s \delta + (1-\varrho)o(s) = -\infty,
\end{align}
ce qui démontre bien que $s$ est borné.


\paragraph{Borne sur la solution utilitaire sur celle neutre au risque}

Soient $\qu$ la solution de 
\begin{equation}
  \maximizeEquation[q \in \Q]{\hEU_\lambda(q)}
\end{equation}
et $\qn$ la solution de
\begin{equation}
  \maximizeEquation[q \in \Q]{\hEN_\lambda(q),}
\end{equation}
où $\hEN(q):=n^{-1}\sumi r_i\,q(x_i)$. On note tout d'abord avec l'inégalité de Jensen que
$u(\hEN(\qu)) \geq \hEU(\qu) \geq \lambda\nq{\qu}^2 \geq 0$. Mais puisque $u$ a un sur-gradient de 1 à
$0$, on déduit que $u(x) \geq 0$ entraîne $x \geq u(x)$. On a ainsi
$\hEN(\qu) - \lambda\nq{\qu}^2 \geq 0$. Mais comme $\qn$ maximise $\hEN_\lambda$, on obtient
\begin{equation}
  \hEN(\qn) - \lambda\nq{\qn}^2 \geq \hEN(\qu) - \lambda\nq{\qu}^2 \geq 0,
\end{equation}
d'où on tire finalement $\nq{\qu} \leq \nq{\qn}$. 



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main_bounds"
%%% End:
