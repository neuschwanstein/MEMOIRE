\subsection{Lemmes}

\begin{lemme}[Stabilité]
\label{lem:stab}
On montre ici que
\begin{equation}
  \beta \leq \frac{(\gamma\rmax\xi)^2}{2\lambda n}.
\end{equation}
\end{lemme}


\begin{lemme}[Décision neutre au risque comme cas limite]
  \label{lem:rn}
  Soient $\qu$ la solution de
  \begin{equation}
    \maximizeEquation[q \in \Q]{\hEU_\lambda(q)}
  \end{equation}
  et $\qn$ la solution de
  \begin{equation}
    \maximizeEquation[q \in \Q]{\hEN_\lambda(q),}
  \end{equation}
  où $\hEN(q):=n^{-1}\sumi r_i\,q(x_i)$. On note tout d'abord avec l'inégalité de Jensen
  que $u(\hEN(\qu)) \geq \hEU(\qu) \geq \lambda\nq{\qu}^2 \geq 0$. Mais puisque $u$ a un sur-gradient de
  1 à $0$, on déduit que $u(x) \geq 0$ entraîne $x \geq u(x)$. On a ainsi
  $\hEN(\qu) - \lambda\nq{\qu}^2 \geq 0$. Mais comme $\qn$ maximise $\hEN_\lambda$, on obtient
  \begin{equation}
    \hEN(\qn) - \lambda\nq{\qn}^2 \geq \hEN(\qu) - \lambda\nq{\qu}^2 \geq 0,
  \end{equation}
  d'où on tire finalement $\nq{\qu} \leq \nq{\qn}$.
\end{lemme}


\begin{lemme}[Borne sur la décision algorithmique]
  \label{lem:bqhat}
  On va ici démontrer que la décision $\hat q(x)$ est bornée, et ce, pour tout $x \in \X$ et
  pour toute solution $\hat q$ de
  \begin{equation}
    \label{b:l1}
    \maximizeEquation[q \in \Q]{\hEU_\lambda(q).}
  \end{equation}
  Pour ce faire, on va mettre à profit la propriété reproductive de $\Q$ induite par
  $\kappa$ qui stipule que
  \begin{equation}
    q(x) = \langle q, \kappa(x,\cdot)\rangle_{\Q} \leq\nq{q}\,\sqrt{\kappa(x,x)},
  \end{equation}
  où l'inégalité découle de l'inégalité Cauchy-Schwartz appliquée au produit interne de
  $\Q$.  On rappelle que, par hypothèse, $\forall x\in\X, \kappa(x,x) \leq \xi^2$; il suffit donc de borner
  $\nq{q}$. De plus, par le Lemme \ref{lem:rn}, il suffit en fait de borner la solution de
  $\hEN_\lambda(q)$. Mais,
  \begin{align}
    \hEN_\lambda(q) &= n^{-1}\sumi r_i\,q(x_i) - \lambda\nq{q}^2\\
              & \leq n^{-1}\sumi r_i\sqrt{\kappa(x_i,x_i)}\nq{q} - \lambda\nq{q}^2\\
              & \leq \rmax \xi \nq{q} - \lambda\nq{q}^2.
  \end{align}
  Puisque l'expression $\rmax \xi \nq{q} - \lambda\nq{q}^2$ est quadratique, elle atteint son
  maximum à
  \begin{equation}
    \nq{q} = \frac{\rmax \xi}{2\lambda},
  \end{equation}
  on en conclut que $\nq{\hat q} \leq (2\lambda)^{-1}\rmax\xi$ et donc que
  \begin{equation}
    \hat q(x) \leq \frac{\rmax\xi^2}{2\lambda}.
  \end{equation}
\end{lemme}



\begin{lemme}[Forte concavité]
  L'objectif est fortement concave, que ce soit sous sa version statistique $\hEU_\lambda$ ou
  probabiliste $\EU_\lambda$. Autrement dit, pour tout $\alpha \in [0,1]$, on a
  \begin{equation}
    \EU_\lambda(tq_1 + (1-\alpha )q_2) \geq \alpha \EU_\lambda(q_1) + (1-\alpha)\EU_\lambda(q_2) + \lambda \alpha(1-\alpha)\nq{q_1-q_2}^2,
  \end{equation}
  et de même pour $\hEU_\lambda$. Effectivement, puisque $u$ est concave et $\nq{\cdot}^2$ est
  convexe, on a successivement:
  \begin{align}
    & \EU_\lambda(\alpha q_1 + (1-\alpha )q_2)\\
    &\qquad= \E u(R\cdot (\alpha q_1+(1-\alpha )q_2)(X)) - \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
    &\qquad= \E u(\alpha (R\cdot q_1(X)) + (1-\alpha )(R\cdot q_2(X)))- \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
    &\qquad\geq \E(\alpha  u(R\cdot q_1(X)) + (1-\alpha )u(R\cdot q_2(X))) - \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
    &\qquad= \alpha\EU(q_1) + (1-\alpha)\EU(q_2) - \lambda\nq{\alpha q_1 + (1-\alpha)q_2}^2\\
    &\qquad= \alpha\EU_\lambda(q_1) + (1-\alpha)\EU_\lambda(q_2) - \lambda(\nq{\alpha q_1+(1-\alpha)q_2}^2 - \alpha\nq{q_1}^2 -
      (1-\alpha)\nq{q_2}^2)\\
    &\qquad \geq \alpha\EU_\lambda(q_1) + (1-\alpha)\EU_\lambda(q_2) -\lambda(\alpha\nq{q_1}^2 + (1-\alpha)\nq{q_2}^2 - \alpha\nq{q_1}^2 -
      (1-\alpha)\nq{q_2}^2)\\
    &\qquad = \alpha\EU_\lambda(q_1) + (1-\alpha)\EU_\lambda(q_2).
  \end{align}
  La preuve est la même lorsqu'on considère $\hEU_\lambda$.
\end{lemme}



\begin{lemme}[Borne sur la décision optimale]
  On veut montrer que $\nq{q^\star}$ est borné. Pour ce faire, on va tout d'abord décomposer
  $q = s\theta$, où on pose $\nq{\theta}=1$ et $s>0$; ainsi on peut poser notre problème
  d'optimisation comme la recherche d'une `direction' $\theta$ et d'une magnitude $s$ dans
  $\Q$. De plus, puisque $\nq{q} = s$, il suffit de montrer que $s^\star$ est borné.

  Notons d'abord que l'hypothèse \ref{hyp:arb} entraîne en particulier qu'il existe
  $\delta > 0$ et $\varrho \geq 0$ tels que
  \begin{equation}
    \pp\{R\cdot \theta(X) \leq -\delta\} > \varrho
  \end{equation}
  pour tout $\theta \in \Q$ tel que $\nq{\theta} = 1$. Définissons maintenant une variable aléatoire à
  deux états: $B = -\delta$ avec probabilité $\varrho$ et $B = \rmax\xi$ avec probabilité
  $1-\varrho$. Puisque $R\cdot \theta(X) \leq \rmax\xi$, on a alors que, pour tout $r \in \R$,
  \begin{equation}
    \pp\{B\geq r\} \geq \pp\{R\cdot \theta(X)\geq r\}
  \end{equation}
  \todo{voir figure a produire.}

  Puisque par hypothèse $u$ est concave et puisque que $B$ domine stochastiquement
  $R\cdot \theta(X)$, on a nécessairement que $\E u(sB) \geq \E u(R\cdot s\theta(X))$, pour tout
  $s > 0$. Or, par hypothèse de sous-linéarité on obtient que
  \begin{align}
    \lim_{s\to\infty}\E u(R\cdot s\theta(X)) &\leq \lim_{s\to\infty}u(sB)\\
                             & = \lim_{s\to\infty}(\varrho u(-s\delta) + (1-\varrho)u(s\rmax\xi))\\
                             & \leq\lim_{s\to\infty}-\varrho s \delta + (1-\varrho)o(s) = -\infty,
  \end{align}
  ce qui démontre bien que $s$ est borné.
\end{lemme}



\begin{lemme}[Borne sur l'équivalent certain]
  
\end{lemme}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main_bounds"
%%% End:
