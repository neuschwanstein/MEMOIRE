* *Le 8 juin*

_Quelques définitions (dans le cas /newsvendor and portfolio/)_

Le vrai risque:

			R_{true}(q) = E_{D(x)}[C(q; D(x))]
				= E_{x}[E_{D(x)}[C(q; D(x))|x]]

Le risque empirique:

			  ^R(q;S_{n}) = 1/n \sum_{i}C(q, S_n^i)
				   = 1/n \sum_{i}C(q, d_i(x_i))

"Since we desire the true risk to be low, a combination of low empirical risk and
sufficient stability ensures this."


Let X:R^p be the information distribution, providing objects of the R^p form, and let R(x):R
be the (real) distribution returns, and let D = R\times{}X the /information space/. 

_Learning Algorithm._
A /learning alogrithm/ is a function whose argument is a dataset drawn from D^n, ie. with n
rows, mapping into the space of functions mapping from X to a portfolio space (to be
defined later) P. We shall denote this space of functions as X^P.

_Loss of a decision vector q._
The /loss/ l of a decision rule q \in Q \subset{} X^P with respect to a sample d=(x,r) \in{} D is defined as

				  l(q,d) = c(q(x),r(x))

for some cost function. 


Now how should this loss function be defined? On one hand we have the actual return of the
chosen stock. The real question here is when would the loss be zero. We always gain more
utility from higher return, and so if the chosen q gives us huge gains, then we cannot
claim that there's a single return at which the loss is completely absent. Our loss
function would therefore take the form of a decreasing exponential function exp(-\mu{}x).

But if we had such a loss function, we would then be out of line with a two-pieces linear
utility function. A log utility is of no use too, since its domain is only on R_{++}_{}. 

[[./Mai2015/FigExpUtility2.png]]

But -exp(-\mu{}x) could be a good utility function, that can besides be scaled and moved
around using a parameter \mu as an increasing rate. However we lose the insight developed
previously with the Newspaper article. 

The loss exp(-\mu{}x) is then perfectly in line with the utility and provide a reasonable
insight since the loss occured for higher return rates is much smaller than the loss
incured when dealing with higher return rates. 

We're also inline with the loss function of the newsvendor, since the loss we're trying to
minimize is now the same as the utility we're trying to maximize (modulo a minus sign).

In the newsvendor case, the loss the hypothesis q incurs on a random variate d=(x,r) drawn
from D, was defined as

				    \ell(q,d) = c(q(x),d)

where c was the newsvendors cost incurred from a demand d and an ordered quantity q(x). 

In our case, we again have not a situation where the loss is null, essentially because
we're in fact /not/ trying to predict the actual return rate, we instead try to form a good
portfolio from which we want to derive utility. The other problem, as mentioned
previously, is also that our utility has no supremum, ie. is not bounded above. In
pratical terms, this means that there's no single best decision that could have been made
had we known the return rate previously: if the rate was to be positive, then would have
massively borrowed to leverage the positive rate. On the other side, if the rate was
negative, then the reverse situation where we could have chosen between a mass short sale
or risk-free lending would have appropriate.

And in this sense, would there be no way to add a regularization term, ie. with which we
would discourage massive investments?

Here's a possible method to define \ell, but we first have to assume that ||q(x)||_2^2 \leq 1,
ie. the investment in the risky asset (under the two-assets portfolio hypothesis) is
bounded by 100%, in financial terms, we cannot borrow money at R_f to invest in the risky
asset. 

If r, ie. the return of the risky asset is above R_f, then we'll have wished we had
invested all our wealth in it, and thus the loss will be 

				  U(1,0) - U(q^{T}x, 1-q^{T}x)

if on the other hand the stock yields a return below R_f, then the reference will be the
utility derived from a perfectly safe portfolio, so that the loss becomes

				  U(0,1) - U(q^{T}x, 1-q^{T}x)

In either case, the loss is always positive (or at most zero).

*Note.* In the above expressions, U(x,y) represents the utility of a portfolio for which a
 proportion x is invested in the risky asset and y in the risk free asset.

According to Bousquet, an algorithm has \beta uniform stability with respect to the loss
function l if for all dataset of m rows, we have the bound

		      `||\ell(q(S_m,d) - \ell(q(S'_m,d))||_{\infty} \leq \beta_m

where q' is the hypothesis formed when removing a single row from S_m, the dataset (see
Bousquet p. 504 for total formalism). 

*Def.* An algorithm is uniformly stable if \beta_{m }\leq O(1/n).

*Def.* The /generalization error/ or the /risk/ depends on the training set S and the algorithm
 A_S in the following way:

				  R(A_S,S) = E_{d}[\ell(A_s,d)]

*Def.* The /empirical error/ is the average of the loss over the training set S of m
elements:

			       R_{emp}(A_S,S) = 1/m \sum_i \ell(A_s, d)



* *Le 9 juin*

We still wish to apply theoretical bounds upon the `loss' we can incur using our
algorithm. We use the notion of uniform stability, as developed by Bousquet and Elisseef
in their seminal work. 

<<uniformStabilityDefinition>>
*Def (Uniform stability).* An algorithm A has uniform stability \beta with loss \ell if, for any
 dataset S of m rows, ie. \forall S \in D^m, and any removed row i, then for d=(x,r) \sim{} D,

		    `   ||\ell(A_S(x), r) - \ell(A_{S\i}(x), r)||_{\infty} \leq \beta

ie.,

	            B_m = sup_{d \in D} |\ell(A_S(x), r) - \ell(A_{S\i}(x), r)| \leq \beta

For notation purposes, the left value, depending solely on the size m of S, will be
henceforth noted B_m or simply B.

Now for the hard part of giving a value to \beta. The problem with the current value of \ell is
how it has not a constant mapping, and will change its underlying application wether r is
above or below r_c (which could be R_f). 

Below are the two forms the loss function, depending on the portfolio composition: p=0 is
a perfectly safe portfolio, p=1 is a perfectly risky portfolio. They were plotted using
the exponential utility. 

[[./Mai2015/expULossAboveZero.png]]

[[./Mai2015/expULossBelowZero.png]]

We'll most probably need to bound the return drawn from the returns distribution R in the
interval [-\rho, \rho]. We can then add probalistic bounds upon it as described [[https://en.wikipedia.org/wiki/Stability_(learning_theory)#Uniform_Stability][in this
article]]. We also note that the loss function \ell thus defined is continuous with respect to
r. 

Let's make our ideas clearer. We have the following identities:
<<definitionCost>>

			c(p,r) = U(r) - U(pr + (1-p)R_f) if r > R_f
			c(p,r) = U(R_f) - U(pr + (1-p)R_f) if r \leq R_f
				    p = q_S^{T}x = q(S)^{T}x

Actually, this `multi-defined' function is in fact not that remote from the stability
criterion exhibited in the newsvendor case, where we have operators min and max operators
(\vee and \wedge{}) on the slopes of the cost function.

Let us first consider a fixed dataset S from which a decision vector q_S is algorithmically
defined, and let us apply the definition of the uniform stability. Then, no matter the
value of r, we're left with the following expression for B:

		  B = sup_{d \in D} |U(pr + (1-p)r_c) - U(p'r + (1-p')r_c)|

where 

				   p' = q(S^{\i})^{T}x

is the `less-informed' decision.

Now we can again apply the Lipschitz property of continuous functions stating that,
provided that dom f = S convex, then

`			     |f(x) - f(y)| \leq \alpha |x-y|

where

			       \alpha = sup_{x \in S} |f'(x)|

If the utility is linear, then \alpha = \beta, the slope of the utility left of r_c. If however we
consider an exponential utility, then the maximum value of the derivative will only be
reached at the edge of the considered interval, and so \alpha = \mu{} exp(\rho), which can be quite
high (more on that later...)

Either way, B is now bounded:

			   B \leq sup_{d \in D} \alpha |(p-p')(r-r_c)|
			     = \alpha sup |r-r_{c}| sup_{d \in D} |p - p'|

In the above expression, sup |r-r_{c}| will be \rho + r_c, since we asume that r_c > 0. We shall
call this quantity \gamma so that

				   B \leq \alpha{} \gamma sup |p - p'|

C'est là qu'on est rendu...


* Le 10 juin

We've been trying for a few days to establish a bound on the uniform stability of the
linear algorithm when dealing with a concave utility, and to do so we must dive into how
the algorithm operates.

We have the following equalities:

			      q_S = argmax_{q} Û(q,S) - \lambda||q||_2^2
			  q' = q_{S\i} = argmax_{q} Û(q,S^{\i}) - \lambda||q||_2^2

By the triangle equality we also have

'		      |p - p'| = |(q - q')^{T}x| \leq \sum_i |x_{i}| |q_i - q_{i}'|

					  * * *

Let's instead follow Theorem 22 from Bousquet. We first want to show that \ell as defined
previously is \sigma-admissible.
<<lossDefinition>>
				  \ell(q,(x,r)) = c(q^{T}x,r)

(Bousquet, Definition 19, p. 512)
<<sigmaAdmissibilityDefinition>>
*Def. (\sigma-admissibility).* A loss function \ell is \sigma-admissible if the associated cost function
 c is convex with respect to its first argument and the following condition holds for any
 p_1, p_2 and r:

`			   |c(p_1,r) - c(p_2,r)| \leq \sigma |p_1 - p_{2}|

We've shown yesterday that the left term reduces to

`				  |(p_1 - p_2)(r - r_c)|

The largest value |r - r_{c}| can reach is \rho+r_c, and so \sigma = \rho+r_c (previously declared as \gamma).


* Le 15 juin

First, previously undefined formally:

<<linearUtilityDefinition>>
_Def. (Linear Utility)_ The linear utility is defined as

				  U(r) = r + min(0, \beta{}r)

where 0 < \beta < 1. We simplify r_c = 0.

<<expUtilityDefinition>>
_Def. (Exponential Utility)_ The exponential utility with parameter \mu is defined as

				    U(r) = -exp(-\mu r)

where \mu > 0. There's no critical return here. 

We also note in the above expression that [[definitionCost][c]] is indeed convex under its first argument, as
long as 0 \leq p \leq 1.

<<sigmaAdmissibilityTheorem>>
_Thm._ The [[lossDefinition][loss function]] is [[sigmaAdmissibilityDefinition][\sigma-admissible]] with \sigma = \rho+R_f in the linear case and
\sigma = (\rho+R_{f}) exp(\mu \rho) in the exponential case.

_Proof._ The expression

				   `|c(p_1,r) - c(p_2,r)|

reduces to

			 `|U(p_{1}r + (1-p_{1})R_f) - U(p_{2}r + (1-p_2)R_f_{}|.

Now because U is Lipschitz continuous, then the above expression is bounded by

		 \sigma |p_{1}r + (1-p_{1})R_f - (p_{2}r + (1-p_2)R_{f}| = \alpha |p_1 - p_{2}||r-R_{f}|

where

			      \alpha = max_{x \in [-\rho,\rho]} |U'(x)|.

Now we've considered two utility forms. In the [[linearUtilityDefinition][linear case]] the derivative is constant (set
to 1) because the derived utility at the left of r_c has always slope 1, and so \alpha=1. If
[[exoUtilityDefinition][utility is exponential]], then \alpha = exp(\mu \rho). 

Now the \sigma bound must hold for all r. The expression |r-R_{f}| will reach its largest value at
r=-\rho since R_{f} is asumed to be non-negative. \Diamond

Just like in the newsvendor case, we'll assume that feature vectors must lie in a ball of
radius X^2_{max}.

<<BousquetTheorem7>>
Let F be a reproducing kernel Hilbert space with kernel k such that \forall x \in X, k(x,x) \leq \kappa^2
< \infty. Let \ell be [[sigmaAdmissibilityDefinition][\sigma-admissible]] with respect to F. The learning algorithm A defined by

		A_{S} = argmin_{g \in F} 1/n \sum_i^n \ell(g,d_i) + \lambda ||g||^2_k

has [[uniformStabilityDefinition][uniform stability]] \alpha_n wrt \ell with

			  \alpha_n \leq \sigma^{2 }\kappa^2 / 2\lambda{}n.

<<algorithm>>
The decision algorithm of our model produced by a dataset S_n = {(x,r)_i} is defined to be 

	     q^\star = argmin_{q \in R^p} 1/n \sum_i^n c(q^{T}x_i,r_i) + \lambda||q||^2_2

The investment decision following information vector x will therefore be p = q^\star^{T}x,
where p is the proportion to be invested in the risky asset.

<<stabilityTheorem>>
Using The [[uniformStabilityDefinition][stability]] \alpha_n of our proposed [[algorithm]] is bounded by the following:

			       \alpha_n \leq (\rho + R_f)^2 X^2_max / 2\lambda{}n

in the case of a linear utility and by 

			      \alpha_n \leq exp(2\mu \rho) X^2_max / 2\lambda{}n.

These results follow from the [[sigmaAdmissibilityTheorem][\sigma-admissibility theorem]] of our [[lossDefinition][loss function]] and
[[BousquetTheorem7][Bousquet's Theorem]].

<<trueRiskDefinition>>
The true risk with respect to algorithm A and learning set S_n is defined as

			      R_{true}(A,S_n) = E_{d}[\ell(A_S, d)]

that is, in plain words, the expected loss we'll have when applying our algorithm in the
wild, ie. out of sample.

<<empiricalRiskDefinition>>
The empirical risk with respect to algorithm A and learning set S_n is defined to be 

			     ^R(A,S_n) = 1/n \sum_i^n \ell(A_S, z_i)

that is, in plain words, the average risk our model has produced over all training
points. 

<<maxCostProposition>>
Using the aforementioned algorithm, the maximum loss we can incur is when p=1 with
r=-\rho. In such a case, 

				 c(1,-\rho) = U(R_f) - U(\rho).

We shall call this quantity \gamma.

<<RudinLemma2>>
Let A be an algorithm with uniform stability \alpha_n wrt a loss function \ell such that 0 \leq
\ell(A_S,d) \leq M, for all d = (x,r) \sim D and all sets S_n of size n. Then for any n\geq1 and any \delta \in
(0,1), the following bound holds with probability at least 1-\delta over the random draw of the
sample S_n:

	       `|R_{true}(A,S_n) - ^R(A,S_n)| \leq 2\alpha_n + (4n\alpha_n + M) \radic(log(2/\delta)/2n)

<<generalizationBoundTheorem>>
Our [[algorithm]] has a generalization bound of 

	       `|R_{true}(A,S_n) - ^R(A,S_n)| \leq 2\alpha_n + (4n\alpha_n + \gamma) \radic(log(2/\delta)/2n)

with probability 1-\delta. It follows from [[RudinLemma2][Rudin et al.'s Lemma 2]] and our [[stabilityTheorem][stability Theorem]]. \diamond


* Le 17 juin

On a commencé à faire des tests numériques. Voici ce qu'il faudrait faire:

 - Etablir une routine de cross-validation afin d'obtenir un \lambda de régularisation
 - Tracer des points de corrélation (R^2 ?) selon la valeur de \lambda
 - Tracer des courbes de stabilité sur plusieurs tests.
 - Tracer des courbes de vrai risque par rapport au risque empirique afin de déterminer
   comment se comporte le modèle.
 - Tracer vis-à-vis ces courbes les bornes théoriques.

Ensuite il faudra commencer à réfléchir sérieusement à la facon dont on peut créer un
portefeuille à plusieurs titres. Il s'agit probablement d'optimiser sur un espace de
matrice, et non pas simplement sur un vecteur q. Par contre la théorie devra être revue en
profondeur afin d'obtenir de nouvelles bornes.

Now how should X^2_max and \rho should be defined in our numerical tests? First off, an
information vector, at least how it as been defined now, is an uncorrelated multivariate
random variable with mean 0. Now we know for a real random variable that with 95%
confidence level, that x will lie within [-1.96, 19.96]. We're interested with X^2_max,
which is ||x||^2_2. First let's suppose p=2, with both coordinates at 1.96. Then we would
have X^2_max = (1.96)^p = (1.96)^2. However does the confidence shrink?

Let's think out loud. I have a first random variable whose value I know with a 95%
confidence interval lies in [-1.96, 1.96]. But each component is unrelated, by
hypothesis. And the probability of two unrelated events is the product of the two
probabilities. 

We can therefore derive a general identity. If \Sigma is diagonal, then with confidence (.95)^p,
the norm of the vector is less than (1.96)^p... TBC


* Le 18 juin

Donc, comment mesurer numériquement la [[uniformStabilityDefinition][stabilité algorithmique]]? Le probleme le plus
évident est qu'il s'agit de maximiser sur D au grand complet. 


* Le 19 juin

Aujourd'hui on travaillera sur les intervalles de confiance, notamment essayer de mieux
quantifier X^2_max.

Rappelons d'abord que X^2_max représente la région dans laquelle, avec probabilité 1-\delta, se
trouvent tous vecteur d'information x. 

Une chose à la fois. Il était question aussi d'établir une routine de cross-validation
afin d'obtenir un \lambda^\star optimal de régularisation. Voici les étapes qu'elle devrait
réaliser:

  1. On fixe un vecteur de transformation t \in R^p. On fixe aussi un ensemble d'entraînement
     S_n et un ensemble de test S_m à partir de t.
  2. On fixe \lambda = 0.
  3. On résout q^\star à partir de S_n. On conserve le coût total de l'ensemble dans un
     vecteur de résultat c_in.
  4. Avec q^\star, on détermine le cout de l'ensemble S_m qu'on stock dans un vecteur de
     résultat c_out.
  5. On recommence (3) avec \lambda = \lambda + \delta\lambda.
  6. END

					  \diamond \diamond \diamond

_Exploration d'un portefeuille constitué de plusieurs titres._

On considère m-1 titres risqués, et un titre sans risque de rendement R_f. Chacun de ces
titres et composé d'un vecteur d'information x_st, ie. de m ensembles d'informations S_n sur
n jours. 

On souhaite encore former une décision linéaire par rapport à l'information disponible. On
avait donc une application q (ou un algorithme) qui à partir d'un vecreur d'information x
permettait de rendre un portefeuille de la forme [p, 1-p]. Autrement dit, on obtenait un
scalaire à partir duquel l'ensemble du portefeuille était bien défini. Maintenant ce dont
on a besoin, c'est d'un portefeuille vecteur. 

Réfléchissons. A tous les jours on n'a non plus un seul vecteur d'information x, mais bien
une matrice d'information X dont chaque rangée représente un titre et chaque colonne
représente une information quelconque, ie. pour chaque titre s dont on a un vecteur
colonne d'information x_s, on a maintenant une matrice définie par

			       X = [x_1 \cdot\cdot\cdot x_{s}]

On doit donc définir un vecteur de décision q \in R^s tel que Xs rend un vecteur de
portefeuille a s éléments. Le rendement de ce portefeuille sera alors donné par r^{T}Xs, où r
est le vecteur de rendement à cette journée donnée. 

A present, comment déterminer la quantité à investir dans le titre sans risque?
Puisqu'auparavant on allouait la quantité restante (ie. 1 - q^{T}x) au titre sans risque, il
s'agit finalement de faire la même chose ici, ie. 1^{T}Xs donne la composition totale dans
les titres risqués, alors 1 - 1^{T}Xq donnera l'allocation sans risque, de sorte que le
rendement total deviendra:

				   r^{T}Xq + (1 - 1^{T}Xq)R_f

A partir de ce nouveau portefeuille, on peut sans doute utiliser les mêmes outils
développés auparavant, ie. pour le portefeuille à un seul titre. Voyons voir...

_Coût et _Perte._

On a auparavant défini notre fonction de perte (loss) \ell comme étant une fonction prenant
comme argument un algorithme déterminé à partir d'un dataset S_n et un vecteur
d'information complète d=(x,r) \in R^{p+1} pour en retourner le coût total. Le coût était quant
à lui déterminé à partir de l'utilité concave U. 

Le coût a été défini de facon très naïve, puisque qu'on a déterminé que si le rendement du
titre risqué était supérieur au rendement sans risque, alors on s'attendait (ou du moins
on voulait privilégier) un portefeuille constitué uniquement du titre risqué, ie. q^{T}x
\geq 1. Si au contraire le rendement était inférieur au titre sans risque, alors on
s'attendait à ce que le portefeuille soit complètement sans risque, ie. q^{T}x \leq 0. 

Est-ce qu'on pourrait dès lors avoir un coût total qui serait une somme des coûts
individuels pour chaque titre? En fait ce n'est pas nécessaire car on obtient déjà un
rendement du portefeuille scalaire auquel on peut simplement appliquer la même logique que
précédemment. 

Ce sera donc c(p,r), où p et r sont les valeurs vectorielles d'allocation et de rendement,
ie. p = Xq. Mais ce n'est pas si clair... Car supposons que tous les titres donnent un
rendement inférieur à R_f, sauf un. Alors on veut clairement que le portefeuille soit
constitué uniquement de ce titre, et de rien dans le reste. le coût serait donc 

			    \lfloor{}U(max r) - U(r^{T}p + (1 - 1^{T}p)R_f)\rfloor

si max r > R_f.

Au contraire, si max r < R_f, alors le coût deviendra 

			   \lfloor{}U(R_f) - U(r^{T}p + (1 - 1^{T}p)R_f)\rfloor.

Il faut donc à présent revérifier les preuves afin de s'assurer que tout fonctionne. 

On peut toutefois énoncer un nouvel algorithme. On chercherait vraisemblablement à une
fois de plus minimiser le cout total sur un dataset S_{n\times{}s}.

	          q^\star = argmin_{q \in R^p} 1/n \sum_i^n c(Xq, r) + \lambda||q||_2.


* Le 25 juin

Pondering, once again. Our loss function \ell defined above was designed in a complicated
fashion, but this was to allow a single reference point where the loss was perfectly zero,
ie. when investment decision were perfect.


* Le 27 juin

Two loss functions were previously defined:

				c(p,r) = -U(rp + (1-p)R_f)

and

			c(p,r) = U(r) - U(pr + (1-p)R_f) if r > R_f
			c(p,r) = U(R_f) - U(pr + (1-p)R_f) if r \leq R_f.

There are a few differences between these two, for example, given a dataset where
information points x_i are positively correlated with returns, then the minimization of the
expression /without a regularization term/ will yield an unconstrained optimization
problem. In contrast, the second expression is bounded below by zero, and so must be
constrained. Bousquet also defines a cost function as having the signature

				      c: Y \times Y \to R_{+}

ie, mapping onto the non-negative real numbers. However, the proofs don't seem to use this
property, at least not for theorems that interest us. 

Another key difference is in the learning itself. With the first cost function, we simply
try to maximize the utility derived from a decision p, whereas with the second cost, we're
in fact minimizing the utility we didn't achieve, but that we could have achieved. And if
we achieve a higher utility, then there's no additional benefits to it. 


* Le 28 juin

Xq: returns the allocation for a m-portfolio.
diag(XQ): returns the allocation for a m-portfolio

Returns for a m-portfolio

			     r^{T}diag(XQ) + (1 - 1^{T}diag(XQ))R_f

which can also be expressed 

			      r^{T}diag(P) + (1 - 1^{T}diag(P))R_f.

But now the cost c(P,r) /must/ be a jointly convex function over P and r. This is how c(P,r)
could be expressed (easiest form):

			c(P,r) = -U(r^{T}diag(P) + (1 - 1^{T}diag(P))R_f)

First of all, are not we losing some cross-information about how an asset i will react to
information about stock j? This reminds me of a covariance matrix of some sort.

At any rate, the question is now to know if, for any P \in R^{m\times{}m} and any r \in R^m, the
expression r^{T}diag(P) is a convex one. However something tells me it might not be the
case... Matlab at the rescue!


* Le 8 juillet

There is still a missing theorem from our analysis, where we we look to quantify the
difference between the in-sample cost using our in-sample decision vector q with the
in-sample cost using the `optimal' decision vector q^* (the true optimal solution).

We recall that R_{true}(q) is the expected value of the cost a decision vector q will yield
over the whole distribution D, whereas ^R(q) is the average cost produced by a decision
vector q over the test set. 

There are therefore two measures we wish to quantify. First is |R_{true} - ^R| using ^q. This
measure indicates how well (or how poorly) our decision q would perform out-sample,
compared to its performance in-sample.

Another measure is |R_{true}(q^*) - ^R(q)| which indicates how far our measured average cost
using trained decision vector q will be from the absolute best, ie. the expected cost
using the optimal decision vector had we access to the true distribution. 

According to Rudin and Gah Vi Yan, this last measure, ie. our performance compared against
the best available performance, is at least (or strictly) superior to the first measure,
where we use our own trained decision vector, and not the best available vector. Two terms
have to be added up tocompensate : first one will be the in-sample bias due to
regularization, and the other one the bias due to the finite sample we have access to.

					  \star \star \star

Is it true we really draw iid ?

					  \star \star \star

Again, let us analyze how Rudin and co. did this trick in quantifying an unknown
distribution with an unknown decision vector, which we know is the best available given
this unknown distribution D. 

First is the following lemma, stating that

    The newsvendor objective funciton equaivalent to the /nonparametric regression loss
    function/ H_r(\cdot), save a constant multiplier.

Which, formally means that 

		 C(q,D) = (1/2) H_r(q-D) := (1/2) [|q-D| + (2r - 1)(q-D)]

where r = b/(b+h).

Now first, how was the cost C(q,D) defined? We recall that D is the `uncertain (random)
future demand' and therefore that

			     C(q,D) = b(D - q)^{+} + h(q - D)^{ +}

that is therefore the cost of having a decision q when faced with a certain demand D. In
our case, this would translate in the cost of having of an investment allocation p when
faced with a return r. We had previously defined it as 

				c(p,r) = -U(pr + (1-p)R_f).

A key difference here is that our cost is wrapped with a utility U whereas /their/ cost is
not wrapped at all and is simply linear. Well, it surely depends on our own utility, if it
is linear then there's probably no problem, but if it's exponential, then we might run
into some problems. 


* Le 9 juillet 2015

_Theorem 3._

Denote the true optimal solution q^* = q^*(x_{n+1}), and assume it is continuously
differentiable up to order k on some fixed open neighborhood of zero in R^p and its k^{th}
derivative is uniformly Hölder continious at zero with exponent \gamma (Condition 1). Then with
probability at least 1-\delta over the random draw of the sample S_n, where each element of S_n
is drawn iid from an unknown distribution on X \times D:

	   `|R_true(q^*) - ^R_in(q^{}^,S_n)| \leq (1) + (b \vee h) M \radic(log n) / n^{s/(2s + p)}

where s = k+\gamma is the order of smoothness of q^* and ^q is the solution to (NV-ML1) and M is
a constant that depends on the demand distribution. 


* Le 13 juillet 2015

When we say the true optimal solution as being

				      q^* = q^*(x_{n+1})

we must be careful in how we understand this function. Because no assumption is being made
on the functional subset of which q^* belongs to. In other words, whereas in our training
we considered only affine functions of the input x, here we make absolutely no assumption
on the form that x^* could really have. Therefore, it indeed makes sense to talk about
continuous derivative on a region, because, in general, x^* is not a linear function of x. 

However, there is still a notational problem, as q^* seems to be defined in term of x_{n+1},
which makes in fact little sense... q^*(x_{n+1}) \in R is in fact a real quantity (in the newsvendor
case: we want to order /real/ quantities).

Therefore we can only infer that in the above equation (distance between in-sample risk
and true optimal risk) q^* really represents a function, and that x_{n+1} has no place
here. This is only a first draft, so I guess it's ok. 

Now onto the proof, not the most easiest one I've seen...

_Condition 1._ It is first said implicitly that q^*(x) corresponds to the /r-th conditional 
quantile function./ Then, we ask that this r-th conditional quantile function q^*(x) is
continuously differentiable up to order k on V an open neighborhood of R^p, and that their
k-th derivates are uniformly Hölder continuous at 0 with exponent \gamma, ie.

			   `|D^{u}q^*(x) - D^{u}q^{*}(0)| \leq c||x||^\gamma

for all x \in V and [u] \leq k.

Then 

					s = k + \gamma

is refered as the /order of smoothness/ of the function q^*(x).

_Proof._ The rest of the proof first applies triangle inequality to the expression, so
that |R_{true}(q^*) - R_true(q^)| is isolated. And then the mystifying step, where

		     `|R_{true}(q^*) - R_true(q^)| \leq (b \vee h) E |q^* - q^{}^ |

Also, let us be clear about each term. E|q^* - q^ | means here that the expected
(numerical) value for each algorithm, the first being the optimal one, and the second
being the one generated from the sample. 

To apply their theorem, Rudin et al. had to relate their cost function to the non
parametric loss function H_r. 

What Rudin et al. actually do is taking results from quantile regression, since a strong
theoretical results have already been proved in the case where a loss corresponds to their
own (newsvendor) loss. Therefore, our task would be to take theses results of a loss of
form 

			    H_\alpha(t) = |t| + (2\alpha - 1)t

and translate them to a form similar to our own linear utility shape, ie. 

			     H_\beta(t) = t + min(0,\beta{}t)

A result from Koenker says that 

				    E[H_\alpha(Z - t)]

is minimized by taking t = \alpha^{th} quantile of Z. So if we go back to the newsvendor context,
where the cost that is to be minimized over the sample set is C(q;D) = H_\alpha(q - D),
with \alpha = b/(b+h). /If/ we knew the distribution of D, then we would know that to minimize

					E[C(q;D)]

that is, over the random variable D, we would have to take the (b/(b+h))^{th} quartile. So
two taks appear here:

- First, we need to determine what would the optimal p^{*} in our case would be, if we knew
  the form of the distribution of r. This p^{\star} value needs to be a function of D, whatever D
  is (D here being the distribution).
- Next we also need a theorem like the one that was devised by Chaudhuri. 


_Section 1.3 of Koenker._

We characterize a real-valued random variable X using its CDF (cumulative distribution
function) F:

				     F(x) = P(X \leq x)

and the *t^{th} quantile* by:

			        F^{-1}(\tau) = inf{x : F(x) \geq \tau}.

_Remark._ The median is defined by F^{-1}(1/2). Dropping the inf notation, and asuming that F
is monotonic, we get 

					F(x) = 1/2. \diamond

We wish now wish to describe a /loss function/ with parameter \tau by:

				  \rho_{\tau}(u) = u(\tau - I(u<0)),

where 0 < \tau < 1. The goal is now to find x^ minimizing the expected loss. We note that 

		         \rho_{\tau}(u) = (\tau-1) min(0, t) + \tau max(0,t),

or, alternatively, by

			      \rho_{\tau}(u) = |u| + (2\tau - 1)u.

Still taking cue from section 1.3, we seek to minimize

          E\rho_{\tau}(X - x^) = (\tau - 1) \int_{-\infty}^{x^} (x - x^) dF(x) + \tau \int_{x^}^{\infty} (x - x^) dF(x).

We then differentiate to obtain 

0 = (1-\tau) \int_{-\infty}^{x^} ....

Difficulties were encountered. 18hours later however, after long and arduous internet
spelunking, we find this new wonderful tool, the /Leibniz Integral Rule/!


* Le 14 juillet 

We recall we were trying to find an analytical minimizer to the linear loss for a random
variable X with CDF F. If loss is characterized by \rho_\tau(u) as above, then we want to find
x^ that will minimize the expected value of the loss using X-x^:

				minimize E[\rho_\tau(X - x^)].

To do so we express the expectation using the fact that the derivative dF(x) of the CDF F
is the PDF of F and therefore

        E[\rho_\tau(X - x^)] = (\tau - 1) \int_{-\infty}^{x^} (x - x^) dF(x) + \tau \int_{x^}^{\infty} (x - x^) dF(x).

Differentiating with respect to x^ and applying Leibniz Integral Rule yields:

	          (1 - \tau) \int_{-\infty}^{x^} dF(x) - \tau \int_{x^}^{\infty} (x - x^) dF(x),

which is equivalent to 

		            \int_{-\infty}^{x^} dF(x) - \tau \int_{-\infty}^{\infty} dF(x).

Because F(-\infty) = 0 and F(\infty) = 1, the first integral evaluates to F(x^) and the second
evaluates to 1. Now the derivative must be zero, and therefore we find the rule:

					F(x^) = \tau,

ie. to minimize the loss characterized by parameter \tau, we must take a regressor x^ equal
to the \tau^{th} quantile. 

Now! Onto real business. What would be our `theoretical' parameter p that would minimize
our loss. Back to a paper pad... We should perhaps start with the canonical cost 

				c(p,r) = -U(pr + (1-p)R_f).

Here, r is the random variable, and p is the chosen quantity. Let us assume r is
characterized by a CDF F(r). Then we wish to minimize the expected loss with respect to r:

				   minimize E_{r}[c(p,r)].



* Le 15 juillet

Back to quantile regression. For some reason, this theory seems interesting and I'd like
to understand more deeply Choquet's utility which seems of the highest interest, since, I
dearly hope, I could relate theorems provided by Chaundhuri (or whatever his name is) to a
big-data scenario, ie. characterize the true bound on in-smaple estimatios to the optimal
decision function used in the true risk function. 

Besides, I think a lot of statistics intuition, of which I deeply lack, might be pallied
up from this book. 

					  \star \star \star

Instead of using the CDF or a random variable X, X is characterized by it CDF (which they
here call the /distribution function/ F such that 

				     F(x) = P(X \leq x).

Importantly, F is related to the probability density function C by

				       dF(x) = C(x)

for continuous F (but anyway F must càdlàg). If X is discrete then the CDF will results in
spikes, but their relative length should result in the probability, or something like
that). Under this perspective, the CDF is therefore more suited in characterizing
graphically the behavior of any random variable. 

The \tau^{th} quantile of X (with 0 < \tau < 1) can be described with

			      F^{-1}(\tau) = inf {x : F(x) \geq \tau},

and the median is defined by F^{-1}(\tau). 
