			    NOTES DE RECHERCHE SUR UN MEMOIRE
					   par
					THIERRY BM
				    ------------------
					   2015


* *Le 8 juin*

_Quelques définitions (dans le cas /newsvendor and portfolio/)_

Le vrai risque:

			R_{true}(q) = E_{D(x)}[C(q; D(x))]
				= E_{x}[E_{D(x)}[C(q; D(x))|x]]

Le risque empirique:

			  ^R(q;S_{n}) = 1/n \sum_{i}C(q, S_n^i)
				   = 1/n \sum_{i}C(q, d_i(x_i))

"Since we desire the true risk to be low, a combination of low empirical risk and
sufficient stability ensures this."


Let X:R^p be the information distribution, providing objects of the R^p form, and let R(x):R
be the (real) distribution returns, and let D = R\times{}X the /information space/. 

_Learning Algorithm._
A /learning alogrithm/ is a function whose argument is a dataset drawn from D^n, ie. with n
rows, mapping into the space of functions mapping from X to a portfolio space (to be
defined later) P. We shall denote this space of functions as X^P.

_Loss of a decision vector q._
The /loss/ l of a decision rule q \in Q \subset{} X^P with respect to a sample d=(x,r) \in{} D is defined as

				  l(q,d) = c(q(x),r(x))

for some cost function. 


Now how should this loss function be defined? On one hand we have the actual return of the
chosen stock. The real question here is when would the loss be zero. We always gain more
utility from higher return, and so if the chosen q gives us huge gains, then we cannot
claim that there's a single return at which the loss is completely absent. Our loss
function would therefore take the form of a decreasing exponential function exp(-\mu{}x).

But if we had such a loss function, we would then be out of line with a two-pieces linear
utility function. A log utility is of no use too, since its domain is only on R_{++}_{}. 

[[./Mai2015/FigExpUtility2.png]]

But -exp(-\mu{}x) could be a good utility function, that can besides be scaled and moved
around using a parameter \mu as an increasing rate. However we lose the insight developed
previously with the Newspaper article. 

The loss exp(-\mu{}x) is then perfectly in line with the utility and provide a reasonable
insight since the loss occured for higher return rates is much smaller than the loss
incured when dealing with higher return rates. 

We're also inline with the loss function of the newsvendor, since the loss we're trying to
minimize is now the same as the utility we're trying to maximize (modulo a minus sign).

In the newsvendor case, the loss the hypothesis q incurs on a random variate d=(x,r) drawn
from D, was defined as

				    \ell(q,d) = c(q(x),d)

where c was the newsvendors cost incurred from a demand d and an ordered quantity q(x). 

In our case, we again have not a situation where the loss is null, essentially because
we're in fact /not/ trying to predict the actual return rate, we instead try to form a good
portfolio from which we want to derive utility. The other problem, as mentioned
previously, is also that our utility has no supremum, ie. is not bounded above. In
pratical terms, this means that there's no single best decision that could have been made
had we known the return rate previously: if the rate was to be positive, then would have
massively borrowed to leverage the positive rate. On the other side, if the rate was
negative, then the reverse situation where we could have chosen between a mass short sale
or risk-free lending would have appropriate.

And in this sense, would there be no way to add a regularization term, ie. with which we
would discourage massive investments?

Here's a possible method to define \ell, but we first have to assume that ||q(x)||_2^2 \leq 1,
ie. the investment in the risky asset (under the two-assets portfolio hypothesis) is
bounded by 100%, in financial terms, we cannot borrow money at R_f to invest in the risky
asset. 

If r, ie. the return of the risky asset is above R_f, then we'll have wished we had
invested all our wealth in it, and thus the loss will be 

				  U(1,0) - U(q^{T}x, 1-q^{T}x)

if on the other hand the stock yields a return below R_f, then the reference will be the
utility derived from a perfectly safe portfolio, so that the loss becomes

				  U(0,1) - U(q^{T}x, 1-q^{T}x)

In either case, the loss is always positive (or at most zero).

*Note.* In the above expressions, U(x,y) represents the utility of a portfolio for which a
 proportion x is invested in the risky asset and y in the risk free asset.

According to Bousquet, an algorithm has \beta uniform stability with respect to the loss
function l if for all dataset of m rows, we have the bound

		      `||\ell(q(S_m,d) - \ell(q(S'_m,d))||_{\infty} \leq \beta_m

where q' is the hypothesis formed when removing a single row from S_m, the dataset (see
Bousquet p. 504 for total formalism). 

*Def.* An algorithm is uniformly stable if \beta_{m }\leq O(1/n).

*Def.* The /generalization error/ or the /risk/ depends on the training set S and the algorithm
 A_S in the following way:

				  R(A_S,S) = E_{d}[\ell(A_s,d)]

*Def.* The /empirical error/ is the average of the loss over the training set S of m
elements:

			       R_{emp}(A_S,S) = 1/m \sum_i \ell(A_s, d)



* *Le 9 juin*

We still wish to apply theoretical bounds upon the `loss' we can incur using our
algorithm. We use the notion of uniform stability, as developed by Bousquet and Elisseef
in their seminal work. 

<<uniformStabilityDefinition>>
*Def (Uniform stability).* An algorithm A has uniform stability \beta with loss \ell if, for any
 dataset S of m rows, ie. \forall S \in D^m, and any removed row i, then for d=(x,r) \sim{} D,

		    `   ||\ell(A_S(x), r) - \ell(A_{S\i}(x), r)||_{\infty} \leq \beta

ie.,

	            B_m = sup_{d \in D} |\ell(A_S(x), r) - \ell(A_{S\i}(x), r)| \leq \beta

For notation purposes, the left value, depending solely on the size m of S, will be
henceforth noted B_m or simply B.

Now for the hard part of giving a value to \beta. The problem with the current value of \ell is
how it has not a constant mapping, and will change its underlying application wether r is
above or below r_c (which could be R_f). 

Below are the two forms the loss function, depending on the portfolio composition: p=0 is
a perfectly safe portfolio, p=1 is a perfectly risky portfolio. They were plotted using
the exponential utility. 

[[./Mai2015/expULossAboveZero.png]]

[[./Mai2015/expULossBelowZero.png]]

We'll most probably need to bound the return drawn from the returns distribution R in the
interval [-\rho, \rho]. We can then add probalistic bounds upon it as described [[https://en.wikipedia.org/wiki/Stability_(learning_theory)#Uniform_Stability][in this
article]]. We also note that the loss function \ell thus defined is continuous with respect to
r. 

Let's make our ideas clearer. We have the following identities:
<<definitionCost>>

			c(p,r) = U(r) - U(pr + (1-p)R_f) if r > R_f
			c(p,r) = U(R_f) - U(pr + (1-p)R_f) if r \leq R_f
				    p = q_S^{T}x = q(S)^{T}x

Actually, this `multi-defined' function is in fact not that remote from the stability
criterion exhibited in the newsvendor case, where we have operators min and max operators
(\vee and \wedge{}) on the slopes of the cost function.

Let us first consider a fixed dataset S from which a decision vector q_S is algorithmically
defined, and let us apply the definition of the uniform stability. Then, no matter the
value of r, we're left with the following expression for B:

		  B = sup_{d \in D} |U(pr + (1-p)r_c) - U(p'r + (1-p')r_c)|

where 

				   p' = q(S^{\i})^{T}x

is the `less-informed' decision.

Now we can again apply the Lipschitz property of continuous functions stating that,
provided that dom f = S convex, then

`			     |f(x) - f(y)| \leq \alpha |x-y|

where

			       \alpha = sup_{x \in S} |f'(x)|

If the utility is linear, then \alpha = \beta, the slope of the utility left of r_c. If however we
consider an exponential utility, then the maximum value of the derivative will only be
reached at the edge of the considered interval, and so \alpha = \mu{} exp(\rho), which can be quite
high (more on that later...)

Either way, B is now bounded:

			   B \leq sup_{d \in D} \alpha |(p-p')(r-r_c)|
			     = \alpha sup |r-r_{c}| sup_{d \in D} |p - p'|

In the above expression, sup |r-r_{c}| will be \rho + r_c, since we asume that r_c > 0. We shall
call this quantity \gamma so that

				   B \leq \alpha{} \gamma sup |p - p'|

C'est là qu'on est rendu...


* Le 10 juin

We've been trying for a few days to establish a bound on the uniform stability of the
linear algorithm when dealing with a concave utility, and to do so we must dive into how
the algorithm operates.

We have the following equalities:

			      q_S = argmax_{q} Û(q,S) - \lambda||q||_2^2
			  q' = q_{S\i} = argmax_{q} Û(q,S^{\i}) - \lambda||q||_2^2

By the triangle equality we also have

'		      |p - p'| = |(q - q')^{T}x| \leq \sum_i |x_{i}| |q_i - q_{i}'|

					  * * *

Let's instead follow Theorem 22 from Bousquet. We first want to show that \ell as defined
previously is \sigma-admissible.
<<lossDefinition>>
				  \ell(q,(x,r)) = c(q^{T}x,r)

(Bousquet, Definition 19, p. 512)
<<sigmaAdmissibilityDefinition>>
*Def. (\sigma-admissibility).* A loss function \ell is \sigma-admissible if the associated cost function
 c is convex with respect to its first argument and the following condition holds for any
 p_1, p_2 and r:

`			   |c(p_1,r) - c(p_2,r)| \leq \sigma |p_1 - p_{2}|

We've shown yesterday that the left term reduces to

`				  |(p_1 - p_2)(r - r_c)|

The largest value |r - r_{c}| can reach is \rho+r_c, and so \sigma = \rho+r_c (previously declared as \gamma).


* Le 15 juin

First, previously undefined formally:

<<linearUtilityDefinition>>
_Def. (Linear Utility)_ The linear utility is defined as

				  U(r) = r + min(0, \beta{}r)

where 0 < \beta < 1. We simplify r_c = 0.

<<expUtilityDefinition>>
_Def. (Exponential Utility)_ The exponential utility with parameter \mu is defined as

				    U(r) = -exp(-\mu r)

where \mu > 0. There's no critical return here. 

We also note in the above expression that [[definitionCost][c]] is indeed convex under its first argument, as
long as 0 \leq p \leq 1.

<<sigmaAdmissibilityTheorem>>
_Thm._ The [[lossDefinition][loss function]] is [[sigmaAdmissibilityDefinition][\sigma-admissible]] with \sigma = \rho+R_f in the linear case and
\sigma = (\rho+R_{f}) exp(\mu \rho) in the exponential case.

_Proof._ The expression

				   `|c(p_1,r) - c(p_2,r)|

reduces to

			 `|U(p_{1}r + (1-p_{1})R_f) - U(p_{2}r + (1-p_2)R_f_{}|.

Now because U is Lipschitz continuous, then the above expression is bounded by

		 \sigma |p_{1}r + (1-p_{1})R_f - (p_{2}r + (1-p_2)R_{f}| = \alpha |p_1 - p_{2}||r-R_{f}|

where

			      \alpha = max_{x \in [-\rho,\rho]} |U'(x)|.

Now we've considered two utility forms. In the [[linearUtilityDefinition][linear case]] the derivative is constant (set
to 1) because the derived utility at the left of r_c has always slope 1, and so \alpha=1. If
[[exoUtilityDefinition][utility is exponential]], then \alpha = exp(\mu \rho). 

Now the \sigma bound must hold for all r. The expression |r-R_{f}| will reach its largest value at
r=-\rho since R_{f} is asumed to be non-negative. \Diamond

Just like in the newsvendor case, we'll assume that feature vectors must lie in a ball of
radius X^2_{max}.

<<BousquetTheorem7>>
Let F be a reproducing kernel Hilbert space with kernel k such that \forall x \in X, k(x,x) \leq \kappa^2
< \infty. Let \ell be [[sigmaAdmissibilityDefinition][\sigma-admissible]] with respect to F. The learning algorithm A defined by

		A_{S} = argmin_{g \in F} 1/n \sum_i^n \ell(g,d_i) + \lambda ||g||^2_k

has [[uniformStabilityDefinition][uniform stability]] \alpha_n wrt \ell with

			  \alpha_n \leq \sigma^{2 }\kappa^2 / 2\lambda{}n.

<<algorithm>>
The decision algorithm of our model produced by a dataset S_n = {(x,r)_i} is defined to be 

	     q^\star = argmin_{q \in R^p} 1/n \sum_i^n c(q^{T}x_i,r_i) + \lambda||q||^2_2

The investment decision following information vector x will therefore be p = q^\star^{T}x,
where p is the proportion to be invested in the risky asset.

<<stabilityTheorem>>
Using The [[uniformStabilityDefinition][stability]] \alpha_n of our proposed [[algorithm]] is bounded by the following:

			       \alpha_n \leq (\rho + R_f)^2 X^2_max / 2\lambda{}n

in the case of a linear utility and by 

			      \alpha_n \leq exp(2\mu \rho) X^2_max / 2\lambda{}n.

These results follow from the [[sigmaAdmissibilityTheorem][\sigma-admissibility theorem]] of our [[lossDefinition][loss function]] and
[[BousquetTheorem7][Bousquet's Theorem]].

<<trueRiskDefinition>>
The true risk with respect to algorithm A and learning set S_n is defined as

			      R_{true}(A,S_n) = E_{d}[\ell(A_S, d)]

that is, in plain words, the expected loss we'll have when applying our algorithm in the
wild, ie. out of sample.

<<empiricalRiskDefinition>>
The empirical risk with respect to algorithm A and learning set S_n is defined to be 

			     ^R(A,S_n) = 1/n \sum_i^n \ell(A_S, z_i)

that is, in plain words, the average risk our model has produced over all training
points. 

<<maxCostProposition>>
Using the aforementioned algorithm, the maximum loss we can incur is when p=1 with
r=-\rho. In such a case, 

				 c(1,-\rho) = U(R_f) - U(\rho).

We shall call this quantity \gamma.

<<RudinLemma2>>
Let A be an algorithm with uniform stability \alpha_n wrt a loss function \ell such that 0 \leq
\ell(A_S,d) \leq M, for all d = (x,r) \sim D and all sets S_n of size n. Then for any n\geq1 and any \delta \in
(0,1), the following bound holds with probability at least 1-\delta over the random draw of the
sample S_n:

	       `|R_{true}(A,S_n) - ^R(A,S_n)| \leq 2\alpha_n + (4n\alpha_n + M) \radic(log(2/\delta)/2n)

<<generalizationBoundTheorem>>
Our [[algorithm]] has a generalization bound of 

	       `|R_{true}(A,S_n) - ^R(A,S_n)| \leq 2\alpha_n + (4n\alpha_n + \gamma) \radic(log(2/\delta)/2n)

with probability 1-\delta. It follows from [[RudinLemma2][Rudin et al.'s Lemma 2]] and our [[stabilityTheorem][stability Theorem]]. \diamond


* Le 17 juin

On a commencé à faire des tests numériques. Voici ce qu'il faudrait faire:

 - Etablir une routine de cross-validation afin d'obtenir un \lambda de régularisation
 - Tracer des points de corrélation (R^2 ?) selon la valeur de \lambda
 - Tracer des courbes de stabilité sur plusieurs tests.
 - Tracer des courbes de vrai risque par rapport au risque empirique afin de déterminer
   comment se comporte le modèle.
 - Tracer vis-à-vis ces courbes les bornes théoriques.

Ensuite il faudra commencer à réfléchir sérieusement à la facon dont on peut créer un
portefeuille à plusieurs titres. Il s'agit probablement d'optimiser sur un espace de
matrice, et non pas simplement sur un vecteur q. Par contre la théorie devra être revue en
profondeur afin d'obtenir de nouvelles bornes.

Now how should X^2_max and \rho should be defined in our numerical tests? First off, an
information vector, at least how it as been defined now, is an uncorrelated multivariate
random variable with mean 0. Now we know for a real random variable that with 95%
confidence level, that x will lie within [-1.96, 19.96]. We're interested with X^2_max,
which is ||x||^2_2. First let's suppose p=2, with both coordinates at 1.96. Then we would
have X^2_max = (1.96)^p = (1.96)^2. However does the confidence shrink?

Let's think out loud. I have a first random variable whose value I know with a 95%
confidence interval lies in [-1.96, 1.96]. But each component is unrelated, by
hypothesis. And the probability of two unrelated events is the product of the two
probabilities. 

We can therefore derive a general identity. If \Sigma is diagonal, then with confidence (.95)^p,
the norm of the vector is less than (1.96)^p... TBC


* Le 18 juin

Donc, comment mesurer numériquement la [[uniformStabilityDefinition][stabilité algorithmique]]? Le probleme le plus
évident est qu'il s'agit de maximiser sur D au grand complet. 


* Le 19 juin

Aujourd'hui on travaillera sur les intervalles de confiance, notamment essayer de mieux
quantifier X^2_max.

Rappelons d'abord que X^2_max représente la région dans laquelle, avec probabilité 1-\delta, se
trouvent tous vecteur d'information x. 

Une chose à la fois. Il était question aussi d'établir une routine de cross-validation
afin d'obtenir un \lambda^\star optimal de régularisation. Voici les étapes qu'elle devrait
réaliser:

  1. On fixe un vecteur de transformation t \in R^p. On fixe aussi un ensemble d'entraînement
     S_n et un ensemble de test S_m à partir de t.
  2. On fixe \lambda = 0.
  3. On résout q^\star à partir de S_n. On conserve le coût total de l'ensemble dans un
     vecteur de résultat c_in.
  4. Avec q^\star, on détermine le cout de l'ensemble S_m qu'on stock dans un vecteur de
     résultat c_out.
  5. On recommence (3) avec \lambda = \lambda + \delta\lambda.
  6. END

					  \diamond \diamond \diamond

_Exploration d'un portefeuille constitué de plusieurs titres._

On considère m-1 titres risqués, et un titre sans risque de rendement R_f. Chacun de ces
titres et composé d'un vecteur d'information x_st, ie. de m ensembles d'informations S_n sur
n jours. 

On souhaite encore former une décision linéaire par rapport à l'information disponible. On
avait donc une application q (ou un algorithme) qui à partir d'un vecreur d'information x
permettait de rendre un portefeuille de la forme [p, 1-p]. Autrement dit, on obtenait un
scalaire à partir duquel l'ensemble du portefeuille était bien défini. Maintenant ce dont
on a besoin, c'est d'un portefeuille vecteur. 

Réfléchissons. A tous les jours on n'a non plus un seul vecteur d'information x, mais bien
une matrice d'information X dont chaque rangée représente un titre et chaque colonne
représente une information quelconque, ie. pour chaque titre s dont on a un vecteur
colonne d'information x_s, on a maintenant une matrice définie par

			       X = [x_1 \cdot\cdot\cdot x_{s}]

On doit donc définir un vecteur de décision q \in R^s tel que Xs rend un vecteur de
portefeuille a s éléments. Le rendement de ce portefeuille sera alors donné par r^{T}Xs, où r
est le vecteur de rendement à cette journée donnée. 

A present, comment déterminer la quantité à investir dans le titre sans risque?
Puisqu'auparavant on allouait la quantité restante (ie. 1 - q^{T}x) au titre sans risque, il
s'agit finalement de faire la même chose ici, ie. 1^{T}Xs donne la composition totale dans
les titres risqués, alors 1 - 1^{T}Xq donnera l'allocation sans risque, de sorte que le
rendement total deviendra:

				   r^{T}Xq + (1 - 1^{T}Xq)R_f

A partir de ce nouveau portefeuille, on peut sans doute utiliser les mêmes outils
développés auparavant, ie. pour le portefeuille à un seul titre. Voyons voir...

_Coût et _Perte._

On a auparavant défini notre fonction de perte (loss) \ell comme étant une fonction prenant
comme argument un algorithme déterminé à partir d'un dataset S_n et un vecteur
d'information complète d=(x,r) \in R^{p+1} pour en retourner le coût total. Le coût était quant
à lui déterminé à partir de l'utilité concave U. 

Le coût a été défini de facon très naïve, puisque qu'on a déterminé que si le rendement du
titre risqué était supérieur au rendement sans risque, alors on s'attendait (ou du moins
on voulait privilégier) un portefeuille constitué uniquement du titre risqué, ie. q^{T}x
\geq 1. Si au contraire le rendement était inférieur au titre sans risque, alors on
s'attendait à ce que le portefeuille soit complètement sans risque, ie. q^{T}x \leq 0. 

Est-ce qu'on pourrait dès lors avoir un coût total qui serait une somme des coûts
individuels pour chaque titre? En fait ce n'est pas nécessaire car on obtient déjà un
rendement du portefeuille scalaire auquel on peut simplement appliquer la même logique que
précédemment. 

Ce sera donc c(p,r), où p et r sont les valeurs vectorielles d'allocation et de rendement,
ie. p = Xq. Mais ce n'est pas si clair... Car supposons que tous les titres donnent un
rendement inférieur à R_f, sauf un. Alors on veut clairement que le portefeuille soit
constitué uniquement de ce titre, et de rien dans le reste. le coût serait donc 

			    \lfloor{}U(max r) - U(r^{T}p + (1 - 1^{T}p)R_f)\rfloor

si max r > R_f.

Au contraire, si max r < R_f, alors le coût deviendra 

			   \lfloor{}U(R_f) - U(r^{T}p + (1 - 1^{T}p)R_f)\rfloor.

Il faut donc à présent revérifier les preuves afin de s'assurer que tout fonctionne. 

On peut toutefois énoncer un nouvel algorithme. On chercherait vraisemblablement à une
fois de plus minimiser le cout total sur un dataset S_{n\times{}s}.

	          q^\star = argmin_{q \in R^p} 1/n \sum_i^n c(Xq, r) + \lambda||q||_2.


* Le 25 juin

Pondering, once again. Our loss function \ell defined above was designed in a complicated
fashion, but this was to allow a single reference point where the loss was perfectly zero,
ie. when investment decision were perfect.


* Le 27 juin

Two loss functions were previously defined:

				c(p,r) = -U(rp + (1-p)R_f)

and

			c(p,r) = U(r) - U(pr + (1-p)R_f) if r > R_f
			c(p,r) = U(R_f) - U(pr + (1-p)R_f) if r \leq R_f.

There are a few differences between these two, for example, given a dataset where
information points x_i are positively correlated with returns, then the minimization of the
expression /without a regularization term/ will yield an unconstrained optimization
problem. In contrast, the second expression is bounded below by zero, and so must be
constrained. Bousquet also defines a cost function as having the signature

				      c: Y \times Y \to R_{+}

ie, mapping onto the non-negative real numbers. However, the proofs don't seem to use this
property, at least not for theorems that interest us. 

Another key difference is in the learning itself. With the first cost function, we simply
try to maximize the utility derived from a decision p, whereas with the second cost, we're
in fact minimizing the utility we didn't achieve, but that we could have achieved. And if
we achieve a higher utility, then there's no additional benefits to it. 


* Le 28 juin

Xq: returns the allocation for a m-portfolio.
diag(XQ): returns the allocation for a m-portfolio

Returns for a m-portfolio

			     r^{T}diag(XQ) + (1 - 1^{T}diag(XQ))R_f

which can also be expressed 

			      r^{T}diag(P) + (1 - 1^{T}diag(P))R_f.

But now the cost c(P,r) /must/ be a jointly convex function over P and r. This is how c(P,r)
could be expressed (easiest form):

			c(P,r) = -U(r^{T}diag(P) + (1 - 1^{T}diag(P))R_f)

First of all, are not we losing some cross-information about how an asset i will react to
information about stock j? This reminds me of a covariance matrix of some sort.

At any rate, the question is now to know if, for any P \in R^{m\times{}m} and any r \in R^m, the
expression r^{T}diag(P) is a convex one. However something tells me it might not be the
case... Matlab at the rescue!


* Le 8 juillet

There is still a missing theorem from our analysis, where we we look to quantify the
difference between the in-sample cost using our in-sample decision vector q with the
in-sample cost using the `optimal' decision vector q^* (the true optimal solution).

We recall that R_{true}(q) is the expected value of the cost a decision vector q will yield
over the whole distribution D, whereas ^R(q) is the average cost produced by a decision
vector q over the test set. 

There are therefore two measures we wish to quantify. First is |R_{true} - ^R| using ^q. This
measure indicates how well (or how poorly) our decision q would perform out-sample,
compared to its performance in-sample.

Another measure is |R_{true}(q^*) - ^R(q)| which indicates how far our measured average cost
using trained decision vector q will be from the absolute best, ie. the expected cost
using the optimal decision vector had we access to the true distribution. 

According to Rudin and Gah Vi Yan, this last measure, ie. our performance compared against
the best available performance, is at least (or strictly) superior to the first measure,
where we use our own trained decision vector, and not the best available vector. Two terms
have to be added up tocompensate : first one will be the in-sample bias due to
regularization, and the other one the bias due to the finite sample we have access to.

					  \star \star \star

Is it true we really draw iid ?

					  \star \star \star

Again, let us analyze how Rudin and co. did this trick in quantifying an unknown
distribution with an unknown decision vector, which we know is the best available given
this unknown distribution D. 

First is the following lemma, stating that

    The newsvendor objective funciton equaivalent to the /nonparametric regression loss
    function/ H_r(\cdot), save a constant multiplier.

Which, formally means that 

		 C(q,D) = (1/2) H_r(q-D) := (1/2) [|q-D| + (2r - 1)(q-D)]

where r = b/(b+h).

Now first, how was the cost C(q,D) defined? We recall that D is the `uncertain (random)
future demand' and therefore that

			     C(q,D) = b(D - q)^{+} + h(q - D)^{ +}

that is therefore the cost of having a decision q when faced with a certain demand D. In
our case, this would translate in the cost of having of an investment allocation p when
faced with a return r. We had previously defined it as 

				c(p,r) = -U(pr + (1-p)R_f).

A key difference here is that our cost is wrapped with a utility U whereas /their/ cost is
not wrapped at all and is simply linear. Well, it surely depends on our own utility, if it
is linear then there's probably no problem, but if it's exponential, then we might run
into some problems. 


* Le 9 juillet 2015

_Theorem 3._

Denote the true optimal solution q^* = q^*(x_{n+1}), and assume it is continuously
differentiable up to order k on some fixed open neighborhood of zero in R^p and its k^{th}
derivative is uniformly Hölder continious at zero with exponent \gamma (Condition 1). Then with
probability at least 1-\delta over the random draw of the sample S_n, where each element of S_n
is drawn iid from an unknown distribution on X \times D:

	   `|R_true(q^*) - ^R_in(q^{}^,S_n)| \leq (1) + (b \vee h) M \radic(log n) / n^{s/(2s + p)}

where s = k+\gamma is the order of smoothness of q^* and ^q is the solution to (NV-ML1) and M is
a constant that depends on the demand distribution. 


* Le 13 juillet 2015

When we say the true optimal solution as being

				      q^* = q^*(x_{n+1})

we must be careful in how we understand this function. Because no assumption is being made
on the functional subset of which q^* belongs to. In other words, whereas in our training
we considered only affine functions of the input x, here we make absolutely no assumption
on the form that x^* could really have. Therefore, it indeed makes sense to talk about
continuous derivative on a region, because, in general, x^* is not a linear function of x. 

However, there is still a notational problem, as q^* seems to be defined in term of x_{n+1},
which makes in fact little sense... q^*(x_{n+1}) \in R is in fact a real quantity (in the newsvendor
case: we want to order /real/ quantities).

Therefore we can only infer that in the above equation (distance between in-sample risk
and true optimal risk) q^* really represents a function, and that x_{n+1} has no place
here. This is only a first draft, so I guess it's ok. 

Now onto the proof, not the most easiest one I've seen...

_Condition 1._ It is first said implicitly that q^*(x) corresponds to the /r-th conditional 
quantile function./ Then, we ask that this r-th conditional quantile function q^*(x) is
continuously differentiable up to order k on V an open neighborhood of R^p, and that their
k-th derivates are uniformly Hölder continuous at 0 with exponent \gamma, ie.

			   `|D^{u}q^*(x) - D^{u}q^{*}(0)| \leq c||x||^\gamma

for all x \in V and [u] \leq k.

Then 

					s = k + \gamma

is refered as the /order of smoothness/ of the function q^*(x).

_Proof._ The rest of the proof first applies triangle inequality to the expression, so
that |R_{true}(q^*) - R_true(q^)| is isolated. And then the mystifying step, where

		     `|R_{true}(q^*) - R_true(q^)| \leq (b \vee h) E |q^* - q^{}^ |

Also, let us be clear about each term. E|q^* - q^ | means here that the expected
(numerical) value for each algorithm, the first being the optimal one, and the second
being the one generated from the sample. 

To apply their theorem, Rudin et al. had to relate their cost function to the non
parametric loss function H_r. 

What Rudin et al. actually do is taking results from quantile regression, since a strong
theoretical results have already been proved in the case where a loss corresponds to their
own (newsvendor) loss. Therefore, our task would be to take theses results of a loss of
form 

			    H_\alpha(t) = |t| + (2\alpha - 1)t

and translate them to a form similar to our own linear utility shape, ie. 

			     H_\beta(t) = t + min(0,\beta{}t)

A result from Koenker says that 

				    E[H_\alpha(Z - t)]

is minimized by taking t = \alpha^{th} quantile of Z. So if we go back to the newsvendor context,
where the cost that is to be minimized over the sample set is C(q;D) = H_\alpha(q - D),
with \alpha = b/(b+h). /If/ we knew the distribution of D, then we would know that to minimize

					E[C(q;D)]

that is, over the random variable D, we would have to take the (b/(b+h))^{th} quartile. So
two taks appear here:

- First, we need to determine what would the optimal p^{*} in our case would be, if we knew
  the form of the distribution of r. This p^{\star} value needs to be a function of D, whatever D
  is (D here being the distribution).
- Next we also need a theorem like the one that was devised by Chaudhuri. 


_Section 1.3 of Koenker._

We characterize a real-valued random variable X using its CDF (cumulative distribution
function) F:

				     F(x) = P(X \leq x)

and the *t^{th} quantile* by:

			        F^{-1}(\tau) = inf{x : F(x) \geq \tau}.

_Remark._ The median is defined by F^{-1}(1/2). Dropping the inf notation, and asuming that F
is monotonic, we get 

					F(x) = 1/2. \diamond

We wish now wish to describe a /loss function/ with parameter \tau by:

				  \rho_{\tau}(u) = u(\tau - I(u<0)),

where 0 < \tau < 1. The goal is now to find x^ minimizing the expected loss. We note that 

		         \rho_{\tau}(u) = (\tau-1) min(0, t) + \tau max(0,t),

or, alternatively, by

			      \rho_{\tau}(u) = |u| + (2\tau - 1)u.

Still taking cue from section 1.3, we seek to minimize

          E\rho_{\tau}(X - x^) = (\tau - 1) \int_{-\infty}^{x^} (x - x^) dF(x) + \tau \int_{x^}^{\infty} (x - x^) dF(x).

We then differentiate to obtain 

0 = (1-\tau) \int_{-\infty}^{x^} ....

Difficulties were encountered. 18hours later however, after long and arduous internet
spelunking, we find this new wonderful tool, the /Leibniz Integral Rule/!


* Le 14 juillet 

We recall we were trying to find an analytical minimizer to the linear loss for a random
variable X with CDF F. If loss is characterized by \rho_\tau(u) as above, then we want to find
x^ that will minimize the expected value of the loss using X-x^:

				minimize E[\rho_\tau(X - x^)].

To do so we express the expectation using the fact that the derivative dF(x) of the CDF F
is the PDF of F and therefore

        E[\rho_\tau(X - x^)] = (\tau - 1) \int_{-\infty}^{x^} (x - x^) dF(x) + \tau \int_{x^}^{\infty} (x - x^) dF(x).

Differentiating with respect to x^ and applying Leibniz Integral Rule yields:

	          (1 - \tau) \int_{-\infty}^{x^} dF(x) - \tau \int_{x^}^{\infty} (x - x^) dF(x),

which is equivalent to 

		            \int_{-\infty}^{x^} dF(x) - \tau \int_{-\infty}^{\infty} dF(x).

Because F(-\infty) = 0 and F(\infty) = 1, the first integral evaluates to F(x^) and the second
evaluates to 1. Now the derivative must be zero, and therefore we find the rule:

					F(x^) = \tau,

ie. to minimize the loss characterized by parameter \tau, we must take a regressor x^ equal
to the \tau^{th} quantile. 

Now! Onto real business. What would be our `theoretical' parameter p that would minimize
our loss. Back to a paper pad... We should perhaps start with the canonical cost 

				c(p,r) = -U(pr + (1-p)R_f).

Here, r is the random variable, and p is the chosen quantity. Let us assume r is
characterized by a CDF F(r). Then we wish to minimize the expected loss with respect to r:

				   minimize E_{r}[c(p,r)].



* Le 15 juillet

Back to quantile regression. For some reason, this theory seems interesting and I'd like
to understand more deeply Choquet's utility which seems of the highest interest, since, I
dearly hope, I could relate theorems provided by Chaundhuri (or whatever his name is) to a
big-data scenario, ie. characterize the true bound on in-smaple estimatios to the optimal
decision function used in the true risk function. 

Besides, I think a lot of statistics intuition, of which I deeply lack, might be pallied
up from this book. 

					  \star \star \star

Instead of using the CDF or a random variable X, X is characterized by it CDF (which they
here call the /distribution function/ F such that 

				     F(x) = P(X \leq x).

Importantly, F is related to the probability density function C by

				       dF(x) = C(x)

for continuous F (but anyway F must càdlàg). If X is discrete then the CDF will results in
spikes, but their relative length should result in the probability, or something like
that). Under this perspective, the CDF is therefore more suited in characterizing
graphically the behavior of any random variable. 

The \tau^{th} quantile of X (with 0 < \tau < 1) can be described with

			      F^{-1}(\tau) = inf {x : F(x) \geq \tau},

and the median is defined by F^{-1}(\tau). 


* Le 16 juillet

_Koenker, section 8.9_ Choquet Utility, Risk, and Pessimistic Portfolios

On souhaite appliquer la théorie de la régression quantile à la théorie de l'optimisation
de portfeuille. Pour ce faire, on introduit /l'utilité espérée de Choquet/.

_Utilité espérée de Choquet._

We have the identity 

		           \int_{-\infty}^{\infty} u(x) dF(x) = \int_{0}^{1} u(F^{-1}(t)) dt.

Instead of integrating over the range of x (spanning all of the reals) we instead
integrate over the t quantiles of x, relating the value of t and x by

					F^{-1}(t) = x,

and

					F(x) = t,

which yields the result. 

_Def. Comonotonicity._ Random variables X and Y are said to be /comonotone/ if there exists a
random variable Z and monotone increasing f and g such that X = f(Z) and y = g(Z). 


* Le 17 juillet

We shall continue our dissertation of the quantile regression theory applied to the
optimization of portfolios, in the idea of applying theorems to derive true optimal bounds
on the returns of sampled portfolios with /true optimal/ portfolios. 

_Koenker, p. 288_

Under this formulation, we are no longer assessing if a random variable is superior to
another one by virtue of its expected utility, but also using a new function \nu, such that
"u transforms probability into utility terms, whereas \nu transforms probability
assessments." \nu is called a /distortion term/. 

We are still integrating with respect to the quantile of the random variable, ie. from 0
to 1. F^{-1}(t) gives the value of the random variable at the t^{th} quantile, and so u(F^{-1}(t))
gives the utility at the t^{th} quantile. 

"The distortion \nu acts to inflate or deflate the probabilities /according to the rank
ordering of the outcomes/ [provided by t]. In the terminology of Choquet (1954), the
distortion measure \nu is a capacity. 

"If \nu is concave, so that d\nu is decreasing, then the least favorable events [if we assume
they correspond to a lower quantile] receive increased weight, and the most favorable
events [those corresponding to a higher quantile].

_Choquet Risk Assessment._

*Monotonicity.* X, Y \in /X/, with X \leq Y => \rho(X) \leq \rho(Y)

					  \star \star \star

Y existe-t-il une transformation par laquelle on peut passer de la fonction de cout du
newsvendor a notre propre fonction de cout ?

La différence majeure est la suivante:

 - Dans le cas du newsvendor, la forme du coût ne change pas, c'est seulement selon la
   quantité demandé D, l'origine se déplace, mais la forme demeure toujours la même.
 - Alors que dans notre cout à nous, selon le taux de rendement, et l'allocation qu'on
   fait à l'actif risqué, les pentes relatives vont changer pour un p fixe et pour un r
   variable. En effet, si p est fixe et qu'on alloue (par exemple) 75% dans un actif
   risqué, le cout peut augmenter ou au contraire dimininuer beaucoup plus rapidement
   lorsque r est grand que lorsque r est faible. 

La morale est donc que si une telle transformation existait, il faudrait tenir compte de
ces différences. 

Car que recherche-t-on finalement? On recherche une certaine fonction de transformation f
telle que f(c_n(p,r)) = c_p(p,r) où c_n serait le cout newsvendor et c_p serait le cout du
portefeuille. On remarque d'ailleurs l'analogie p \gets{}\to q les quantités que l'on peut choisir
(ie. déterministe) et r \gets{}\to d, les quantités aléatoires, imposées par le système. 

Plus simple peut être: comment peut-on à partir d'une fonction abs(x) obtenir une fonction
2-pièces linéaire? Supposons par ailleurs que cette fonction soit de la forme 

			      g(x) = h min(0,x) + b max(0,x)

La branche négative a donc une pente h, et la branche positive une pente b. Alors la
fonction f qui transformera abs(x) vers sera donnée par 

Bon, ca ne fonctionne pas car abs(x) n'est pas surjective. Screwed... Quoique l'idée était
d'abord d'appliquer une application linéaire. Toutefois, elle doit s'appliquer sur des
points (x,f(x)). De plus il faudrait en fait deux matrices de rotation, la première
s'appliquant pour les points (x,f(x)) tels que x>0 et une autre s'appliquant aux points
x<0.

Non décidément ca ne fonctionne pas... 

Que tente donc de faire cette fameuse théorie de la régression quantile. On tente de
trouver un estimateur qui minimise la perte sur un semble d'observation, en supposant que
le bruit est linéaire, et possiblement asymétrique. J'y reviens, car il me semble que la
théorie qui y est développée peut peut être se traduire dans un cadre où le cout n'est pas
centré. 


					  \star \star \star

Peut être vaudrait il mieux tâcher de se concentrer sur autre chose, plutôt que sur cette
maudite borne. Faire des tests numériques pourrait être un bon point de départ. 


* Le 4 août 2015

We wish to develop a robust method that allows us to mix /covariance/ between each feature
of each asset in an investment policy.

To do so, we first propose an investment policy based on a vector q such that q = [q_1;
q_{2}], in Matlab notation., ie. two columns vectors stacked on each other. 

Let X be the feature matrix for a stock i. Then, following investment policy q, we would
have the following return on investment:

			      r_i q_{2}^{T }X_{i }q_1 = r_i Tr(X q_1 q_{2}^{T})

On the right hand expression q_1 q_2^T is a rank-1 matrix. We could in fact propose a more
general formulation where we would have instead a full p-rank matrix Q, and so the
investment policy would therefore become

					r_i Tr(XQ).

However, let us first dissect the first proposition. 


* Le 7 août 2015

*Révision.*

_First approach._ Only one decision vector q such that total return on a single period is
given by 

				   r^{T}Xq + (1 - 1^{T}Xq)R_f

Therefore, the role of q is to go from the feature space over the allocation space, ie. Xq
is the allocation vector for the m assets. Using summing notation, we have instead, *only
for the risky assets* the following:

					 \sum_i^m x^{T}_{i}q.

_Second approach._ We instead introduce a decision matrix Q_{p\times{}m} with m columns for
each asset, and p rows for each feature. The expression XQ therefore means that elements
on the diagonal of the resultant matrix have the form x_i^{T}q_i where q_i is an allocation
vector assigned only to the values of the i^{th} asset. Again, with summation notation, the
total return on risky assets is of the form:

				       \sum_i r_i x_i^{T}q_i.

As noted in /Travail/, this method still has a disavantage though, in that it won't consider
the eventual influence of information x_j of asset j on asset k, ie. each stock is trained
independantly on to each other. 

_Alternative second approach._

			        \sum_i r_i (x_i^{T}q_1 + \sum_{j\neq{}i} x_j^{T}q_2)

The role of q_1 is therefore to therefore to map from the feature space of asset i to the
its scalar allocation x_i^{T}q_1, while adding another correction term in the form of the
influence from the other feature vector from all other assets. 

According to Delage, this would be equivalent to having Q = [q_1 q_2 \cdots q_{2}] and going
with total return expressed with 

				      \sum_i r_i tr(X_i Q)

This leads to the following sum for allocation of the asset i:

				x_i^{T}q_1 + x_1^{T}q_2 + \cdots + x_m^{T}q_m.

So what does this expression mean? The first term is obviously how to go from the feature
space of the considered asset to its allocation scalar space. We must note however that
the decision vector to go from a space to another is the same for all assets, and so in
the minimization process we might actually average over all the reaction of the asset to
the market. 

What about the rest of the terms in the sum. We hope to have conditioned q_2,\dots,q_m so
that they can tell us what should be the contribution from each of the other features from
other assets 1,\dots,m.

So what are the problems with such an approach? Well, the cross-influence of the j^{th} asset
on other assets will always be x_j^{T}q_j. This is a problem, since some asset i might be more
sensitive to asset j than asset k would be to the same asset. For example GOOG asset price
might react more strongly to YHOO than GE would react to YHOO. Besides, GOOG might react
more strongly to a certain feature than YHOO would react to the same feature. 

					  \diamond \diamond \diamond

We should also consider what happens when we have multiple feature of the same kind during
the day. Aka news, of which we might have many datapoints for a single holding period. 

					  \diamond \diamond \diamond

So what do we want? First, we want independance of the results. We also want symmetry,
ie. the order 1,\dots,m matters not. Ideally, we'd also have a regularization term so that
liked stocks (eg. technology) be aggregated one to each other. Actually, I'd rather have
an aggregation over the influence from other stocks, ie. GOOG should react perhaps more
strongly from technology stocks, than from other. Of course, this should be set using a
regularization parameter. I'm thinking of something like L1 regularization, where either
you're in the group, or you're not. 

So do we sort all of this. Here's what we need:

 - Given an asset, we insist that the decision relies more strongly on its own feature
   vector than from feature vectors from other stocks.
 - Second, for each stock, the decision vector mapping from its own feature space to the
   allocation space should be trained independantly, ie. we suppose that some assets are
   likely to react more to some features than other would. Hence, we need a q decision
   vector mapping to each feature vector for each of the stocks. That is, we need m
   different qs for each of the stock.
 - The influence the m-1 features from other assets have on a single asset should also be
   independant. So for asset i, we need q_{1i},\dots,q_{1m} for all different
   vectors. Considering that features be normalized, we also demand that \Norm{}q_{ii}\Norm > \sum
   \Norm{}q_{ij}\Norm. This indicates that the influence is stronger for features from a certain
   asset.
 - Finally, we demand that ther eis some kind of aggregation between stocks, so that news
   from a sector only affects a handful of assets. How can we have this demand in a
   mathematical form? Is a good question.
 - At any rate this means that we need a matrix for erach of the asset. And the
   regularization term will link these matrices one to each other. 


* Le 9 août 2015

* Le 13 août 2015

Comment caractériser la norme de matrice qu'on tente de définir? Reposons le problème. On
a une matrice n\times{}n qu'son souhaite caracteriser en tant que matrice de k blocs. On suppose
que M est equivalente a M' si de M on peut passer a M' en appliquant une suite
d'operations lineaires qui interchangeant les colonnes de M. Supposons que l'operation qui
fait passer de la colonne c a c' est A_c^{c'}. Alors on peut appliquer une suite d'operations de
la forme 

			                   M' = A_c^{c' }\cdots{} A_{\sigma}^{\sigma'} M.

